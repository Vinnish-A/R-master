


# 二分类和多分类问题


```r
data = as_tibble(iris)
data_no_setosa = filter(data, Species != "setosa")
data_no_versicolor = filter(data, Species != "versicolor")
data_no_virginica = filter(data, Species != "virginica")
```

## 二分类


```r
two_class = function(data) {
  model = glm(Species ~ ., data = data, family = binomial())
  predict(model, newdata = data[, -5], type = "response")
}
two_class(data_no_setosa)
```

```
##            1            2            3            4            5            6 
## 1.171672e-05 4.856237e-05 1.198626e-03 4.220049e-05 1.408470e-03 1.018578e-04 
##            7            8            9           10           11           12 
## 1.305727e-03 5.351876e-10 1.458241e-05 1.481064e-05 3.990780e-08 3.744346e-05 
##           13           14           15           16           17           18 
## 9.947107e-08 7.988665e-04 1.378280e-08 2.828836e-06 1.326003e-03 1.481153e-08 
##           19           20           21           22           23           24 
## 5.959820e-02 8.712675e-08 4.048381e-01 3.405812e-07 2.248338e-01 4.023809e-05 
##           25           26           27           28           29           30 
## 1.410660e-06 7.060188e-06 7.124099e-04 2.760617e-01 9.651525e-04 1.290424e-10 
##           31           32           33           34           35           36 
## 8.469327e-08 5.298820e-09 8.707382e-08 8.676299e-01 2.169221e-03 2.129823e-04 
##           37           38           39           40           41           42 
## 2.979719e-04 2.551360e-04 7.884147e-07 1.109268e-05 3.969831e-05 1.596216e-04 
##           43           44           45           46           47           48 
## 4.360614e-07 8.158121e-10 1.502115e-05 2.541253e-07 3.085679e-06 2.309662e-06 
##           49           50           51           52           53           54 
## 6.163826e-11 2.344150e-06 1.000000e+00 9.996139e-01 9.999990e-01 9.997188e-01 
##           55           56           57           58           59           60 
## 9.999999e-01 1.000000e+00 8.908123e-01 9.999955e-01 9.999921e-01 1.000000e+00 
##           61           62           63           64           65           66 
## 9.902584e-01 9.997429e-01 9.999800e-01 9.999673e-01 9.999999e-01 9.999952e-01 
##           67           68           69           70           71           72 
## 9.976994e-01 9.999999e-01 1.000000e+00 9.204923e-01 9.999996e-01 9.995130e-01 
##           73           74           75           76           77           78 
## 1.000000e+00 9.484339e-01 9.999824e-01 9.995586e-01 8.245440e-01 8.022990e-01 
##           79           80           81           82           83           84 
## 9.999992e-01 9.712013e-01 9.999969e-01 9.999189e-01 9.999999e-01 2.048741e-01 
##           85           86           87           88           89           90 
## 9.664047e-01 1.000000e+00 9.999999e-01 9.964973e-01 6.691425e-01 9.998717e-01 
##           91           92           93           94           95           96 
## 1.000000e+00 9.999440e-01 9.996139e-01 1.000000e+00 1.000000e+00 9.999932e-01 
##           97           98           99          100 
## 9.991067e-01 9.989939e-01 9.999956e-01 9.776789e-01
```

注意到，二分类输出的是一个个0到1之间的概率值，数据均衡的话取阈值0.5，将概率化为二分类结果。数据不均衡的话采用其它阈值。

二分类预测结果的评价指标是交叉熵损失函数，越低越好了。


```r
cross_entropy = function (truth, response) {
  # 注意到，当truth为0或1时，其中一项就被消掉了
  - mean(truth * log(response) + (1-truth) * log(1-response)) 
}

cross_entropy(as.numeric(data_no_setosa$Species)-2, two_class(data_no_setosa))
```

```
## [1] 0.05949273
```

简而言之，n分类就是把二分类重复n次。


```r
data_is_setosa = mutate(data, Species = ifelse(Species == "setosa", 0, 1))
data_is_versicolor = mutate(data, Species = ifelse(Species == "versicolor", 0, 1))
data_is_virginica = mutate(data, Species = ifelse(Species == "virginica", 0, 1))

two_class(data_is_setosa)
```

```
## Warning: glm.fit: algorithm did not converge
```

```
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
```

```
##            1            2            3            4            5            6 
## 2.220446e-16 2.220446e-16 2.220446e-16 1.875686e-12 2.220446e-16 2.220446e-16 
##            7            8            9           10           11           12 
## 2.077262e-13 2.220446e-16 1.268389e-11 2.220446e-16 2.220446e-16 1.266230e-13 
##           13           14           15           16           17           18 
## 2.220446e-16 2.220446e-16 2.220446e-16 2.220446e-16 2.220446e-16 2.220446e-16 
##           19           20           21           22           23           24 
## 2.220446e-16 2.220446e-16 2.220446e-16 2.220446e-16 2.220446e-16 3.968396e-11 
##           25           26           27           28           29           30 
## 5.245006e-11 2.775502e-13 9.076879e-13 2.220446e-16 2.220446e-16 1.969335e-12 
##           31           32           33           34           35           36 
## 1.331002e-12 2.220446e-16 2.220446e-16 2.220446e-16 2.220446e-16 2.220446e-16 
##           37           38           39           40           41           42 
## 2.220446e-16 2.220446e-16 7.767584e-13 2.220446e-16 2.220446e-16 5.033679e-10 
##           43           44           45           46           47           48 
## 1.618733e-13 3.120309e-11 5.037930e-12 4.553236e-13 2.220446e-16 1.148666e-13 
##           49           50           51           52           53           54 
## 2.220446e-16 2.220446e-16 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##           55           56           57           58           59           60 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##           61           62           63           64           65           66 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##           67           68           69           70           71           72 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##           73           74           75           76           77           78 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##           79           80           81           82           83           84 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##           85           86           87           88           89           90 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##           91           92           93           94           95           96 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##           97           98           99          100          101          102 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##          103          104          105          106          107          108 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##          109          110          111          112          113          114 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##          115          116          117          118          119          120 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##          121          122          123          124          125          126 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##          127          128          129          130          131          132 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##          133          134          135          136          137          138 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##          139          140          141          142          143          144 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##          145          146          147          148          149          150 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00
```

```r
two_class(data_is_versicolor)
```

```
##          1          2          3          4          5          6          7 
## 0.91508678 0.71708213 0.82801733 0.73198542 0.93292480 0.97659676 0.90489939 
##          8          9         10         11         12         13         14 
## 0.87455330 0.62894618 0.69007968 0.94679577 0.85338449 0.65195952 0.71076006 
##         15         16         17         18         19         20         21 
## 0.98537301 0.99578893 0.98602837 0.93433221 0.96257651 0.96652246 0.85535651 
##         22         23         24         25         26         27         28 
## 0.96646344 0.95520481 0.90529428 0.79694417 0.66637629 0.91420835 0.90640934 
##         29         30         31         32         33         34         35 
## 0.89304874 0.76450653 0.71553621 0.93058069 0.97517261 0.98826215 0.74617711 
##         36         37         38         39         40         41         42 
## 0.85527797 0.93130409 0.91134267 0.71884097 0.87722046 0.94059041 0.32816243 
##         43         44         45         46         47         48         49 
## 0.81728097 0.96088927 0.95751567 0.76554646 0.95043481 0.80466977 0.94554618 
##         50         51         52         53         54         55         56 
## 0.85736072 0.73176309 0.80169711 0.67139461 0.22449792 0.54276509 0.38957228 
##         57         58         59         60         61         62         63 
## 0.84119709 0.26480262 0.48001142 0.55343880 0.08486774 0.75194940 0.09707761 
##         64         65         66         67         68         69         70 
## 0.48595238 0.72874996 0.73970106 0.65505546 0.26653438 0.19020107 0.25348721 
##         71         72         73         74         75         76         77 
## 0.84731131 0.57587291 0.24774872 0.29079401 0.56587107 0.67704460 0.42666361 
##         78         79         80         81         82         83         84 
## 0.69212565 0.61297198 0.37091722 0.22219378 0.19791172 0.45168126 0.35209499 
##         85         86         87         88         89         90         91 
## 0.64388465 0.89434765 0.71670119 0.17238522 0.64819734 0.33618752 0.23077053 
##         92         93         94         95         96         97         98 
## 0.58778493 0.35322319 0.21818490 0.41115082 0.55637471 0.55592485 0.55377949 
##         99        100        101        102        103        104        105 
## 0.49487549 0.51907745 0.92128497 0.54353665 0.69789142 0.48049108 0.75012175 
##        106        107        108        109        110        111        112 
## 0.51009989 0.40780938 0.32032711 0.20403096 0.96733885 0.88313482 0.51476423 
##        113        114        115        116        117        118        119 
## 0.78399944 0.49998454 0.86335239 0.92881951 0.59435952 0.92046379 0.29067444 
##        120        121        122        123        124        125        126 
## 0.10390241 0.89714680 0.72025710 0.28831708 0.57007603 0.86304725 0.61212629 
##        127        128        129        130        131        132        133 
## 0.66120010 0.74498856 0.62253140 0.40227498 0.38536752 0.91182791 0.68527934 
##        134        135        136        137        138        139        140 
## 0.36948806 0.11130750 0.78201219 0.95199319 0.65410628 0.76474800 0.84873120 
##        141        142        143        144        145        146        147 
## 0.90433846 0.93550181 0.54353665 0.86745626 0.95036600 0.90153107 0.46737587 
##        148        149        150 
## 0.79113474 0.95014869 0.68141028
```

```r
two_class(data_is_virginica)
```

```
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
```

```
##            1            2            3            4            5            6 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##            7            8            9           10           11           12 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##           13           14           15           16           17           18 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##           19           20           21           22           23           24 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##           25           26           27           28           29           30 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##           31           32           33           34           35           36 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##           37           38           39           40           41           42 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##           43           44           45           46           47           48 
## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 
##           49           50           51           52           53           54 
## 1.000000e+00 1.000000e+00 9.999883e-01 9.999514e-01 9.988014e-01 9.999578e-01 
##           55           56           57           58           59           60 
## 9.985915e-01 9.998981e-01 9.986943e-01 1.000000e+00 9.999854e-01 9.999852e-01 
##           61           62           63           64           65           66 
## 1.000000e+00 9.999626e-01 9.999999e-01 9.992011e-01 1.000000e+00 9.999972e-01 
##           67           68           69           70           71           72 
## 9.986740e-01 1.000000e+00 9.404018e-01 9.999999e-01 5.951619e-01 9.999997e-01 
##           73           74           75           76           77           78 
## 7.751662e-01 9.999598e-01 9.999986e-01 9.999929e-01 9.992876e-01 7.239383e-01 
##           79           80           81           82           83           84 
## 9.990348e-01 1.000000e+00 9.999999e-01 1.000000e+00 9.999999e-01 1.323701e-01 
##           85           86           87           88           89           90 
## 9.978308e-01 9.997870e-01 9.997020e-01 9.997449e-01 9.999992e-01 9.999889e-01 
##           91           92           93           94           95           96 
## 9.999603e-01 9.998404e-01 9.999996e-01 1.000000e+00 9.999850e-01 9.999997e-01 
##           97           98           99          100          101          102 
## 9.999969e-01 9.999977e-01 1.000000e+00 9.999977e-01 2.585234e-10 3.860921e-04 
##          103          104          105          106          107          108 
## 9.653910e-07 2.811503e-04 9.071537e-08 4.502238e-09 1.091877e-01 4.498827e-06 
##          109          110          111          112          113          114 
## 7.901235e-06 6.870636e-09 9.741562e-03 2.570987e-04 2.002431e-05 3.272223e-05 
##          115          116          117          118          119          120 
## 8.057896e-08 4.834130e-06 2.300631e-03 7.550917e-08 6.068062e-13 7.950771e-02 
##          121          122          123          124          125          126 
## 3.815786e-07 4.870092e-04 3.671479e-09 5.156610e-02 1.761615e-05 4.413947e-04 
##          127          128          129          130          131          132 
## 1.754560e-01 1.977010e-01 7.647259e-07 2.879872e-02 3.125252e-06 8.108555e-05 
##          133          134          135          136          137          138 
## 1.228427e-07 7.951259e-01 3.359534e-02 1.658605e-08 1.364130e-07 3.502736e-03 
##          139          140          141          142          143          144 
## 3.308575e-01 1.283021e-04 4.927955e-08 5.603902e-05 3.860921e-04 4.523713e-08 
##          145          146          147          148          149          150 
## 1.172982e-08 6.834793e-06 8.933153e-04 1.006087e-03 4.374956e-06 2.232115e-02
```

此时的概率就可以作为分类的依据，这种找最大值或者最小值的方法被称为hardmax，同理有softmax。softmax的形式是将概率指数化，是一种将概率归一化的方法，求导时softmax的梯度就是对应的softmax值，所以深度学习分类任务中常用softmax而非hardmax。无他，反向传播算的快。

## hardmax


```r
result_hardmax = tibble(
  is_setosa = two_class(data_is_setosa), 
  is_versicolor = two_class(data_is_versicolor), 
  is_virginica = two_class(data_is_virginica)
) %>% mutate( # pmap是一种按行操作的方法
  which = pmap_dbl(., ~ which.min(c(...))), 
  prob = pmap_dbl(., ~ c(...)[which.min(c(...))])
)
```

```
## Warning: glm.fit: algorithm did not converge
```

```
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
```

## softmax


```r
softmax = function(value, values) {
  exp(value) / sum(exp(values))
}

result_softmax = tibble(
  is_setosa = two_class(data_is_setosa), 
  is_versicolor = two_class(data_is_versicolor), 
  is_virginica = two_class(data_is_virginica)
  ) %>% mutate(
    is_setosa_so =  pmap_dbl(., ~ softmax(..1, c(..1, ..2, ..3))), 
    is_versicolor_so = pmap_dbl(., ~ softmax(..2, c(..1, ..2, ..3))), 
    is_virginica_so = pmap_dbl(., ~ softmax(..3, c(..1, ..2, ..3)))
  ) %>% select(contains("so")) %>% mutate(
    which = pmap_dbl(., ~ which.min(c(...))), 
    prob = pmap_dbl(., ~ c(...)[which.min(c(...))])
  )
```

```
## Warning: glm.fit: algorithm did not converge
```

```
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
```

多分类的评价指标是对数似然损失，交叉熵是对数似然损失的特殊形式。


```r
log_likelihood = function (truth, response) {
  -log(sum(truth * response))
}
```

交叉熵的完整写法


```r
truth = list(c(1, 0), c(0, 1))
response = list(c(0.9, 0.1), c(0.2, 0.8))
mean(map2_dbl(truth, response, log_likelihood))
```

```
## [1] 0.164252
```

二者等价


```r
truth = c(1, 0)
response = c(0.9, 0.2)
cross_entropy(truth, response)
```

```
## [1] 0.164252
```

## 多分类的损失函数


```r
truth = list(c(1, 0, 0), c(0, 1, 0))
response = list(c(0.85, 0.1, 0.05), c(0.15, 0.8, 0.05))
mean(map2_dbl(truth, response, log_likelihood))
```

```
## [1] 0.1928312
```

