[["index.html", "R语言高手计划 1 预备 1.1 工作环境 1.2 使用project管理项目 1.3 第一周 1.4 第二周 1.5 第三周 1.6 第四周", " R语言高手计划 Vinnish 普尔弥什 1 预备 1.1 工作环境 卸载掉不需要的软件后，确认工作环境清洁（最好将电脑的用户路径改为英文），安装编程语言：R软件、R的集成开发环境：Rstudio 学习如何使用R安装R的软件包。首先是学会配置Cran和Bioconductor的镜像。然后是尝试从cran、github、bioconductor三种途径，安装上tidyverse、mlr3verse、mlr3proba、annoprobe等包（bioconductor并不常用，只需要会用devtools的install_github和base的install.packages函数安装包即可）。 参考书目： R语言教程——基于tidyverse R语言教程——北大数院 mlr3教学ppt 1.2 使用project管理项目 再左上角的File里创建New Project，学学怎么用链接。 1.3 第一周 1.3.1 目标 第一周是学习常见文件拓展名比如txt、csv、tsv的意义。使用tidyverse的read_csv或原生的read.csv等函数读入文件为数据框，然后跟着参考书籍学习使用tidyverse的select、filter、mutate、across、group_by、summarise等函数，并且用r原生的方法实现以上这些函数的功能，主要是数据选择、筛选、修改、汇总四个部分。 1.3.2 示例及任务 示例见example1.R 在内置数据集airquality中，进行 查看每列的缺失值个数 使用每列的平均值对缺失值进行填补（注意mean函数和na） 汇总显示Ozone到Temp列每月的平均值 tidyverse是相对独立于base的R的一种方言，是一门专门操作文件（在R中表现为数据框）的语言，本章的目的在于希望来者能认识数据，认知掌握如何操作数据框，培养数据意识。 1.4 第二周 1.4.1 目标 第二周是对R的基础与特征的学习： r中的数值运算符号，如 + - * ** / //等 r中的逻辑运算符号，如 ! &amp; |等 r的控制结构，如if for循环（在数据科学中数量掌握这两个可以应对90%的情况） r的索引方法，如 $ [] [[]]等 r的特点，向量化（需要一定的线性代数知识，对矩阵乘法有个印象就够了），重点学习apply函数簇中的apply和lapply函数 内容较杂，希望一定要借助chatgpt来学习。 1.4.2 示例及任务 示例见example2.R 我设计了一个函数，目的是将每个位次上字符的频率视作概率，重新生成一定数量的序列，求问为什么函数一可以运行而函数二会报错？ data_task = readRDS(&quot;./示例及任务/data/示例数据.Rds&quot;) head(data_task) ## # A tibble: 6 × 14 ## A6 A5 A4 A3 A2 A1 G6 G5 G4 G3 G2 G1 ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 R A Q L S Q X A X L S X ## 2 A A Q L S Q A A X L S X ## 3 C A Q L S Q S A X L S X ## 4 D A Q L S Q N A X L S X ## 5 E A Q L S Q X A X L S X ## 6 F A Q L S Q F A X L S X ## # ℹ 2 more variables: Activity &lt;dbl&gt;, Selectivity &lt;dbl&gt; generator_1 = function(sample_space, num) { # 计算类别A中每个种类的构成 sample_list_1 = sample_space %&gt;% select(matches(&quot;A&quot;), -Activity) %&gt;% lapply(\\(vec) table(vec)/length(vec)) # 计算类别G中每个种类的构成 sample_list_2 = sample_space %&gt;% select(matches(&quot;G&quot;)) %&gt;% lapply(\\(vec) table(vec)/length(vec)) # 采用递归方法连接字符串 seq1 = Reduce(paste0, lapply(sample_list_1, \\(.x) sample(names(.x), num, T, .x))) seq2 = Reduce(paste0, lapply(sample_list_2, \\(.x) sample(names(.x), num, T, .x))) list(Sequence1 = seq1, Sequnce2 = seq2) } # 为什么1能运行2运行不了呢？ generator_2 = function(sample_space, num) { # 计算类别A中每个种类的构成 sample_list_1 = sample_space %&gt;% select(matches(&quot;A&quot;), -Activity) %&gt;% apply(2, \\(vec) table(vec)/length(vec)) # 计算类别G中每个种类的构成 sample_list_2 = sample_space %&gt;% select(matches(&quot;G&quot;)) %&gt;% apply(2, \\(vec) table(vec)/length(vec)) # 采用递归方法连接字符串 seq1 = Reduce(paste0, lapply(sample_list_1, \\(.x) sample(names(.x), num, T, .x))) seq2 = Reduce(paste0, lapply(sample_list_2, \\(.x) sample(names(.x), num, T, .x))) list(Sequence1 = seq1, Sequnce2 = seq2) } 本章的目的是令来者学习R的基本数据结构和控制结构，属于Rbase的部分，非常基础。 1.5 第三周 1.5.1 目标 第三周主要是浅尝R中的各类函数与实战 R中的字符串操作。包括： base中的paste, paste0, grep与grepl函数、stringr中的str_c, str_dectet, str_sub, str_extract等函数。 熟悉正则表达式（不需要了解，只需要对相关概念有印象）。（3）熟悉with函数的变量掩蔽概念（不需要了解，只需要知道为什么有的地方可以不用字符串）。 R中的建模函数。包括： 学习base中的公式语法 学习lm与glm函数，尝试使用lm进行简单线性回归，使用glm进行逻辑回归。 另外，这周还需要学习部分理论知识，包括： 回归与分类的概念。熟悉简单线性回归，多元线性回归，逻辑回归和k近邻分类（不需要了解和推导，只需要建立起概念）。参考书目为台大生物统计课程和课本回归分析（当然也可以自己搜集资料）。 另外，线性代数的学习也需要加紧。 1.5.2 示例及任务 示例见example3.R 有数据形如下 按要求要求整理成如下形式： 只保留编码区域（CDS）内的基因，或者一段基因的转录本（== “transcript”或== “CDS”） 提取基因id、基因name、转录本id 只保留每个基因中长度最长的片段，无论是CDS还是transcript 本人给出的任务和示例是R语言基础的部分提纲。之后对于R语言除巩固基础外，更多的是在实战中熟悉R语言生态（评价一个人代码能力的不是一个人基础多好，而是这个人对这门语言的生态有多了解、会用多少的包、脑中有多少编程范式，从而高效的解决自己遇到的问题。但是对于基础的掌握会让你更快熟悉另一门语言，因为基础总是相通的。 1.6 第四周 1.6.1 目标 第四周的学习任务主要是： 熟悉决策树这类树模型，熟悉基于树的统计学习方法，包括随机森林和GBDT模型。 熟悉主要的预测结果的评价指标，包括回归的指标mse、rmse，分类的指标信息熵、交叉熵等。着重学习二分类的分类指标ROC曲线。 熟悉mlr3机器学习框架。学会创建task、学习器，简单的训练学习器并预测。 1.6.2 示例及任务 示例见example4.R，example5.R example4.R是我总结的学习mlr3基础期间的常见误区，值得学习！ 限时挑战（本周）：设计r程序，计算roc曲线的auc（参考r语言教程基于tidyverse中序章部分和task3）。 如果学会了简单（多元）线性回归和逻辑回归（最简单的回归和分类任务），知道从表格型数据得到回归或分类预测结果的数据形式，那么就使用更复杂的模型而言，就不存在问题了。 "],["用tidyr读写操作文件.html", "2 用tidyr读写、操作文件 2.1 使用内置数据集iris 2.2 %&gt;% 的使用 2.3 select的使用 2.4 mutate的使用 2.5 group_by + summarise 2.6 检验环节", " 2 用tidyr读写、操作文件 2.1 使用内置数据集iris head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa # tibble是tidyverse的dataframe，是dataframe的超集 data = as_tibble(iris) data ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # ℹ 140 more rows 2.2 %&gt;% 的使用 %&gt;%名为管道符，是tidyr的重要组件,其作用是将管道符前的数据传递给管道符后的函数的第一个参数,快捷键是 Ctrl + Shift + M。 data %&gt;% View() # 这里等同于 View(data) 在单个函数时管道符的作用并不明显，但是在操作数据框时，我们往往需要多个步骤。使用管道符可以减少中间变量的个数，但是注意不要过分依赖管道。 以下是分组汇总每种花各个特征的平均数，使用管道显得简洁明了。 data %&gt;% group_by(Species) %&gt;% summarise(across(everything(), mean)) ## # A tibble: 3 × 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 0.246 ## 2 versicolor 5.94 2.77 4.26 1.33 ## 3 virginica 6.59 2.97 5.55 2.03 2.3 select的使用 select的作用是选择一列或多列 data %&gt;% select(Species) ## # A tibble: 150 × 1 ## Species ## &lt;fct&gt; ## 1 setosa ## 2 setosa ## 3 setosa ## 4 setosa ## 5 setosa ## 6 setosa ## 7 setosa ## 8 setosa ## 9 setosa ## 10 setosa ## # ℹ 140 more rows 选择iris中所有的数值列的列名,sapply在example2中有介绍。 features = colnames(data)[sapply(data, class) == &quot;numeric&quot;] 注意这个弹出警告,select内只能选择列名，不能接受一个变量。 data %&gt;% select(features) ## Warning: Using an external vector in selections was deprecated in tidyselect 1.1.0. ## ℹ Please use `all_of()` or `any_of()` instead. ## # Was: ## data %&gt;% select(features) ## ## # Now: ## data %&gt;% select(all_of(features)) ## ## See &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## # A tibble: 150 × 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 4.6 3.4 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # ℹ 140 more rows 如果我们想正确的查找一个变量内的若干列,应该用all_of等函数括起来。 data %&gt;% select(all_of(features)) ## # A tibble: 150 × 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 4.6 3.4 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # ℹ 140 more rows 除了all_of外，还有诸如any_of的其他函数，请自行翻阅学习 如果只能这么做的话那tidyverse也称不上强大,毕竟原生的dataframe也支持变量索引。 iris[, features] ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3.0 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5.0 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 4.6 3.4 1.4 0.3 ## 8 5.0 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## 11 5.4 3.7 1.5 0.2 ## 12 4.8 3.4 1.6 0.2 ## 13 4.8 3.0 1.4 0.1 ## 14 4.3 3.0 1.1 0.1 ## 15 5.8 4.0 1.2 0.2 ## 16 5.7 4.4 1.5 0.4 ## 17 5.4 3.9 1.3 0.4 ## 18 5.1 3.5 1.4 0.3 ## 19 5.7 3.8 1.7 0.3 ## 20 5.1 3.8 1.5 0.3 ## 21 5.4 3.4 1.7 0.2 ## 22 5.1 3.7 1.5 0.4 ## 23 4.6 3.6 1.0 0.2 ## 24 5.1 3.3 1.7 0.5 ## 25 4.8 3.4 1.9 0.2 ## 26 5.0 3.0 1.6 0.2 ## 27 5.0 3.4 1.6 0.4 ## 28 5.2 3.5 1.5 0.2 ## 29 5.2 3.4 1.4 0.2 ## 30 4.7 3.2 1.6 0.2 ## 31 4.8 3.1 1.6 0.2 ## 32 5.4 3.4 1.5 0.4 ## 33 5.2 4.1 1.5 0.1 ## 34 5.5 4.2 1.4 0.2 ## 35 4.9 3.1 1.5 0.2 ## 36 5.0 3.2 1.2 0.2 ## 37 5.5 3.5 1.3 0.2 ## 38 4.9 3.6 1.4 0.1 ## 39 4.4 3.0 1.3 0.2 ## 40 5.1 3.4 1.5 0.2 ## 41 5.0 3.5 1.3 0.3 ## 42 4.5 2.3 1.3 0.3 ## 43 4.4 3.2 1.3 0.2 ## 44 5.0 3.5 1.6 0.6 ## 45 5.1 3.8 1.9 0.4 ## 46 4.8 3.0 1.4 0.3 ## 47 5.1 3.8 1.6 0.2 ## 48 4.6 3.2 1.4 0.2 ## 49 5.3 3.7 1.5 0.2 ## 50 5.0 3.3 1.4 0.2 ## 51 7.0 3.2 4.7 1.4 ## 52 6.4 3.2 4.5 1.5 ## 53 6.9 3.1 4.9 1.5 ## 54 5.5 2.3 4.0 1.3 ## 55 6.5 2.8 4.6 1.5 ## 56 5.7 2.8 4.5 1.3 ## 57 6.3 3.3 4.7 1.6 ## 58 4.9 2.4 3.3 1.0 ## 59 6.6 2.9 4.6 1.3 ## 60 5.2 2.7 3.9 1.4 ## 61 5.0 2.0 3.5 1.0 ## 62 5.9 3.0 4.2 1.5 ## 63 6.0 2.2 4.0 1.0 ## 64 6.1 2.9 4.7 1.4 ## 65 5.6 2.9 3.6 1.3 ## 66 6.7 3.1 4.4 1.4 ## 67 5.6 3.0 4.5 1.5 ## 68 5.8 2.7 4.1 1.0 ## 69 6.2 2.2 4.5 1.5 ## 70 5.6 2.5 3.9 1.1 ## 71 5.9 3.2 4.8 1.8 ## 72 6.1 2.8 4.0 1.3 ## 73 6.3 2.5 4.9 1.5 ## 74 6.1 2.8 4.7 1.2 ## 75 6.4 2.9 4.3 1.3 ## 76 6.6 3.0 4.4 1.4 ## 77 6.8 2.8 4.8 1.4 ## 78 6.7 3.0 5.0 1.7 ## 79 6.0 2.9 4.5 1.5 ## 80 5.7 2.6 3.5 1.0 ## 81 5.5 2.4 3.8 1.1 ## 82 5.5 2.4 3.7 1.0 ## 83 5.8 2.7 3.9 1.2 ## 84 6.0 2.7 5.1 1.6 ## 85 5.4 3.0 4.5 1.5 ## 86 6.0 3.4 4.5 1.6 ## 87 6.7 3.1 4.7 1.5 ## 88 6.3 2.3 4.4 1.3 ## 89 5.6 3.0 4.1 1.3 ## 90 5.5 2.5 4.0 1.3 ## 91 5.5 2.6 4.4 1.2 ## 92 6.1 3.0 4.6 1.4 ## 93 5.8 2.6 4.0 1.2 ## 94 5.0 2.3 3.3 1.0 ## 95 5.6 2.7 4.2 1.3 ## 96 5.7 3.0 4.2 1.2 ## 97 5.7 2.9 4.2 1.3 ## 98 6.2 2.9 4.3 1.3 ## 99 5.1 2.5 3.0 1.1 ## 100 5.7 2.8 4.1 1.3 ## 101 6.3 3.3 6.0 2.5 ## 102 5.8 2.7 5.1 1.9 ## 103 7.1 3.0 5.9 2.1 ## 104 6.3 2.9 5.6 1.8 ## 105 6.5 3.0 5.8 2.2 ## 106 7.6 3.0 6.6 2.1 ## 107 4.9 2.5 4.5 1.7 ## 108 7.3 2.9 6.3 1.8 ## 109 6.7 2.5 5.8 1.8 ## 110 7.2 3.6 6.1 2.5 ## 111 6.5 3.2 5.1 2.0 ## 112 6.4 2.7 5.3 1.9 ## 113 6.8 3.0 5.5 2.1 ## 114 5.7 2.5 5.0 2.0 ## 115 5.8 2.8 5.1 2.4 ## 116 6.4 3.2 5.3 2.3 ## 117 6.5 3.0 5.5 1.8 ## 118 7.7 3.8 6.7 2.2 ## 119 7.7 2.6 6.9 2.3 ## 120 6.0 2.2 5.0 1.5 ## 121 6.9 3.2 5.7 2.3 ## 122 5.6 2.8 4.9 2.0 ## 123 7.7 2.8 6.7 2.0 ## 124 6.3 2.7 4.9 1.8 ## 125 6.7 3.3 5.7 2.1 ## 126 7.2 3.2 6.0 1.8 ## 127 6.2 2.8 4.8 1.8 ## 128 6.1 3.0 4.9 1.8 ## 129 6.4 2.8 5.6 2.1 ## 130 7.2 3.0 5.8 1.6 ## 131 7.4 2.8 6.1 1.9 ## 132 7.9 3.8 6.4 2.0 ## 133 6.4 2.8 5.6 2.2 ## 134 6.3 2.8 5.1 1.5 ## 135 6.1 2.6 5.6 1.4 ## 136 7.7 3.0 6.1 2.3 ## 137 6.3 3.4 5.6 2.4 ## 138 6.4 3.1 5.5 1.8 ## 139 6.0 3.0 4.8 1.8 ## 140 6.9 3.1 5.4 2.1 ## 141 6.7 3.1 5.6 2.4 ## 142 6.9 3.1 5.1 2.3 ## 143 5.8 2.7 5.1 1.9 ## 144 6.8 3.2 5.9 2.3 ## 145 6.7 3.3 5.7 2.5 ## 146 6.7 3.0 5.2 2.3 ## 147 6.3 2.5 5.0 1.9 ## 148 6.5 3.0 5.2 2.0 ## 149 6.2 3.4 5.4 2.3 ## 150 5.9 3.0 5.1 1.8 where、contains是dplyr里的函数,其作用是用于选择列，对列做判断,这些函数只能在select起作用。 下文中提到的across是一种特殊的select，所以也可以在里面用,这里只简单介绍where。 data %&gt;% select(where(is.numeric)) ## # A tibble: 150 × 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 4.6 3.4 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # ℹ 140 more rows R中!表示非门 !c(T, T, F) ## [1] FALSE FALSE TRUE 同时在select中可以用-来删除列,以下方法可以用于删除列。 data %&gt;% select(-Species) ## # A tibble: 150 × 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 4.6 3.4 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # ℹ 140 more rows data %&gt;% select(!Species) ## # A tibble: 150 × 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 4.6 3.4 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # ℹ 140 more rows data %&gt;% select(-where(is.numeric)) ## # A tibble: 150 × 1 ## Species ## &lt;fct&gt; ## 1 setosa ## 2 setosa ## 3 setosa ## 4 setosa ## 5 setosa ## 6 setosa ## 7 setosa ## 8 setosa ## 9 setosa ## 10 setosa ## # ℹ 140 more rows data %&gt;% select(!where(is.numeric)) ## # A tibble: 150 × 1 ## Species ## &lt;fct&gt; ## 1 setosa ## 2 setosa ## 3 setosa ## 4 setosa ## 5 setosa ## 6 setosa ## 7 setosa ## 8 setosa ## 9 setosa ## 10 setosa ## # ℹ 140 more rows select还支持一些其它的索引方法。 数字索引 data %&gt;% select(1:4) ## # A tibble: 150 × 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 4.6 3.4 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # ℹ 140 more rows 行名顺序索引 data %&gt;% select(Sepal.Length:Petal.Width) ## # A tibble: 150 × 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 4.6 3.4 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # ℹ 140 more rows 有的时候我们想什么都不做,everything可以用来调换列的位置，书中有详解。 data %&gt;% select(everything()) ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # ℹ 140 more rows 2.4 mutate的使用 mutate是对列进行增改的函数 # 增添 data %&gt;% mutate(new = 1) ## # A tibble: 150 × 6 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species new ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 setosa 1 ## 2 4.9 3 1.4 0.2 setosa 1 ## 3 4.7 3.2 1.3 0.2 setosa 1 ## 4 4.6 3.1 1.5 0.2 setosa 1 ## 5 5 3.6 1.4 0.2 setosa 1 ## 6 5.4 3.9 1.7 0.4 setosa 1 ## 7 4.6 3.4 1.4 0.3 setosa 1 ## 8 5 3.4 1.5 0.2 setosa 1 ## 9 4.4 2.9 1.4 0.2 setosa 1 ## 10 4.9 3.1 1.5 0.1 setosa 1 ## # ℹ 140 more rows # 修改 data %&gt;% mutate(Species = 1) ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 1 ## 2 4.9 3 1.4 0.2 1 ## 3 4.7 3.2 1.3 0.2 1 ## 4 4.6 3.1 1.5 0.2 1 ## 5 5 3.6 1.4 0.2 1 ## 6 5.4 3.9 1.7 0.4 1 ## 7 4.6 3.4 1.4 0.3 1 ## 8 5 3.4 1.5 0.2 1 ## 9 4.4 2.9 1.4 0.2 1 ## 10 4.9 3.1 1.5 0.1 1 ## # ℹ 140 more rows # 变换 data %&gt;% mutate(sum = Sepal.Length + Sepal.Width) ## # A tibble: 150 × 6 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species sum ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 setosa 8.6 ## 2 4.9 3 1.4 0.2 setosa 7.9 ## 3 4.7 3.2 1.3 0.2 setosa 7.9 ## 4 4.6 3.1 1.5 0.2 setosa 7.7 ## 5 5 3.6 1.4 0.2 setosa 8.6 ## 6 5.4 3.9 1.7 0.4 setosa 9.3 ## 7 4.6 3.4 1.4 0.3 setosa 8 ## 8 5 3.4 1.5 0.2 setosa 8.4 ## 9 4.4 2.9 1.4 0.2 setosa 7.3 ## 10 4.9 3.1 1.5 0.1 setosa 8 ## # ℹ 140 more rows 有的时候我们想只修改特定条件下的列,这时候需要借助ifelse进行判断。 ifelse咋用请搜索，?ifelse可以在右下角弹出窗口查阅文档。 data %&gt;% mutate(Sepal.Length_new = ifelse(Species == &quot;setosa&quot;, Sepal.Length + 10, Sepal.Length)) ## # A tibble: 150 × 6 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Sepal.Length_new ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 setosa 15.1 ## 2 4.9 3 1.4 0.2 setosa 14.9 ## 3 4.7 3.2 1.3 0.2 setosa 14.7 ## 4 4.6 3.1 1.5 0.2 setosa 14.6 ## 5 5 3.6 1.4 0.2 setosa 15 ## 6 5.4 3.9 1.7 0.4 setosa 15.4 ## 7 4.6 3.4 1.4 0.3 setosa 14.6 ## 8 5 3.4 1.5 0.2 setosa 15 ## 9 4.4 2.9 1.4 0.2 setosa 14.4 ## 10 4.9 3.1 1.5 0.1 setosa 14.9 ## # ℹ 140 more rows 有的时候我们相对多列进行相同的操作,难道我们要一个一个写出来吗？ data %&gt;% mutate(Sepal.Length = Sepal.Length ** 2, Sepal.Width = Sepal.Width ** 2, Petal.Length = Petal.Length ** 2, Petal.Width = Petal.Width ** 2) ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 26.0 12.2 1.96 0.04 setosa ## 2 24.0 9 1.96 0.04 setosa ## 3 22.1 10.2 1.69 0.04 setosa ## 4 21.2 9.61 2.25 0.04 setosa ## 5 25 13.0 1.96 0.04 setosa ## 6 29.2 15.2 2.89 0.16 setosa ## 7 21.2 11.6 1.96 0.09 setosa ## 8 25 11.6 2.25 0.04 setosa ## 9 19.4 8.41 1.96 0.04 setosa ## 10 24.0 9.61 2.25 0.01 setosa ## # ℹ 140 more rows 答案是no,我们应该用across，across相当于在mutate内应用select。 千万注意是where(is.numeric)不是where(is.numeric())，至于为什么就不能加那个括号在example2中有所涉及。 之所以这个函数写法那么怪，在example2中有所涉及。 square = function(vec) vec ** 2 data %&gt;% mutate(across(where(is.numeric), square)) ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 26.0 12.2 1.96 0.04 setosa ## 2 24.0 9 1.96 0.04 setosa ## 3 22.1 10.2 1.69 0.04 setosa ## 4 21.2 9.61 2.25 0.04 setosa ## 5 25 13.0 1.96 0.04 setosa ## 6 29.2 15.2 2.89 0.16 setosa ## 7 21.2 11.6 1.96 0.09 setosa ## 8 25 11.6 2.25 0.04 setosa ## 9 19.4 8.41 1.96 0.04 setosa ## 10 24.0 9.61 2.25 0.01 setosa ## # ℹ 140 more rows 更常见的是匿名函数写法,匿名函数就是没有名字的临时函数,括号里面的是临时变量名，随便起。 data %&gt;% mutate(across(where(is.numeric), function(vec) vec ** 2)) ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 26.0 12.2 1.96 0.04 setosa ## 2 24.0 9 1.96 0.04 setosa ## 3 22.1 10.2 1.69 0.04 setosa ## 4 21.2 9.61 2.25 0.04 setosa ## 5 25 13.0 1.96 0.04 setosa ## 6 29.2 15.2 2.89 0.16 setosa ## 7 21.2 11.6 1.96 0.09 setosa ## 8 25 11.6 2.25 0.04 setosa ## 9 19.4 8.41 1.96 0.04 setosa ## 10 24.0 9.61 2.25 0.01 setosa ## # ℹ 140 more rows # \\()是匿名函数中function()的简写 data %&gt;% mutate(across(where(is.numeric), \\(vec) vec ** 2)) ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 26.0 12.2 1.96 0.04 setosa ## 2 24.0 9 1.96 0.04 setosa ## 3 22.1 10.2 1.69 0.04 setosa ## 4 21.2 9.61 2.25 0.04 setosa ## 5 25 13.0 1.96 0.04 setosa ## 6 29.2 15.2 2.89 0.16 setosa ## 7 21.2 11.6 1.96 0.09 setosa ## 8 25 11.6 2.25 0.04 setosa ## 9 19.4 8.41 1.96 0.04 setosa ## 10 24.0 9.61 2.25 0.01 setosa ## # ℹ 140 more rows ~和.x是tidyverse中的新型匿名函数形式,它省略了为变量命名的步骤，认为变量就应该叫.x。 # 等同于\\(.x)，写起来简约一些 data %&gt;% mutate(across(where(is.numeric), ~ .x ** 2)) ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 26.0 12.2 1.96 0.04 setosa ## 2 24.0 9 1.96 0.04 setosa ## 3 22.1 10.2 1.69 0.04 setosa ## 4 21.2 9.61 2.25 0.04 setosa ## 5 25 13.0 1.96 0.04 setosa ## 6 29.2 15.2 2.89 0.16 setosa ## 7 21.2 11.6 1.96 0.09 setosa ## 8 25 11.6 2.25 0.04 setosa ## 9 19.4 8.41 1.96 0.04 setosa ## 10 24.0 9.61 2.25 0.01 setosa ## # ℹ 140 more rows 回到上面那个问题，如果我们非想在where里的函数加上括号，也应该用匿名函数。 data %&gt;% mutate(across(where(~ is.numeric(.x)), ~ .x ** 2)) ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 26.0 12.2 1.96 0.04 setosa ## 2 24.0 9 1.96 0.04 setosa ## 3 22.1 10.2 1.69 0.04 setosa ## 4 21.2 9.61 2.25 0.04 setosa ## 5 25 13.0 1.96 0.04 setosa ## 6 29.2 15.2 2.89 0.16 setosa ## 7 21.2 11.6 1.96 0.09 setosa ## 8 25 11.6 2.25 0.04 setosa ## 9 19.4 8.41 1.96 0.04 setosa ## 10 24.0 9.61 2.25 0.01 setosa ## # ℹ 140 more rows data %&gt;% mutate(across(where(\\(.x) is.numeric(.x)), ~ .x ** 2)) ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 26.0 12.2 1.96 0.04 setosa ## 2 24.0 9 1.96 0.04 setosa ## 3 22.1 10.2 1.69 0.04 setosa ## 4 21.2 9.61 2.25 0.04 setosa ## 5 25 13.0 1.96 0.04 setosa ## 6 29.2 15.2 2.89 0.16 setosa ## 7 21.2 11.6 1.96 0.09 setosa ## 8 25 11.6 2.25 0.04 setosa ## 9 19.4 8.41 1.96 0.04 setosa ## 10 24.0 9.61 2.25 0.01 setosa ## # ℹ 140 more rows 我们可以这么理解，一旦一个函数加了括号，意思就是我们打算立即执行这个函数。 function_test = function() features function_test() ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; data %&gt;% select(all_of(function_test())) ## # A tibble: 150 × 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 4.6 3.4 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # ℹ 140 more rows 2.5 group_by + summarise group_by和summarise是对分组数据进行描述的方法,就是先分组，再汇总。 data %&gt;% group_by(Species) %&gt;% summarise(Sepal.Length_mean = mean(Sepal.Length)) ## # A tibble: 3 × 2 ## Species Sepal.Length_mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 5.01 ## 2 versicolor 5.94 ## 3 virginica 6.59 同理，summarise里也可以用across。 data %&gt;% group_by(Species) %&gt;% summarise(across(everything(), mean)) ## # A tibble: 3 × 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 0.246 ## 2 versicolor 5.94 2.77 4.26 1.33 ## 3 virginica 6.59 2.97 5.55 2.03 同理，summarise里也可以用where匿名函数。 data %&gt;% group_by(Species) %&gt;% summarise(across(where(is.numeric), ~ mean(.x))) ## # A tibble: 3 × 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 0.246 ## 2 versicolor 5.94 2.77 4.26 1.33 ## 3 virginica 6.59 2.97 5.55 2.03 简单做个正态性检验。 data %&gt;% group_by(Species) %&gt;% summarise(across(where(is.numeric), ~ shapiro.test(.x)$p.value)) ## # A tibble: 3 × 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 0.460 0.272 0.0548 0.000000866 ## 2 versicolor 0.465 0.338 0.158 0.0273 ## 3 virginica 0.258 0.181 0.110 0.0870 group_nest是group_by的一种变体，但是用法和group_by完全不一样,group_nest利用了tibble的神奇特性，dataframe里可以嵌套列表或另一个dataframe，但是这种操作过于复杂了，并且不够通用。书中有详解，这里不过多介绍。 lm是R中用于建模的函数，下面第一个~是匿名函数，第二个函数是R的公式写法 data %&gt;% group_nest(Species) %&gt;% mutate(models = map(data, ~ lm(Sepal.Length ~ ., data = .x))) ## # A tibble: 3 × 3 ## Species data models ## &lt;fct&gt; &lt;list&lt;tibble[,4]&gt;&gt; &lt;list&gt; ## 1 setosa [50 × 4] &lt;lm&gt; ## 2 versicolor [50 × 4] &lt;lm&gt; ## 3 virginica [50 × 4] &lt;lm&gt; 2.6 检验环节 data(&quot;diamonds&quot;) # 生成不定比例的整数 random_int = function(ratio_min, size) { ratio = runif(1, ratio_min, 3*ratio_min) result = unique(round(runif(round(ratio*size), min = 1, max = size))) return(result) } # 计算众数 cal_mode = function(vec) { result = names(which.max(table(vec))) if (is.integer(vec)) { return(as.integer(result)) } else { return(result) } } # 制造缺失值 ## 波浪号是tidyr中匿名函数的简约写法，等同于function(x) rows = nrow(diamonds) data = diamonds %&gt;% select(carat:price) %&gt;% mutate(index = 1:nrow(.), across(everything(), ~ ifelse(index %in% random_int(0.1, rows), NA, .x))) %&gt;% select(-index) # 对整数列的缺失值进行众数插补，对浮点数列的缺失值进行均值插补 data_interpolated = data %&gt;% mutate(across(where(is.integer), ~ ifelse(is.na(.x), cal_mode(.x), .x)), across(where(is.double), ~ ifelse(is.na(.x), mean(.x, na.rm = T), .x))) 汇总数据，包括所有浮点数的均值，每种cut的个数。注意为什么一个用了匿名函数，一个没有用匿名函数。 data %&gt;% select(cut, where(is.double)) %&gt;% group_by(cut) %&gt;% summarise(n = n(), across(everything(), ~ mean(.x, na.rm = T))) ## # A tibble: 6 × 5 ## cut n carat depth table ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1433 1.05 64.1 59.0 ## 2 2 4279 0.849 62.4 58.7 ## 3 3 10637 0.808 61.8 58.0 ## 4 4 12114 0.891 61.3 58.7 ## 5 5 18975 0.704 61.7 55.9 ## 6 NA 6502 0.794 61.8 57.4 data_interpolated %&gt;% select(cut, where(is.double)) %&gt;% group_by(cut) %&gt;% summarise(num = n(), across(everything(), mean)) ## # A tibble: 5 × 5 ## cut num carat depth table ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1433 1.01 63.8 58.9 ## 2 2 4279 0.841 62.3 58.5 ## 3 3 10637 0.807 61.8 57.9 ## 4 4 12114 0.878 61.3 58.6 ## 5 5 25477 0.737 61.7 56.5 "],["r的基础和特征.html", "3 R的基础和特征 3.1 R中的数据结构 3.2 向量化 3.3 for循环 3.4 for的向量化 3.5 标准化 3.6 求单个变量和其它变量的相关性 3.7 函数式编程 3.8 实例和小测验", " 3 R的基础和特征 3.1 R中的数据结构 R中最基本的数据格式是向量,R中事实上没有单个的数据格式比如字符串、数值,单个是数据实质上都是一维向量。 class(&quot;class&quot;) ## [1] &quot;character&quot; class(c(&quot;class&quot;)) ## [1] &quot;character&quot; class(c(&quot;class1&quot;, &quot;class2&quot;)) ## [1] &quot;character&quot; class(1) ## [1] &quot;numeric&quot; class(c(1)) ## [1] &quot;numeric&quot; 向量是同质性的，如果格式不同则强制转格式。 c(1, &quot;class&quot;) ## [1] &quot;1&quot; &quot;class&quot; 列表是特殊的向量（异质性向量） list(1, &quot;class&quot;) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] &quot;class&quot; list(c(1, 2), &quot;class&quot;) ## [[1]] ## [1] 1 2 ## ## [[2]] ## [1] &quot;class&quot; 向量和列表的每一个元素都可以拥有名字,可以通过名字索引，所以不能重名。 vec_exp = c(age = 1, class = &quot;class&quot;) vec_exp[[&quot;age&quot;]] ## [1] &quot;1&quot; 注意以下两者的区别 vec_exp[[1]] ## [1] &quot;1&quot; vec_exp[1] ## age ## &quot;1&quot; # 向量中无法使用$ # vec_exp$age lst_exp = list(age = 1, class = &quot;class&quot;) 注意以下两者的区别 lst_exp[[1]] ## [1] 1 lst_exp[1] ## $age ## [1] 1 以下三者等价 lst_exp[[&quot;age&quot;]] == lst_exp$age ## [1] TRUE lst_exp$age ## [1] 1 lst_exp[1]$age ## [1] 1 使用names调用向量和列表的名字。 names(lst_exp) ## [1] &quot;age&quot; &quot;class&quot; dataframe是特殊的列表,列表中每个元素都是等长的向量,dataframe的每一列都是一个向量。 data.frame(list(age = c(1, 2), class = c(&quot;class1&quot;, &quot;class2&quot;))) ## age class ## 1 1 class1 ## 2 2 class2 data_exp = data.frame(age = c(1, 2), class = c(&quot;class1&quot;, &quot;class2&quot;)) 所以基本上可以用操作列表的方法操作dataframe。 var = &quot;age&quot; data_exp$age ## [1] 1 2 data_exp[[&quot;age&quot;]] ## [1] 1 2 data_exp[[var]] ## [1] 1 2 尤其注意以下两者的区别 data_exp[1] ## age ## 1 1 ## 2 2 data_exp[[1]] ## [1] 1 2 dataframe特有的方法是[x, y]行列索引,空格为全选。 data_exp[, 1] ## [1] 1 2 data_exp[1, ] ## age class ## 1 1 class1 由于历史问题，下两者属性不同,但是在tidyverse的数据表tibble中这个问题不再。 class(data_exp[1, ]) ## [1] &quot;data.frame&quot; class(data_exp[, 1]) ## [1] &quot;numeric&quot; dataframe的名同列表，但不同的是多了行名属性,行名在tibble中并不常用。 names(data_exp) ## [1] &quot;age&quot; &quot;class&quot; rownames(data_exp) ## [1] &quot;1&quot; &quot;2&quot; 3.2 向量化 向量化是R最典型的特征,不是说别的语言没有。 矩阵运算 c(1, 2) + c(2, 3) ## [1] 3 5 c(1, 2) * c(2, 3) ## [1] 2 6 matrix(1:4, nrow = 2) %*% c(1, 2) ## [,1] ## [1,] 7 ## [2,] 10 除了矩阵乘法，更多是作为泛函。 data_exp %&gt;% apply(2, length) ## age class ## 2 2 rep(c(1, 2), c(3, 5)) ## [1] 1 1 1 2 2 2 2 2 最常用的tidyr其实就是向量化的一种体现。 data_exp %&gt;% mutate(gender = c(&quot;boy&quot;, &quot;boy&quot;), location = paste(gender, &quot;of&quot;, class)) ## age class gender location ## 1 1 class1 boy boy of class1 ## 2 2 class2 boy boy of class2 3.3 for循环 r中的for循环并不慢，之所以名声不好，是因为for循环太简单了，容易写出效率不高的冗余代码。 for (i in 1:10) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 注意这里的{}，其实没有也行,{}的作用是新创建一个空间，就是把一整段代码变成看起来只有一段代码。 for (i in 1:10) print(i) ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 匿名函数就是这个原理，有花括号的地方都可以这么省。 sapply(1:10, function(x) x + 1) ## [1] 2 3 4 5 6 7 8 9 10 11 if (T) print(&quot;TRUE&quot;) ## [1] &quot;TRUE&quot; 3.4 for的向量化 result = c() for (i in 1:ncol(data_exp)) { result[i] = length(data_exp[[i]]) } result ## [1] 2 2 哪些情况用for循环，哪些情况用apply，随你便，但是apply在简单情形可以少写几行代码，而且apply函数簇更容易并行加速，因为apply的操作单位有且只有单个对象。 3.5 标准化 iris %&gt;% select(where(is.numeric)) %&gt;% apply(2, scale) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## [1,] -0.89767388 1.01560199 -1.33575163 -1.3110521482 ## [2,] -1.13920048 -0.13153881 -1.33575163 -1.3110521482 ## [3,] -1.38072709 0.32731751 -1.39239929 -1.3110521482 ## [4,] -1.50149039 0.09788935 -1.27910398 -1.3110521482 ## [5,] -1.01843718 1.24503015 -1.33575163 -1.3110521482 ## [6,] -0.53538397 1.93331463 -1.16580868 -1.0486667950 ## [7,] -1.50149039 0.78617383 -1.33575163 -1.1798594716 ## [8,] -1.01843718 0.78617383 -1.27910398 -1.3110521482 ## [9,] -1.74301699 -0.36096697 -1.33575163 -1.3110521482 ## [10,] -1.13920048 0.09788935 -1.27910398 -1.4422448248 ## [11,] -0.53538397 1.47445831 -1.27910398 -1.3110521482 ## [12,] -1.25996379 0.78617383 -1.22245633 -1.3110521482 ## [13,] -1.25996379 -0.13153881 -1.33575163 -1.4422448248 ## [14,] -1.86378030 -0.13153881 -1.50569459 -1.4422448248 ## [15,] -0.05233076 2.16274279 -1.44904694 -1.3110521482 ## [16,] -0.17309407 3.08045544 -1.27910398 -1.0486667950 ## [17,] -0.53538397 1.93331463 -1.39239929 -1.0486667950 ## [18,] -0.89767388 1.01560199 -1.33575163 -1.1798594716 ## [19,] -0.17309407 1.70388647 -1.16580868 -1.1798594716 ## [20,] -0.89767388 1.70388647 -1.27910398 -1.1798594716 ## [21,] -0.53538397 0.78617383 -1.16580868 -1.3110521482 ## [22,] -0.89767388 1.47445831 -1.27910398 -1.0486667950 ## [23,] -1.50149039 1.24503015 -1.56234224 -1.3110521482 ## [24,] -0.89767388 0.55674567 -1.16580868 -0.9174741184 ## [25,] -1.25996379 0.78617383 -1.05251337 -1.3110521482 ## [26,] -1.01843718 -0.13153881 -1.22245633 -1.3110521482 ## [27,] -1.01843718 0.78617383 -1.22245633 -1.0486667950 ## [28,] -0.77691058 1.01560199 -1.27910398 -1.3110521482 ## [29,] -0.77691058 0.78617383 -1.33575163 -1.3110521482 ## [30,] -1.38072709 0.32731751 -1.22245633 -1.3110521482 ## [31,] -1.25996379 0.09788935 -1.22245633 -1.3110521482 ## [32,] -0.53538397 0.78617383 -1.27910398 -1.0486667950 ## [33,] -0.77691058 2.39217095 -1.27910398 -1.4422448248 ## [34,] -0.41462067 2.62159911 -1.33575163 -1.3110521482 ## [35,] -1.13920048 0.09788935 -1.27910398 -1.3110521482 ## [36,] -1.01843718 0.32731751 -1.44904694 -1.3110521482 ## [37,] -0.41462067 1.01560199 -1.39239929 -1.3110521482 ## [38,] -1.13920048 1.24503015 -1.33575163 -1.4422448248 ## [39,] -1.74301699 -0.13153881 -1.39239929 -1.3110521482 ## [40,] -0.89767388 0.78617383 -1.27910398 -1.3110521482 ## [41,] -1.01843718 1.01560199 -1.39239929 -1.1798594716 ## [42,] -1.62225369 -1.73753594 -1.39239929 -1.1798594716 ## [43,] -1.74301699 0.32731751 -1.39239929 -1.3110521482 ## [44,] -1.01843718 1.01560199 -1.22245633 -0.7862814418 ## [45,] -0.89767388 1.70388647 -1.05251337 -1.0486667950 ## [46,] -1.25996379 -0.13153881 -1.33575163 -1.1798594716 ## [47,] -0.89767388 1.70388647 -1.22245633 -1.3110521482 ## [48,] -1.50149039 0.32731751 -1.33575163 -1.3110521482 ## [49,] -0.65614727 1.47445831 -1.27910398 -1.3110521482 ## [50,] -1.01843718 0.55674567 -1.33575163 -1.3110521482 ## [51,] 1.39682886 0.32731751 0.53362088 0.2632599711 ## [52,] 0.67224905 0.32731751 0.42032558 0.3944526477 ## [53,] 1.27606556 0.09788935 0.64691619 0.3944526477 ## [54,] -0.41462067 -1.73753594 0.13708732 0.1320672944 ## [55,] 0.79301235 -0.59039513 0.47697323 0.3944526477 ## [56,] -0.17309407 -0.59039513 0.42032558 0.1320672944 ## [57,] 0.55148575 0.55674567 0.53362088 0.5256453243 ## [58,] -1.13920048 -1.50810778 -0.25944625 -0.2615107354 ## [59,] 0.91377565 -0.36096697 0.47697323 0.1320672944 ## [60,] -0.77691058 -0.81982329 0.08043967 0.2632599711 ## [61,] -1.01843718 -2.42582042 -0.14615094 -0.2615107354 ## [62,] 0.06843254 -0.13153881 0.25038262 0.3944526477 ## [63,] 0.18919584 -1.96696410 0.13708732 -0.2615107354 ## [64,] 0.30995914 -0.36096697 0.53362088 0.2632599711 ## [65,] -0.29385737 -0.36096697 -0.08950329 0.1320672944 ## [66,] 1.03453895 0.09788935 0.36367793 0.2632599711 ## [67,] -0.29385737 -0.13153881 0.42032558 0.3944526477 ## [68,] -0.05233076 -0.81982329 0.19373497 -0.2615107354 ## [69,] 0.43072244 -1.96696410 0.42032558 0.3944526477 ## [70,] -0.29385737 -1.27867961 0.08043967 -0.1303180588 ## [71,] 0.06843254 0.32731751 0.59026853 0.7880306775 ## [72,] 0.30995914 -0.59039513 0.13708732 0.1320672944 ## [73,] 0.55148575 -1.27867961 0.64691619 0.3944526477 ## [74,] 0.30995914 -0.59039513 0.53362088 0.0008746178 ## [75,] 0.67224905 -0.36096697 0.30703027 0.1320672944 ## [76,] 0.91377565 -0.13153881 0.36367793 0.2632599711 ## [77,] 1.15530226 -0.59039513 0.59026853 0.2632599711 ## [78,] 1.03453895 -0.13153881 0.70356384 0.6568380009 ## [79,] 0.18919584 -0.36096697 0.42032558 0.3944526477 ## [80,] -0.17309407 -1.04925145 -0.14615094 -0.2615107354 ## [81,] -0.41462067 -1.50810778 0.02379201 -0.1303180588 ## [82,] -0.41462067 -1.50810778 -0.03285564 -0.2615107354 ## [83,] -0.05233076 -0.81982329 0.08043967 0.0008746178 ## [84,] 0.18919584 -0.81982329 0.76021149 0.5256453243 ## [85,] -0.53538397 -0.13153881 0.42032558 0.3944526477 ## [86,] 0.18919584 0.78617383 0.42032558 0.5256453243 ## [87,] 1.03453895 0.09788935 0.53362088 0.3944526477 ## [88,] 0.55148575 -1.73753594 0.36367793 0.1320672944 ## [89,] -0.29385737 -0.13153881 0.19373497 0.1320672944 ## [90,] -0.41462067 -1.27867961 0.13708732 0.1320672944 ## [91,] -0.41462067 -1.04925145 0.36367793 0.0008746178 ## [92,] 0.30995914 -0.13153881 0.47697323 0.2632599711 ## [93,] -0.05233076 -1.04925145 0.13708732 0.0008746178 ## [94,] -1.01843718 -1.73753594 -0.25944625 -0.2615107354 ## [95,] -0.29385737 -0.81982329 0.25038262 0.1320672944 ## [96,] -0.17309407 -0.13153881 0.25038262 0.0008746178 ## [97,] -0.17309407 -0.36096697 0.25038262 0.1320672944 ## [98,] 0.43072244 -0.36096697 0.30703027 0.1320672944 ## [99,] -0.89767388 -1.27867961 -0.42938920 -0.1303180588 ## [100,] -0.17309407 -0.59039513 0.19373497 0.1320672944 ## [101,] 0.55148575 0.55674567 1.27004036 1.7063794137 ## [102,] -0.05233076 -0.81982329 0.76021149 0.9192233541 ## [103,] 1.51759216 -0.13153881 1.21339271 1.1816087073 ## [104,] 0.55148575 -0.36096697 1.04344975 0.7880306775 ## [105,] 0.79301235 -0.13153881 1.15674505 1.3128013839 ## [106,] 2.12140867 -0.13153881 1.60992627 1.1816087073 ## [107,] -1.13920048 -1.27867961 0.42032558 0.6568380009 ## [108,] 1.75911877 -0.36096697 1.43998331 0.7880306775 ## [109,] 1.03453895 -1.27867961 1.15674505 0.7880306775 ## [110,] 1.63835547 1.24503015 1.32668801 1.7063794137 ## [111,] 0.79301235 0.32731751 0.76021149 1.0504160307 ## [112,] 0.67224905 -0.81982329 0.87350679 0.9192233541 ## [113,] 1.15530226 -0.13153881 0.98680210 1.1816087073 ## [114,] -0.17309407 -1.27867961 0.70356384 1.0504160307 ## [115,] -0.05233076 -0.59039513 0.76021149 1.5751867371 ## [116,] 0.67224905 0.32731751 0.87350679 1.4439940605 ## [117,] 0.79301235 -0.13153881 0.98680210 0.7880306775 ## [118,] 2.24217198 1.70388647 1.66657392 1.3128013839 ## [119,] 2.24217198 -1.04925145 1.77986923 1.4439940605 ## [120,] 0.18919584 -1.96696410 0.70356384 0.3944526477 ## [121,] 1.27606556 0.32731751 1.10009740 1.4439940605 ## [122,] -0.29385737 -0.59039513 0.64691619 1.0504160307 ## [123,] 2.24217198 -0.59039513 1.66657392 1.0504160307 ## [124,] 0.55148575 -0.81982329 0.64691619 0.7880306775 ## [125,] 1.03453895 0.55674567 1.10009740 1.1816087073 ## [126,] 1.63835547 0.32731751 1.27004036 0.7880306775 ## [127,] 0.43072244 -0.59039513 0.59026853 0.7880306775 ## [128,] 0.30995914 -0.13153881 0.64691619 0.7880306775 ## [129,] 0.67224905 -0.59039513 1.04344975 1.1816087073 ## [130,] 1.63835547 -0.13153881 1.15674505 0.5256453243 ## [131,] 1.87988207 -0.59039513 1.32668801 0.9192233541 ## [132,] 2.48369858 1.70388647 1.49663097 1.0504160307 ## [133,] 0.67224905 -0.59039513 1.04344975 1.3128013839 ## [134,] 0.55148575 -0.59039513 0.76021149 0.3944526477 ## [135,] 0.30995914 -1.04925145 1.04344975 0.2632599711 ## [136,] 2.24217198 -0.13153881 1.32668801 1.4439940605 ## [137,] 0.55148575 0.78617383 1.04344975 1.5751867371 ## [138,] 0.67224905 0.09788935 0.98680210 0.7880306775 ## [139,] 0.18919584 -0.13153881 0.59026853 0.7880306775 ## [140,] 1.27606556 0.09788935 0.93015445 1.1816087073 ## [141,] 1.03453895 0.09788935 1.04344975 1.5751867371 ## [142,] 1.27606556 0.09788935 0.76021149 1.4439940605 ## [143,] -0.05233076 -0.81982329 0.76021149 0.9192233541 ## [144,] 1.15530226 0.32731751 1.21339271 1.4439940605 ## [145,] 1.03453895 0.55674567 1.10009740 1.7063794137 ## [146,] 1.03453895 -0.13153881 0.81685914 1.4439940605 ## [147,] 0.55148575 -1.27867961 0.70356384 0.9192233541 ## [148,] 0.79301235 -0.13153881 0.81685914 1.0504160307 ## [149,] 0.43072244 0.78617383 0.93015445 1.4439940605 ## [150,] 0.06843254 -0.13153881 0.76021149 0.7880306775 3.6 求单个变量和其它变量的相关性 Sepal_Length = iris$Sepal.Length iris %&gt;% select(where(is.numeric), -Sepal.Length) %&gt;% apply(2, \\(vec) cor(vec, Sepal_Length)) ## Sepal.Width Petal.Length Petal.Width ## -0.1175698 0.8717538 0.8179411 但是在多重循环中，最好不要使用apply函数或者显式套用apply函数。比如我们想求变量间两两的相关性，看起来非常乱。 data_num = select(iris, where(is.numeric)) apply(data_num, 2, \\(vec1) apply(data_num, 2, \\(vec2) cor(vec1, vec2))) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 1.0000000 -0.1175698 0.8717538 0.8179411 ## Sepal.Width -0.1175698 1.0000000 -0.4284401 -0.3661259 ## Petal.Length 0.8717538 -0.4284401 1.0000000 0.9628654 ## Petal.Width 0.8179411 -0.3661259 0.9628654 1.0000000 双层嵌套，看起来不是很美观。 result = data.frame(matrix(1:16, nrow = 4)) for (a in 1:ncol(data_num)) { for (b in 1:ncol(data_num)) { result[a, b] = cor(data_num[[a]], data_num[[b]]) } } result ## X1 X2 X3 X4 ## 1 1.0000000 -0.1175698 0.8717538 0.8179411 ## 2 -0.1175698 1.0000000 -0.4284401 -0.3661259 ## 3 0.8717538 -0.4284401 1.0000000 0.9628654 ## 4 0.8179411 -0.3661259 0.9628654 1.0000000 我们可以结合for循环和apply减少一层嵌套。 result = data.frame(matrix(1:16, nrow = 4)) for (a in 1:ncol(data_num)) { result[[a]] = apply(data_num, 2, \\(vec) cor(data_num[[a]], vec)) } result ## X1 X2 X3 X4 ## 1 1.0000000 -0.1175698 0.8717538 0.8179411 ## 2 -0.1175698 1.0000000 -0.4284401 -0.3661259 ## 3 0.8717538 -0.4284401 1.0000000 0.9628654 ## 4 0.8179411 -0.3661259 0.9628654 1.0000000 3.7 函数式编程 函数式编程是R的另一大特点，也有可能是因R的面向对象太难用了… 简单求个定积分 integrate(\\(x) x**3 + x, 0, 1) ## 0.75 with absolute error &lt; 8.3e-15 3.8 实例和小测验 # data_exp = as_tibble(data_train_merged[1:100, ]) # saveRDS(data_exp, &quot;示例数据.Rds&quot;) data_task = readRDS(&quot;示例及任务\\\\data\\\\示例数据.Rds&quot;) data_task ## # A tibble: 100 × 14 ## A6 A5 A4 A3 A2 A1 G6 G5 G4 G3 G2 G1 ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 R A Q L S Q X A X L S X ## 2 A A Q L S Q A A X L S X ## 3 C A Q L S Q S A X L S X ## 4 D A Q L S Q N A X L S X ## 5 E A Q L S Q X A X L S X ## 6 F A Q L S Q F A X L S X ## 7 G A Q L S Q G A X L S X ## 8 H A Q L S Q S A X L S X ## 9 I A Q L S Q I A X L S X ## 10 K A Q L S Q X A X L S X ## # ℹ 90 more rows ## # ℹ 2 more variables: Activity &lt;dbl&gt;, Selectivity &lt;dbl&gt; generator_1 = function(sample_space, num) { # 计算类别A中每个种类的构成 sample_list_1 = sample_space %&gt;% select(matches(&quot;A&quot;), -Activity) %&gt;% lapply(\\(vec) table(vec)/length(vec)) # 计算类别G中每个种类的构成 sample_list_2 = sample_space %&gt;% select(matches(&quot;G&quot;)) %&gt;% lapply(\\(vec) table(vec)/length(vec)) # 采用递归方法连接字符串 seq1 = Reduce(paste0, lapply(sample_list_1, \\(.x) sample(names(.x), num, T, .x))) seq2 = Reduce(paste0, lapply(sample_list_2, \\(.x) sample(names(.x), num, T, .x))) list(Sequence1 = seq1, Sequnce2 = seq2) } generator_1(data_task, 100) ## $Sequence1 ## [1] &quot;RAWLSV&quot; &quot;IYQLSQ&quot; &quot;KASLSQ&quot; &quot;NAQLSQ&quot; &quot;RAQLSQ&quot; &quot;RAQLSQ&quot; &quot;RAQLSQ&quot; &quot;RRQLSQ&quot; ## [9] &quot;RAQLSS&quot; &quot;RAQLSQ&quot; &quot;RAQLSQ&quot; &quot;RAQLST&quot; &quot;RAQLRQ&quot; &quot;RAQLSK&quot; &quot;RAQLSQ&quot; &quot;EAQLSQ&quot; ## [17] &quot;RAALFQ&quot; &quot;RVQLSQ&quot; &quot;RAQLSQ&quot; &quot;RTSKLV&quot; &quot;RAQLSQ&quot; &quot;KAQLSQ&quot; &quot;RAQLSQ&quot; &quot;RAQQSQ&quot; ## [25] &quot;RALLSQ&quot; &quot;RATLSQ&quot; &quot;RAQLSQ&quot; &quot;RAQLSQ&quot; &quot;SAQLSQ&quot; &quot;RAQLWS&quot; &quot;RTQWSQ&quot; &quot;TAQLSQ&quot; ## [33] &quot;RALLSD&quot; &quot;RAQLSQ&quot; &quot;RAQTSQ&quot; &quot;RAQLSQ&quot; &quot;RAQLSQ&quot; &quot;VAQLSE&quot; &quot;RAQLHQ&quot; &quot;RAFLSQ&quot; ## [41] &quot;RAPISQ&quot; &quot;RAQLSQ&quot; &quot;RAQLPK&quot; &quot;RAQLDD&quot; &quot;RAQLSQ&quot; &quot;RAWLSQ&quot; &quot;RAQLKQ&quot; &quot;RAQLST&quot; ## [49] &quot;RAQLSV&quot; &quot;GAQLSQ&quot; &quot;RAQLSE&quot; &quot;RAQLSQ&quot; &quot;RAVLSQ&quot; &quot;RAQWSQ&quot; &quot;RAQLSQ&quot; &quot;RAQLDQ&quot; ## [57] &quot;QLQLSQ&quot; &quot;RAQTSQ&quot; &quot;RLQRSQ&quot; &quot;RAQIYQ&quot; &quot;RGQLSQ&quot; &quot;RAELSQ&quot; &quot;RRALTQ&quot; &quot;RAQLSQ&quot; ## [65] &quot;TAQWSQ&quot; &quot;RAQLSP&quot; &quot;RAQLNH&quot; &quot;RAQLSQ&quot; &quot;RAQLSQ&quot; &quot;RNQASQ&quot; &quot;RAQLSQ&quot; &quot;RAQLST&quot; ## [73] &quot;RAQLQQ&quot; &quot;RAQLSQ&quot; &quot;RAQLRQ&quot; &quot;RMQLSQ&quot; &quot;AAQPSQ&quot; &quot;RAQLSQ&quot; &quot;RAQLSQ&quot; &quot;RAQFSQ&quot; ## [81] &quot;RAQLSQ&quot; &quot;RAQLQQ&quot; &quot;LAQWVW&quot; &quot;RSQLSQ&quot; &quot;RAQLKQ&quot; &quot;RAQLSQ&quot; &quot;RAQLSQ&quot; &quot;PAFLSH&quot; ## [89] &quot;WAQLEQ&quot; &quot;RASLSQ&quot; &quot;RAQVSQ&quot; &quot;RAQLFT&quot; &quot;RAQLSQ&quot; &quot;RAQLSQ&quot; &quot;RAQKSQ&quot; &quot;YAQLSQ&quot; ## [97] &quot;GNQLSQ&quot; &quot;RAQLLQ&quot; &quot;RAQLSK&quot; &quot;RAQLSQ&quot; ## ## $Sequnce2 ## [1] &quot;SAXLSX&quot; &quot;XAXPGX&quot; &quot;XAXLSN&quot; &quot;GANLSX&quot; &quot;XAXLSX&quot; &quot;XAXLSX&quot; &quot;XAXLSX&quot; &quot;XAALSX&quot; ## [9] &quot;XAXLSX&quot; &quot;SASPFX&quot; &quot;SAXLSS&quot; &quot;XAXLSX&quot; &quot;XAXLSX&quot; &quot;NAXSSX&quot; &quot;XAXLSX&quot; &quot;XAXSSX&quot; ## [17] &quot;XAXLSX&quot; &quot;XAXLSF&quot; &quot;XSXLSX&quot; &quot;NGXLSX&quot; &quot;NAXLSX&quot; &quot;XAXLSS&quot; &quot;XAXLSX&quot; &quot;XAXLFX&quot; ## [25] &quot;NAGLSI&quot; &quot;SAXLSX&quot; &quot;XAXLSX&quot; &quot;XAXLXX&quot; &quot;XASLSX&quot; &quot;XAXLSX&quot; &quot;XAXLFX&quot; &quot;XAXLNX&quot; ## [33] &quot;XAXLLX&quot; &quot;XAXLNX&quot; &quot;XAXLSX&quot; &quot;FAXLSX&quot; &quot;XAXLSN&quot; &quot;AAXLSN&quot; &quot;XAXLSX&quot; &quot;XAXLSS&quot; ## [41] &quot;XASLSX&quot; &quot;XAXLSX&quot; &quot;XAXLSX&quot; &quot;XAFLSX&quot; &quot;SAXLSX&quot; &quot;XAXLSX&quot; &quot;XXXLNX&quot; &quot;FAXLFX&quot; ## [49] &quot;XAXXSX&quot; &quot;XAXLSI&quot; &quot;XAXASX&quot; &quot;IAXLSX&quot; &quot;XAXLSX&quot; &quot;XAXLSS&quot; &quot;XAXLSX&quot; &quot;XAXLSA&quot; ## [57] &quot;XAXLSX&quot; &quot;XAXLSX&quot; &quot;XAXISX&quot; &quot;XAXLSX&quot; &quot;XAXLSX&quot; &quot;ALPLSX&quot; &quot;XAXLAX&quot; &quot;XAXLSX&quot; ## [65] &quot;XAXLSX&quot; &quot;XAXLSX&quot; &quot;FFXLSX&quot; &quot;XAXLSX&quot; &quot;XAXLSX&quot; &quot;XAXLSX&quot; &quot;XAXSSX&quot; &quot;XAXLSX&quot; ## [73] &quot;XAXFSX&quot; &quot;XAXLSX&quot; &quot;XLXLSX&quot; &quot;SNXLSX&quot; &quot;XAXLSX&quot; &quot;XAXLSX&quot; &quot;XAXFSX&quot; &quot;XXXLXX&quot; ## [81] &quot;XAXLSX&quot; &quot;XAXISX&quot; &quot;SAGLNX&quot; &quot;XAXLSX&quot; &quot;XAXXSX&quot; &quot;XAXLSX&quot; &quot;XAIXSX&quot; &quot;XAXLXX&quot; ## [89] &quot;XAXLSX&quot; &quot;XAXLSI&quot; &quot;XAXLSX&quot; &quot;XAXSSG&quot; &quot;XAXLSX&quot; &quot;XAXLSX&quot; &quot;GANLSX&quot; &quot;XAXXSX&quot; ## [97] &quot;SAXLSX&quot; &quot;XAXLSX&quot; &quot;XAXLSX&quot; &quot;XAXLSX&quot; 为什么1能运行2运行不了呢？ generator_2 = function(sample_space, num) { # 计算类别A中每个种类的构成 sample_list_1 = sample_space %&gt;% select(matches(&quot;A&quot;), -Activity) %&gt;% apply(2, \\(vec) table(vec)/length(vec)) # 计算类别G中每个种类的构成 sample_list_2 = sample_space %&gt;% select(matches(&quot;G&quot;)) %&gt;% apply(2, \\(vec) table(vec)/length(vec)) # 采用递归方法连接字符串 seq1 = Reduce(paste0, lapply(sample_list_1, \\(.x) sample(names(.x), num, T, .x))) seq2 = Reduce(paste0, lapply(sample_list_2, \\(.x) sample(names(.x), num, T, .x))) list(Sequence1 = seq1, Sequnce2 = seq2) } generator_2(data_task, 100) "],["字符串操作和r进阶.html", "4 字符串操作和R进阶 4.1 提纲 4.2 实战", " 4 字符串操作和R进阶 4.1 提纲 4.1.1 变量掩蔽 正常情况下，我们需要使用索引来调用某一列。 iris$Species ## [1] setosa setosa setosa setosa setosa setosa ## [7] setosa setosa setosa setosa setosa setosa ## [13] setosa setosa setosa setosa setosa setosa ## [19] setosa setosa setosa setosa setosa setosa ## [25] setosa setosa setosa setosa setosa setosa ## [31] setosa setosa setosa setosa setosa setosa ## [37] setosa setosa setosa setosa setosa setosa ## [43] setosa setosa setosa setosa setosa setosa ## [49] setosa setosa versicolor versicolor versicolor versicolor ## [55] versicolor versicolor versicolor versicolor versicolor versicolor ## [61] versicolor versicolor versicolor versicolor versicolor versicolor ## [67] versicolor versicolor versicolor versicolor versicolor versicolor ## [73] versicolor versicolor versicolor versicolor versicolor versicolor ## [79] versicolor versicolor versicolor versicolor versicolor versicolor ## [85] versicolor versicolor versicolor versicolor versicolor versicolor ## [91] versicolor versicolor versicolor versicolor versicolor versicolor ## [97] versicolor versicolor versicolor versicolor virginica virginica ## [103] virginica virginica virginica virginica virginica virginica ## [109] virginica virginica virginica virginica virginica virginica ## [115] virginica virginica virginica virginica virginica virginica ## [121] virginica virginica virginica virginica virginica virginica ## [127] virginica virginica virginica virginica virginica virginica ## [133] virginica virginica virginica virginica virginica virginica ## [139] virginica virginica virginica virginica virginica virginica ## [145] virginica virginica virginica virginica virginica virginica ## Levels: setosa versicolor virginica iris[[&quot;Species&quot;]] ## [1] setosa setosa setosa setosa setosa setosa ## [7] setosa setosa setosa setosa setosa setosa ## [13] setosa setosa setosa setosa setosa setosa ## [19] setosa setosa setosa setosa setosa setosa ## [25] setosa setosa setosa setosa setosa setosa ## [31] setosa setosa setosa setosa setosa setosa ## [37] setosa setosa setosa setosa setosa setosa ## [43] setosa setosa setosa setosa setosa setosa ## [49] setosa setosa versicolor versicolor versicolor versicolor ## [55] versicolor versicolor versicolor versicolor versicolor versicolor ## [61] versicolor versicolor versicolor versicolor versicolor versicolor ## [67] versicolor versicolor versicolor versicolor versicolor versicolor ## [73] versicolor versicolor versicolor versicolor versicolor versicolor ## [79] versicolor versicolor versicolor versicolor versicolor versicolor ## [85] versicolor versicolor versicolor versicolor versicolor versicolor ## [91] versicolor versicolor versicolor versicolor versicolor versicolor ## [97] versicolor versicolor versicolor versicolor virginica virginica ## [103] virginica virginica virginica virginica virginica virginica ## [109] virginica virginica virginica virginica virginica virginica ## [115] virginica virginica virginica virginica virginica virginica ## [121] virginica virginica virginica virginica virginica virginica ## [127] virginica virginica virginica virginica virginica virginica ## [133] virginica virginica virginica virginica virginica virginica ## [139] virginica virginica virginica virginica virginica virginica ## [145] virginica virginica virginica virginica virginica virginica ## Levels: setosa versicolor virginica 如果需要索引的部分过多，我们可能会累死。 iris$Sepal.Length + iris$Sepal.Width + iris$Petal.Length ## [1] 10.0 9.3 9.2 9.2 10.0 11.0 9.4 9.9 8.7 9.5 10.6 9.8 9.2 8.4 11.0 ## [16] 11.6 10.6 10.0 11.2 10.4 10.5 10.3 9.2 10.1 10.1 9.6 10.0 10.2 10.0 9.5 ## [31] 9.5 10.3 10.8 11.1 9.5 9.4 10.3 9.9 8.7 10.0 9.8 8.1 8.9 10.1 10.8 ## [46] 9.2 10.5 9.2 10.5 9.7 14.9 14.1 14.9 11.8 13.9 13.0 14.3 10.6 14.1 11.8 ## [61] 10.5 13.1 12.2 13.7 12.1 14.2 13.1 12.6 12.9 12.0 13.9 12.9 13.7 13.6 13.6 ## [76] 14.0 14.4 14.7 13.4 11.8 11.7 11.6 12.4 13.8 12.9 13.9 14.5 13.0 12.7 12.0 ## [91] 12.5 13.7 12.4 10.6 12.5 12.9 12.8 13.4 10.6 12.6 15.6 13.6 16.0 14.8 15.3 ## [106] 17.2 11.9 16.5 15.0 16.9 14.8 14.4 15.3 13.2 13.7 14.9 15.0 18.2 17.2 13.2 ## [121] 15.8 13.3 17.2 13.9 15.7 16.4 13.8 14.0 14.8 16.0 16.3 18.1 14.8 14.2 14.3 ## [136] 16.8 15.3 15.0 13.8 15.4 15.4 15.1 13.6 15.9 15.7 14.9 13.8 14.7 15.0 14.0 这时候我们可以用with进行变量掩蔽，就是一种省力的办法。 with(data = iris, expr = Sepal.Length + Sepal.Width + Petal.Length) ## [1] 10.0 9.3 9.2 9.2 10.0 11.0 9.4 9.9 8.7 9.5 10.6 9.8 9.2 8.4 11.0 ## [16] 11.6 10.6 10.0 11.2 10.4 10.5 10.3 9.2 10.1 10.1 9.6 10.0 10.2 10.0 9.5 ## [31] 9.5 10.3 10.8 11.1 9.5 9.4 10.3 9.9 8.7 10.0 9.8 8.1 8.9 10.1 10.8 ## [46] 9.2 10.5 9.2 10.5 9.7 14.9 14.1 14.9 11.8 13.9 13.0 14.3 10.6 14.1 11.8 ## [61] 10.5 13.1 12.2 13.7 12.1 14.2 13.1 12.6 12.9 12.0 13.9 12.9 13.7 13.6 13.6 ## [76] 14.0 14.4 14.7 13.4 11.8 11.7 11.6 12.4 13.8 12.9 13.9 14.5 13.0 12.7 12.0 ## [91] 12.5 13.7 12.4 10.6 12.5 12.9 12.8 13.4 10.6 12.6 15.6 13.6 16.0 14.8 15.3 ## [106] 17.2 11.9 16.5 15.0 16.9 14.8 14.4 15.3 13.2 13.7 14.9 15.0 18.2 17.2 13.2 ## [121] 15.8 13.3 17.2 13.9 15.7 16.4 13.8 14.0 14.8 16.0 16.3 18.1 14.8 14.2 14.3 ## [136] 16.8 15.3 15.0 13.8 15.4 15.4 15.1 13.6 15.9 15.7 14.9 13.8 14.7 15.0 14.0 注意到tidyverse中也是基于这种原理。 iris %&gt;% mutate(sum = Sepal.Length + Sepal.Width + Petal.Length) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species sum ## 1 5.1 3.5 1.4 0.2 setosa 10.0 ## 2 4.9 3.0 1.4 0.2 setosa 9.3 ## 3 4.7 3.2 1.3 0.2 setosa 9.2 ## 4 4.6 3.1 1.5 0.2 setosa 9.2 ## 5 5.0 3.6 1.4 0.2 setosa 10.0 ## 6 5.4 3.9 1.7 0.4 setosa 11.0 ## 7 4.6 3.4 1.4 0.3 setosa 9.4 ## 8 5.0 3.4 1.5 0.2 setosa 9.9 ## 9 4.4 2.9 1.4 0.2 setosa 8.7 ## 10 4.9 3.1 1.5 0.1 setosa 9.5 ## 11 5.4 3.7 1.5 0.2 setosa 10.6 ## 12 4.8 3.4 1.6 0.2 setosa 9.8 ## 13 4.8 3.0 1.4 0.1 setosa 9.2 ## 14 4.3 3.0 1.1 0.1 setosa 8.4 ## 15 5.8 4.0 1.2 0.2 setosa 11.0 ## 16 5.7 4.4 1.5 0.4 setosa 11.6 ## 17 5.4 3.9 1.3 0.4 setosa 10.6 ## 18 5.1 3.5 1.4 0.3 setosa 10.0 ## 19 5.7 3.8 1.7 0.3 setosa 11.2 ## 20 5.1 3.8 1.5 0.3 setosa 10.4 ## 21 5.4 3.4 1.7 0.2 setosa 10.5 ## 22 5.1 3.7 1.5 0.4 setosa 10.3 ## 23 4.6 3.6 1.0 0.2 setosa 9.2 ## 24 5.1 3.3 1.7 0.5 setosa 10.1 ## 25 4.8 3.4 1.9 0.2 setosa 10.1 ## 26 5.0 3.0 1.6 0.2 setosa 9.6 ## 27 5.0 3.4 1.6 0.4 setosa 10.0 ## 28 5.2 3.5 1.5 0.2 setosa 10.2 ## 29 5.2 3.4 1.4 0.2 setosa 10.0 ## 30 4.7 3.2 1.6 0.2 setosa 9.5 ## 31 4.8 3.1 1.6 0.2 setosa 9.5 ## 32 5.4 3.4 1.5 0.4 setosa 10.3 ## 33 5.2 4.1 1.5 0.1 setosa 10.8 ## 34 5.5 4.2 1.4 0.2 setosa 11.1 ## 35 4.9 3.1 1.5 0.2 setosa 9.5 ## 36 5.0 3.2 1.2 0.2 setosa 9.4 ## 37 5.5 3.5 1.3 0.2 setosa 10.3 ## 38 4.9 3.6 1.4 0.1 setosa 9.9 ## 39 4.4 3.0 1.3 0.2 setosa 8.7 ## 40 5.1 3.4 1.5 0.2 setosa 10.0 ## 41 5.0 3.5 1.3 0.3 setosa 9.8 ## 42 4.5 2.3 1.3 0.3 setosa 8.1 ## 43 4.4 3.2 1.3 0.2 setosa 8.9 ## 44 5.0 3.5 1.6 0.6 setosa 10.1 ## 45 5.1 3.8 1.9 0.4 setosa 10.8 ## 46 4.8 3.0 1.4 0.3 setosa 9.2 ## 47 5.1 3.8 1.6 0.2 setosa 10.5 ## 48 4.6 3.2 1.4 0.2 setosa 9.2 ## 49 5.3 3.7 1.5 0.2 setosa 10.5 ## 50 5.0 3.3 1.4 0.2 setosa 9.7 ## 51 7.0 3.2 4.7 1.4 versicolor 14.9 ## 52 6.4 3.2 4.5 1.5 versicolor 14.1 ## 53 6.9 3.1 4.9 1.5 versicolor 14.9 ## 54 5.5 2.3 4.0 1.3 versicolor 11.8 ## 55 6.5 2.8 4.6 1.5 versicolor 13.9 ## 56 5.7 2.8 4.5 1.3 versicolor 13.0 ## 57 6.3 3.3 4.7 1.6 versicolor 14.3 ## 58 4.9 2.4 3.3 1.0 versicolor 10.6 ## 59 6.6 2.9 4.6 1.3 versicolor 14.1 ## 60 5.2 2.7 3.9 1.4 versicolor 11.8 ## 61 5.0 2.0 3.5 1.0 versicolor 10.5 ## 62 5.9 3.0 4.2 1.5 versicolor 13.1 ## 63 6.0 2.2 4.0 1.0 versicolor 12.2 ## 64 6.1 2.9 4.7 1.4 versicolor 13.7 ## 65 5.6 2.9 3.6 1.3 versicolor 12.1 ## 66 6.7 3.1 4.4 1.4 versicolor 14.2 ## 67 5.6 3.0 4.5 1.5 versicolor 13.1 ## 68 5.8 2.7 4.1 1.0 versicolor 12.6 ## 69 6.2 2.2 4.5 1.5 versicolor 12.9 ## 70 5.6 2.5 3.9 1.1 versicolor 12.0 ## 71 5.9 3.2 4.8 1.8 versicolor 13.9 ## 72 6.1 2.8 4.0 1.3 versicolor 12.9 ## 73 6.3 2.5 4.9 1.5 versicolor 13.7 ## 74 6.1 2.8 4.7 1.2 versicolor 13.6 ## 75 6.4 2.9 4.3 1.3 versicolor 13.6 ## 76 6.6 3.0 4.4 1.4 versicolor 14.0 ## 77 6.8 2.8 4.8 1.4 versicolor 14.4 ## 78 6.7 3.0 5.0 1.7 versicolor 14.7 ## 79 6.0 2.9 4.5 1.5 versicolor 13.4 ## 80 5.7 2.6 3.5 1.0 versicolor 11.8 ## 81 5.5 2.4 3.8 1.1 versicolor 11.7 ## 82 5.5 2.4 3.7 1.0 versicolor 11.6 ## 83 5.8 2.7 3.9 1.2 versicolor 12.4 ## 84 6.0 2.7 5.1 1.6 versicolor 13.8 ## 85 5.4 3.0 4.5 1.5 versicolor 12.9 ## 86 6.0 3.4 4.5 1.6 versicolor 13.9 ## 87 6.7 3.1 4.7 1.5 versicolor 14.5 ## 88 6.3 2.3 4.4 1.3 versicolor 13.0 ## 89 5.6 3.0 4.1 1.3 versicolor 12.7 ## 90 5.5 2.5 4.0 1.3 versicolor 12.0 ## 91 5.5 2.6 4.4 1.2 versicolor 12.5 ## 92 6.1 3.0 4.6 1.4 versicolor 13.7 ## 93 5.8 2.6 4.0 1.2 versicolor 12.4 ## 94 5.0 2.3 3.3 1.0 versicolor 10.6 ## 95 5.6 2.7 4.2 1.3 versicolor 12.5 ## 96 5.7 3.0 4.2 1.2 versicolor 12.9 ## 97 5.7 2.9 4.2 1.3 versicolor 12.8 ## 98 6.2 2.9 4.3 1.3 versicolor 13.4 ## 99 5.1 2.5 3.0 1.1 versicolor 10.6 ## 100 5.7 2.8 4.1 1.3 versicolor 12.6 ## 101 6.3 3.3 6.0 2.5 virginica 15.6 ## 102 5.8 2.7 5.1 1.9 virginica 13.6 ## 103 7.1 3.0 5.9 2.1 virginica 16.0 ## 104 6.3 2.9 5.6 1.8 virginica 14.8 ## 105 6.5 3.0 5.8 2.2 virginica 15.3 ## 106 7.6 3.0 6.6 2.1 virginica 17.2 ## 107 4.9 2.5 4.5 1.7 virginica 11.9 ## 108 7.3 2.9 6.3 1.8 virginica 16.5 ## 109 6.7 2.5 5.8 1.8 virginica 15.0 ## 110 7.2 3.6 6.1 2.5 virginica 16.9 ## 111 6.5 3.2 5.1 2.0 virginica 14.8 ## 112 6.4 2.7 5.3 1.9 virginica 14.4 ## 113 6.8 3.0 5.5 2.1 virginica 15.3 ## 114 5.7 2.5 5.0 2.0 virginica 13.2 ## 115 5.8 2.8 5.1 2.4 virginica 13.7 ## 116 6.4 3.2 5.3 2.3 virginica 14.9 ## 117 6.5 3.0 5.5 1.8 virginica 15.0 ## 118 7.7 3.8 6.7 2.2 virginica 18.2 ## 119 7.7 2.6 6.9 2.3 virginica 17.2 ## 120 6.0 2.2 5.0 1.5 virginica 13.2 ## 121 6.9 3.2 5.7 2.3 virginica 15.8 ## 122 5.6 2.8 4.9 2.0 virginica 13.3 ## 123 7.7 2.8 6.7 2.0 virginica 17.2 ## 124 6.3 2.7 4.9 1.8 virginica 13.9 ## 125 6.7 3.3 5.7 2.1 virginica 15.7 ## 126 7.2 3.2 6.0 1.8 virginica 16.4 ## 127 6.2 2.8 4.8 1.8 virginica 13.8 ## 128 6.1 3.0 4.9 1.8 virginica 14.0 ## 129 6.4 2.8 5.6 2.1 virginica 14.8 ## 130 7.2 3.0 5.8 1.6 virginica 16.0 ## 131 7.4 2.8 6.1 1.9 virginica 16.3 ## 132 7.9 3.8 6.4 2.0 virginica 18.1 ## 133 6.4 2.8 5.6 2.2 virginica 14.8 ## 134 6.3 2.8 5.1 1.5 virginica 14.2 ## 135 6.1 2.6 5.6 1.4 virginica 14.3 ## 136 7.7 3.0 6.1 2.3 virginica 16.8 ## 137 6.3 3.4 5.6 2.4 virginica 15.3 ## 138 6.4 3.1 5.5 1.8 virginica 15.0 ## 139 6.0 3.0 4.8 1.8 virginica 13.8 ## 140 6.9 3.1 5.4 2.1 virginica 15.4 ## 141 6.7 3.1 5.6 2.4 virginica 15.4 ## 142 6.9 3.1 5.1 2.3 virginica 15.1 ## 143 5.8 2.7 5.1 1.9 virginica 13.6 ## 144 6.8 3.2 5.9 2.3 virginica 15.9 ## 145 6.7 3.3 5.7 2.5 virginica 15.7 ## 146 6.7 3.0 5.2 2.3 virginica 14.9 ## 147 6.3 2.5 5.0 1.9 virginica 13.8 ## 148 6.5 3.0 5.2 2.0 virginica 14.7 ## 149 6.2 3.4 5.4 2.3 virginica 15.0 ## 150 5.9 3.0 5.1 1.8 virginica 14.0 4.1.2 字符串操作 这里只简单的演示操作。 a = c(&quot;北京&quot;, &quot;欢迎&quot;, &quot;你&quot;) b = c(&quot;为你&quot;, &quot;开天&quot;, &quot;辟地&quot;) # paste与stringr中的str_c等价 paste(a, b) ## [1] &quot;北京 为你&quot; &quot;欢迎 开天&quot; &quot;你 辟地&quot; 下二者等价 paste(a, b, sep = &quot;&quot;) ## [1] &quot;北京为你&quot; &quot;欢迎开天&quot; &quot;你辟地&quot; paste0(a, b) ## [1] &quot;北京为你&quot; &quot;欢迎开天&quot; &quot;你辟地&quot; 下二者等价 paste(a, collapse = &quot;&quot;) ## [1] &quot;北京欢迎你&quot; paste0(a, collapse = &quot;&quot;) ## [1] &quot;北京欢迎你&quot; paste(paste(a, collapse = &quot;&quot;), paste(b, collapse = &quot;&quot;), sep = &quot;, &quot;) ## [1] &quot;北京欢迎你, 为你开天辟地&quot; str_c(a, b) ## [1] &quot;北京为你&quot; &quot;欢迎开天&quot; &quot;你辟地&quot; 4.1.3 grep grep显示匹配的位次，grepl显示每个位次是否匹配。 grep(&quot;北&quot;, a) ## [1] 1 grep(&quot;你&quot;, a) ## [1] 3 grepl(&quot;你&quot;, a) ## [1] FALSE FALSE TRUE 下二者等价 grep(&quot;你&quot;, a) ## [1] 3 which(grepl(&quot;你&quot;, a)) ## [1] 3 str_detect与grepl等价，但是顺序不同，更方便tidy写法。 str_detect(a, &quot;北&quot;) ## [1] TRUE FALSE FALSE 4.1.4 substr str_sub支持负数索引，更有用。 substr(&quot;北京欢迎你&quot;, start = 3, stop = 4) ## [1] &quot;欢迎&quot; str_sub(&quot;北京欢迎你&quot;, start = 3, end = -2) ## [1] &quot;欢迎&quot; 4.1.5 正则表达式与str_extract c = c(&quot;《北京欢迎你》是由林夕作词，小柯作曲，百位明星共同演唱的一首以奥运为主题的歌曲，发行于2008年4月30日。&quot;, &quot;2009年这首歌曲成为MusicRadio音乐之声点播冠军曲。&quot;) # 下面这两个最上面只匹配一次，下面的匹配全部 str_extract(c, &quot;\\\\d+&quot;) ## [1] &quot;2008&quot; &quot;2009&quot; str_extract_all(c, &quot;\\\\d+&quot;) ## [[1]] ## [1] &quot;2008&quot; &quot;4&quot; &quot;30&quot; ## ## [[2]] ## [1] &quot;2009&quot; # baseR的实现困难一些 regmatches(c, gregexpr(&quot;\\\\d+&quot;, c, perl = TRUE)) ## [[1]] ## [1] &quot;2008&quot; &quot;4&quot; &quot;30&quot; ## ## [[2]] ## [1] &quot;2009&quot; 4.1.6 str_subset 可以用于匹配符号条件的字符。 grep(&quot;欢&quot;, a, value = T) ## [1] &quot;欢迎&quot; str_subset(a, &quot;欢&quot;) ## [1] &quot;欢迎&quot; 4.1.7 str_split genes = c(&quot;LOC441259 /// POLR2J2&quot;, &quot;KLHDC7B&quot;, &quot;ATAD3A /// ATAD3B /// LOC732419&quot;) strsplit(genes, &quot; /// &quot;) ## [[1]] ## [1] &quot;LOC441259&quot; &quot;POLR2J2&quot; ## ## [[2]] ## [1] &quot;KLHDC7B&quot; ## ## [[3]] ## [1] &quot;ATAD3A&quot; &quot;ATAD3B&quot; &quot;LOC732419&quot; 简而言之，stringr中的字符串操作命名统一一点，功能多一点，没比BaseR强在功能上。 str_split(genes, &quot; /// &quot;) ## [[1]] ## [1] &quot;LOC441259&quot; &quot;POLR2J2&quot; ## ## [[2]] ## [1] &quot;KLHDC7B&quot; ## ## [[3]] ## [1] &quot;ATAD3A&quot; &quot;ATAD3B&quot; &quot;LOC732419&quot; str_split(genes, &quot; /// &quot;, simplify = T) ## [,1] [,2] [,3] ## [1,] &quot;LOC441259&quot; &quot;POLR2J2&quot; &quot;&quot; ## [2,] &quot;KLHDC7B&quot; &quot;&quot; &quot;&quot; ## [3,] &quot;ATAD3A&quot; &quot;ATAD3B&quot; &quot;LOC732419&quot; 4.2 实战 #有数据如下 # type # 1 a, b # 2 - # 3 a #需统计其出现位置，如下 # a b - # [1,] 1 1 0 # [2,] 0 0 1 # [3,] 1 0 0 data = data.frame(type = c(&quot;a, b&quot;, &quot;-&quot;, &quot;a&quot;)) 可采用以下若干方法 4.2.1 1. 向量化 type = data$type %&gt;% unique() %&gt;% str_split(&quot;, &quot;) %&gt;% unlist() %&gt;% unique() # 下面这种写法利用了R的参数传递机制 # 灵活、强大，但是不美观，表现为把传参过程写在函数的外面 # 但是有一些特定用途，比如说多个函数依赖于同一个参数 sapply(type, \\(x, invec) as.numeric(grepl(x, invec)), invec = data$type) ## a b - ## [1,] 1 1 0 ## [2,] 0 0 1 ## [3,] 1 0 0 # 在没有上述特定情况下，个人更喜欢以下写法 sapply(type, \\(x, invec = data$type) as.numeric(grepl(x, invec))) ## a b - ## [1,] 1 1 0 ## [2,] 0 0 1 ## [3,] 1 0 0 # 像下面这样写也是可以的，但是总让人觉得这段程序要崩溃 # 在环境管理不严格的动态语言中（如python和R）经常看到这种写法，让人觉得很不安全 # 就是说内部的环境可以使用全局环境的变量，但是全局的不能使用局部的 # 就是说儿子能花爹的钱，爹不能找儿子借钱 # 但是只是看着不安全，如果你理解了上述的参数传递机制和R的命名空间知识后，就会觉得没什么大不了的 # 但是在极端情况下还是会崩溃，比如在r的shiny软件中惰性求值 sapply(type, \\(x) as.numeric(grepl(x, data$type))) ## a b - ## [1,] 1 1 0 ## [2,] 0 0 1 ## [3,] 1 0 0 # map函数同样可以，不过需要提前命名 # map函数是purrr中的函数，相当于apply函数簇的拓展，用法相似 names(type) = type map(type, \\(x, invec = data$type) as.numeric(grepl(x, invec))) %&gt;% as_tibble() ## # A tibble: 3 × 3 ## a b `-` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 ## 2 0 0 1 ## 3 1 0 0 map_dfc(type, \\(x, invec = data$type) as.numeric(grepl(x, invec))) ## # A tibble: 3 × 3 ## a b `-` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 ## 2 0 0 1 ## 3 1 0 0 # purrr的匿名函数设计中因为没有显式的传参 # 所以只能用于单个参数，剩下一个参数只能从全局调用 # 等同于上述的那种写法 map_dfc(type, ~ as.numeric(grepl(.x, data$type))) ## # A tibble: 3 × 3 ## a b `-` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 ## 2 0 0 1 ## 3 1 0 0 4.2.2 2. tibble的nested tibble性质 tibble拓展了dataframe的范围，支持储存列表，甚至套娃另一个df。 data %&gt;% as_tibble() %&gt;% mutate( y = str_split(type, &quot;, &quot;), y = map(y, ~ set_names(.x, .x))) %&gt;% unnest_wider(y) %&gt;% mutate(across(-type, ~ ifelse(is.na(.x), 0, 1))) ## # A tibble: 3 × 4 ## type a b `-` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a, b 1 1 0 ## 2 - 0 0 1 ## 3 a 1 0 0 4.2.3 3. dataframe操作 利用tibble的性质确实有些超模了，只利用tidyr的数据框操作也可以。如果想单纯的使用mutate解决，无疑就陷入了误区，但是思路比较刁钻。 data %&gt;% mutate(id = type, value = 1) %&gt;% separate_rows(type, sep = &quot;, &quot;) %&gt;% pivot_wider(names_from = type, values_from = value, values_fill = 0) ## # A tibble: 3 × 4 ## id a b `-` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a, b 1 1 0 ## 2 - 0 0 1 ## 3 a 1 0 0 4.2.4 4. 循环 type = data$type %&gt;% unique() %&gt;% str_split(&quot;, &quot;) %&gt;% unlist() %&gt;% unique() result = list() for (element in type) { result[[element]] = as.numeric(grepl(element, data$type)) } as.data.frame(result) ## a b X. ## 1 1 1 0 ## 2 0 0 1 ## 3 1 0 0 "],["初探mlr3.html", "5 初探mlr3 5.1 mlr3中使用po()进行特征工程", " 5 初探mlr3 机器学习/深度学习的边际效益极低，一个人只需要两个星期就可以写出能实现我能写出的模型百分之九十甚至以上效果的模型。 lrns() # 查看学习器 ## &lt;DictionaryLearner&gt; with 141 stored values ## Keys: classif.abess, classif.AdaBoostM1, classif.bart, classif.C50, ## classif.catboost, classif.cforest, classif.ctree, classif.cv_glmnet, ## classif.debug, classif.earth, classif.featureless, classif.fnn, ## classif.gam, classif.gamboost, classif.gausspr, classif.gbm, ## classif.glmboost, classif.glmer, classif.glmnet, classif.IBk, ## classif.imbalanced_rfsrc, classif.J48, classif.JRip, classif.kknn, ## classif.ksvm, classif.lda, classif.liblinear, classif.lightgbm, ## classif.LMT, classif.log_reg, classif.lssvm, classif.mob, ## classif.multinom, classif.naive_bayes, classif.nnet, classif.OneR, ## classif.PART, classif.priority_lasso, classif.qda, ## classif.randomForest, classif.ranger, classif.rfsrc, classif.rpart, ## classif.svm, classif.xgboost, clust.agnes, clust.ap, clust.cmeans, ## clust.cobweb, clust.dbscan, clust.diana, clust.em, clust.fanny, ## clust.featureless, clust.ff, clust.hclust, clust.kkmeans, ## clust.kmeans, clust.MBatchKMeans, clust.mclust, clust.meanshift, ## clust.pam, clust.SimpleKMeans, clust.xmeans, dens.kde_ks, ## dens.locfit, dens.logspline, dens.mixed, dens.nonpar, dens.pen, ## dens.plug, dens.spline, regr.abess, regr.bart, regr.catboost, ## regr.catboost_modified, regr.cforest, regr.ctree, regr.cubist, ## regr.cv_glmnet, regr.debug, regr.earth, regr.featureless, regr.fnn, ## regr.gam, regr.gamboost, regr.gausspr, regr.gbm, regr.glm, ## regr.glmboost, regr.glmnet, regr.IBk, regr.kknn, regr.km, regr.ksvm, ## regr.liblinear, regr.lightgbm, regr.lm, regr.lmer, regr.M5Rules, ## regr.mars, regr.mob, regr.nnet, regr.priority_lasso, ## regr.randomForest, regr.ranger, regr.rfsrc, regr.rpart, regr.rsm, ## regr.rvm, regr.svm, regr.xgboost, surv.akritas, surv.aorsf, ## surv.blackboost, surv.cforest, surv.coxboost, surv.coxtime, ## surv.ctree, surv.cv_coxboost, surv.cv_glmnet, surv.deephit, ## surv.deepsurv, surv.dnnsurv, surv.flexible, surv.gamboost, surv.gbm, ## surv.glmboost, surv.glmnet, surv.loghaz, surv.mboost, surv.nelson, ## surv.obliqueRSF, surv.parametric, surv.pchazard, surv.penalized, ## surv.priority_lasso, surv.ranger, surv.rfsrc, surv.svm, surv.xgboost lrn_c_ranger = lrn(&quot;classif.ranger&quot;) # 随机森林 分类学习器 lrn_r_ranger = lrn(&quot;regr.ranger&quot;) # 随机森林 回归学习器 tsks() # 查看自带任务 ## &lt;DictionaryTask&gt; with 21 stored values ## Keys: ames_housing, bike_sharing, boston_housing, breast_cancer, ## german_credit, ilpd, iris, kc_housing, moneyball, mtcars, optdigits, ## penguins, penguins_simple, pima, ruspini, sonar, spam, titanic, ## usarrests, wine, zoo tsk_c = tsk(&quot;titanic&quot;) # 二分类任务 tsk_r = tsk(&quot;boston_housing&quot;) # 回归任务 tsk_c$missings() ## survived age cabin embarked fare name parch pclass ## 418 263 1014 2 1 0 0 0 ## sex sib_sp ticket ## 0 0 0 tsk_r$missings() ## cmedv age b chas crim dis indus lat lon lstat ## 0 0 0 0 0 0 0 0 0 0 ## nox ptratio rad rm tax town tract zn ## 0 0 0 0 0 0 0 0 tsk_c$data() |&gt; View() mlr3是基于R6对象开发的，R6对象和其它语言的面向对象形式十分相似，面向对象中一个显著的特点是把数据和操纵数据的方法结合到一起。 #比如说以下我们用tsk_c$filter(useful_sample)就已经筛选了满足条件的行，而不需要tsk_c = tsk_c$filter(useful_sample)的形式用赋值符号链接，可以简单的把面向对象理解成“自成体系”，就是用对象内部的函数处理对象内部的数据。 useful_sample = which(!is.na(tsk_c$data()$survived)) # 这段函数是不是看起来十分抽象？ tsk_c$filter(useful_sample) tsk_c$missings() ## survived age cabin embarked fare name parch pclass ## 0 177 687 2 0 0 0 0 ## sex sib_sp ticket ## 0 0 0 tsk_c$feature_types # 查看每列特征的类型 ## id type ## 1: age numeric ## 2: cabin character ## 3: embarked factor ## 4: fare numeric ## 5: name character ## 6: parch integer ## 7: pclass ordered ## 8: sex factor ## 9: sib_sp integer ## 10: ticket character col_not_cha = tsk_c$feature_types$id[tsk_c$feature_types$type != &quot;character&quot;] # 为字符类型的列，去掉 tsk_c$select(col_not_cha) tsk_c$col_info # 查看每列特征的类型 ## id type levels label fix_factor_levels ## 1: ..row_id integer &lt;NA&gt; FALSE ## 2: age numeric &lt;NA&gt; FALSE ## 3: cabin character &lt;NA&gt; FALSE ## 4: embarked factor C,Q,S &lt;NA&gt; FALSE ## 5: fare numeric &lt;NA&gt; FALSE ## 6: name character &lt;NA&gt; FALSE ## 7: parch integer &lt;NA&gt; FALSE ## 8: pclass ordered 1,2,3 &lt;NA&gt; FALSE ## 9: sex factor female,male &lt;NA&gt; FALSE ## 10: sib_sp integer &lt;NA&gt; FALSE ## 11: survived factor yes,no &lt;NA&gt; TRUE ## 12: ticket character &lt;NA&gt; FALSE tsk_c$missings() ## survived age embarked fare parch pclass sex sib_sp ## 0 177 2 0 0 0 0 0 # 注意到embarked是多分类因子，需要编码，且数据中含有缺失值，进行特征工程 5.1 mlr3中使用po()进行特征工程 preprocess_pipe1 = po(&quot;imputemode&quot;) # 使用众数插补 preprocess_pipe1$train(list(tsk_c)) ## $output ## &lt;TaskClassif:titanic&gt; (891 x 8): Titanic ## * Target: survived ## * Properties: twoclass ## * Features (7): ## - dbl (2): age, fare ## - fct (2): embarked, sex ## - int (2): parch, sib_sp ## - ord (1): pclass tsk_c_imputed = preprocess_pipe1$predict(list(tsk_c))[[1]] tsk_c_imputed$missings() # 插补好了 ## survived fare parch pclass sex sib_sp age embarked ## 0 0 0 0 0 0 0 0 但是实际过程中一般不采用众数或者常数插补，一般将利用其它特征训练一个学习器用于插补缺失值，参考mlr3的ppt。 多个po可以用 %&gt;&gt;% 连接起来 preprocess_pipe2 = po(&quot;imputemode&quot;) %&gt;&gt;% po(&quot;encode&quot;, method = &quot;one-hot&quot;) preprocess_pipe2$train(tsk_c) ## $encode.output ## &lt;TaskClassif:titanic&gt; (891 x 13): Titanic ## * Target: survived ## * Properties: twoclass ## * Features (12): ## - dbl (10): age, embarked.C, embarked.Q, embarked.S, fare, pclass.1, ## pclass.2, pclass.3, sex.female, sex.male ## - int (2): parch, sib_sp onehot编码即独热编码，是将分类变量化作数值变量的一种方法,看到这里你可能很迷惑，为什么上面要括号底下不要？ 这里是mlr3作者偷了懒，大家只要记得一个po要list括起来，多个po不要,以及一定要记得，最后返回的是一个列表，所以结尾要加个[[1]]提取结果。 tsk_c_imputed_expanded = preprocess_pipe2$predict(tsk_c)[[1]] 以下简单演示一下学习器插补，这里是有坑的，后面可以参考避坑。 都用决策树用来插补，回归学习器用来插补数值变量，分类学习器插补因子，这里的两个坑，一是不同学习器必须有不同的id，二是必须严格指明影响的范围。 preprocess_pipe0 = po(&quot;imputelearner&quot;, id = &quot;num&quot;, lrn(&quot;regr.rpart&quot;), affect_columns = selector_type(&quot;numeric&quot;)) %&gt;&gt;% po(&quot;imputelearner&quot;, id = &quot;fct&quot;, lrn(&quot;classif.rpart&quot;), affect_columns = selector_type(&quot;factor&quot;)) 现在我们有了良好的数据，但是我们还需要将数据划分为训练集和测试集，前者用于训练模型，后者用于评估模型效果。 split_c = partition(tsk_c_imputed_expanded, ratio = 0.7) split_r = partition(tsk_r, ratio = 0.7) # 三七划分 split_c$train # 其实就是个列表 ## [1] 1 6 7 8 14 15 17 19 21 27 28 30 31 35 39 41 42 43 ## [19] 49 50 51 52 58 61 65 68 70 71 72 73 77 81 84 87 90 91 ## [37] 93 95 97 100 101 103 104 105 109 112 113 114 116 118 119 120 121 123 ## [55] 125 127 130 131 132 133 135 136 139 140 144 145 146 148 149 150 153 154 ## [73] 155 160 161 163 165 168 171 172 174 176 179 181 182 183 186 190 192 197 ## [91] 198 200 201 204 206 211 214 215 218 220 222 223 224 226 228 230 232 233 ## [109] 235 236 237 240 241 244 245 246 247 250 252 253 254 261 263 265 266 267 ## [127] 271 274 277 278 279 283 285 286 294 295 296 297 305 309 313 315 318 321 ## [145] 325 327 332 334 336 340 343 344 345 350 351 352 353 354 355 358 361 362 ## [163] 363 364 365 366 372 373 374 378 379 383 385 386 387 393 397 398 402 405 ## [181] 406 407 409 414 416 419 420 422 423 424 425 426 434 435 437 440 442 443 ## [199] 451 452 453 460 462 464 465 466 467 469 472 475 476 477 478 479 481 482 ## [217] 486 488 489 491 492 493 495 498 499 500 501 502 503 506 512 515 516 518 ## [235] 522 523 525 526 528 529 530 532 533 535 537 539 543 545 549 552 556 558 ## [253] 562 563 568 575 579 583 585 590 591 593 594 595 599 604 606 607 612 614 ## [271] 615 617 620 625 626 632 634 635 637 638 639 641 653 655 656 657 659 660 ## [289] 664 667 669 672 673 676 677 681 685 686 689 695 699 700 705 706 712 714 ## [307] 716 720 723 724 726 729 730 732 734 735 736 737 739 740 742 744 747 749 ## [325] 750 753 754 757 758 759 761 768 769 770 772 773 776 779 783 786 790 791 ## [343] 792 793 794 796 801 806 807 808 811 813 815 816 817 818 819 823 825 826 ## [361] 833 834 835 837 838 842 847 849 852 853 855 862 864 865 868 869 871 873 ## [379] 874 879 883 886 887 891 2 3 4 9 10 16 18 20 22 24 26 33 ## [397] 40 44 45 48 53 57 62 66 67 69 82 83 85 89 99 107 108 110 ## [415] 126 137 142 147 167 173 185 187 188 191 193 194 195 196 199 205 208 209 ## [433] 210 216 217 221 225 227 231 242 248 249 257 258 259 260 262 269 272 273 ## [451] 275 280 284 287 289 290 291 300 301 302 304 306 307 308 310 311 312 319 ## [469] 323 324 326 328 331 338 339 341 342 346 347 348 349 360 367 368 369 370 ## [487] 376 377 388 392 394 401 408 413 415 417 427 428 430 433 438 441 444 445 ## [505] 446 447 448 449 461 470 473 480 484 487 490 505 510 511 513 519 524 527 ## [523] 534 536 538 540 541 547 548 550 557 559 560 570 571 572 573 574 577 578 ## [541] 580 581 582 592 597 601 608 609 610 616 619 622 623 628 631 633 636 642 ## [559] 644 645 648 650 661 665 670 674 678 682 690 691 692 693 698 701 702 707 ## [577] 708 709 711 713 717 725 727 738 743 752 755 756 760 763 764 766 775 780 ## [595] 781 782 787 789 798 802 803 804 822 824 828 829 832 843 850 854 856 857 ## [613] 858 859 863 866 867 870 875 876 880 881 888 然后给列标注上训练和测试的信息，有两种方式。我们上面提到面向对象具有不需要赋值的特点，但是有的时候我们不得不赋值，可以采用新建一个对象，即内部的clone方法。 tsk_c_imputed_expanded$set_row_roles(split_c$test, roles = &quot;test&quot;) # 这个是旧方法，新版本用不到了，但是有特定的用途，后面介绍早停的时候会用到 tsk_r_train = tsk_r$clone(deep = T)$filter(split_r$train) tsk_r_test = tsk_r$clone(deep = T)$filter(split_r$test) 同理，训练和预测也是两种方法 lrn_c_ranger$train(tsk_c_imputed_expanded, row_ids = split_c$train) lrn_c_ranger$predict(tsk_c_imputed_expanded, row_ids = split_c$test) ## &lt;PredictionClassif&gt; for 268 observations: ## row_ids truth response ## 5 no no ## 13 no no ## 25 no no ## --- ## 840 yes no ## 872 yes yes ## 890 yes no lrn_r_ranger$train(tsk_r_train) lrn_r_ranger$predict(tsk_r_test) ## &lt;PredictionRegr&gt; for 152 observations: ## row_ids truth response ## 1 24.0 27.08351 ## 6 28.7 27.50547 ## 9 16.5 17.10051 ## --- ## 488 20.6 20.70861 ## 494 21.8 21.31282 ## 506 19.0 21.13678 任务、学习器都有各自的对象，预测结果也有，就是prediction对象，实际上mlr3里任何东西都有个对象。 predicition_c = lrn_c_ranger$predict(tsk_c_imputed_expanded, row_ids = split_c$test) predicition_r = lrn_r_ranger$predict(tsk_r_test) 可以用score方法查看默认的指标，也可以在score内指明其它指标，使用msrs()查看所有指标。 predicition_c$score() ## classif.ce ## 0.1753731 predicition_c$score(msr(&quot;classif.acc&quot;)) ## classif.acc ## 0.8246269 "],["二分类和多分类问题.html", "6 二分类和多分类问题 6.1 二分类 6.2 hardmax 6.3 softmax 6.4 多分类的损失函数", " 6 二分类和多分类问题 data = as_tibble(iris) data_no_setosa = filter(data, Species != &quot;setosa&quot;) data_no_versicolor = filter(data, Species != &quot;versicolor&quot;) data_no_virginica = filter(data, Species != &quot;virginica&quot;) 6.1 二分类 two_class = function(data) { model = glm(Species ~ ., data = data, family = binomial()) predict(model, newdata = data[, -5], type = &quot;response&quot;) } two_class(data_no_setosa) ## 1 2 3 4 5 6 ## 1.171672e-05 4.856237e-05 1.198626e-03 4.220049e-05 1.408470e-03 1.018578e-04 ## 7 8 9 10 11 12 ## 1.305727e-03 5.351876e-10 1.458241e-05 1.481064e-05 3.990780e-08 3.744346e-05 ## 13 14 15 16 17 18 ## 9.947107e-08 7.988665e-04 1.378280e-08 2.828836e-06 1.326003e-03 1.481153e-08 ## 19 20 21 22 23 24 ## 5.959820e-02 8.712675e-08 4.048381e-01 3.405812e-07 2.248338e-01 4.023809e-05 ## 25 26 27 28 29 30 ## 1.410660e-06 7.060188e-06 7.124099e-04 2.760617e-01 9.651525e-04 1.290424e-10 ## 31 32 33 34 35 36 ## 8.469327e-08 5.298820e-09 8.707382e-08 8.676299e-01 2.169221e-03 2.129823e-04 ## 37 38 39 40 41 42 ## 2.979719e-04 2.551360e-04 7.884147e-07 1.109268e-05 3.969831e-05 1.596216e-04 ## 43 44 45 46 47 48 ## 4.360614e-07 8.158121e-10 1.502115e-05 2.541253e-07 3.085679e-06 2.309662e-06 ## 49 50 51 52 53 54 ## 6.163826e-11 2.344150e-06 1.000000e+00 9.996139e-01 9.999990e-01 9.997188e-01 ## 55 56 57 58 59 60 ## 9.999999e-01 1.000000e+00 8.908123e-01 9.999955e-01 9.999921e-01 1.000000e+00 ## 61 62 63 64 65 66 ## 9.902584e-01 9.997429e-01 9.999800e-01 9.999673e-01 9.999999e-01 9.999952e-01 ## 67 68 69 70 71 72 ## 9.976994e-01 9.999999e-01 1.000000e+00 9.204923e-01 9.999996e-01 9.995130e-01 ## 73 74 75 76 77 78 ## 1.000000e+00 9.484339e-01 9.999824e-01 9.995586e-01 8.245440e-01 8.022990e-01 ## 79 80 81 82 83 84 ## 9.999992e-01 9.712013e-01 9.999969e-01 9.999189e-01 9.999999e-01 2.048741e-01 ## 85 86 87 88 89 90 ## 9.664047e-01 1.000000e+00 9.999999e-01 9.964973e-01 6.691425e-01 9.998717e-01 ## 91 92 93 94 95 96 ## 1.000000e+00 9.999440e-01 9.996139e-01 1.000000e+00 1.000000e+00 9.999932e-01 ## 97 98 99 100 ## 9.991067e-01 9.989939e-01 9.999956e-01 9.776789e-01 注意到，二分类输出的是一个个0到1之间的概率值，数据均衡的话取阈值0.5，将概率化为二分类结果。数据不均衡的话采用其它阈值。 二分类预测结果的评价指标是交叉熵损失函数，越低越好了。 cross_entropy = function (truth, response) { # 注意到，当truth为0或1时，其中一项就被消掉了 - mean(truth * log(response) + (1-truth) * log(1-response)) } cross_entropy(as.numeric(data_no_setosa$Species)-2, two_class(data_no_setosa)) ## [1] 0.05949273 简而言之，n分类就是把二分类重复n次。 data_is_setosa = mutate(data, Species = ifelse(Species == &quot;setosa&quot;, 0, 1)) data_is_versicolor = mutate(data, Species = ifelse(Species == &quot;versicolor&quot;, 0, 1)) data_is_virginica = mutate(data, Species = ifelse(Species == &quot;virginica&quot;, 0, 1)) two_class(data_is_setosa) ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## 1 2 3 4 5 6 ## 2.220446e-16 2.220446e-16 2.220446e-16 1.875686e-12 2.220446e-16 2.220446e-16 ## 7 8 9 10 11 12 ## 2.077262e-13 2.220446e-16 1.268389e-11 2.220446e-16 2.220446e-16 1.266230e-13 ## 13 14 15 16 17 18 ## 2.220446e-16 2.220446e-16 2.220446e-16 2.220446e-16 2.220446e-16 2.220446e-16 ## 19 20 21 22 23 24 ## 2.220446e-16 2.220446e-16 2.220446e-16 2.220446e-16 2.220446e-16 3.968396e-11 ## 25 26 27 28 29 30 ## 5.245006e-11 2.775502e-13 9.076879e-13 2.220446e-16 2.220446e-16 1.969335e-12 ## 31 32 33 34 35 36 ## 1.331002e-12 2.220446e-16 2.220446e-16 2.220446e-16 2.220446e-16 2.220446e-16 ## 37 38 39 40 41 42 ## 2.220446e-16 2.220446e-16 7.767584e-13 2.220446e-16 2.220446e-16 5.033679e-10 ## 43 44 45 46 47 48 ## 1.618733e-13 3.120309e-11 5.037930e-12 4.553236e-13 2.220446e-16 1.148666e-13 ## 49 50 51 52 53 54 ## 2.220446e-16 2.220446e-16 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 55 56 57 58 59 60 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 61 62 63 64 65 66 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 67 68 69 70 71 72 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 73 74 75 76 77 78 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 79 80 81 82 83 84 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 85 86 87 88 89 90 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 91 92 93 94 95 96 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 97 98 99 100 101 102 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 103 104 105 106 107 108 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 109 110 111 112 113 114 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 115 116 117 118 119 120 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 121 122 123 124 125 126 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 127 128 129 130 131 132 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 133 134 135 136 137 138 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 139 140 141 142 143 144 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 145 146 147 148 149 150 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 two_class(data_is_versicolor) ## 1 2 3 4 5 6 7 ## 0.91508678 0.71708213 0.82801733 0.73198542 0.93292480 0.97659676 0.90489939 ## 8 9 10 11 12 13 14 ## 0.87455330 0.62894618 0.69007968 0.94679577 0.85338449 0.65195952 0.71076006 ## 15 16 17 18 19 20 21 ## 0.98537301 0.99578893 0.98602837 0.93433221 0.96257651 0.96652246 0.85535651 ## 22 23 24 25 26 27 28 ## 0.96646344 0.95520481 0.90529428 0.79694417 0.66637629 0.91420835 0.90640934 ## 29 30 31 32 33 34 35 ## 0.89304874 0.76450653 0.71553621 0.93058069 0.97517261 0.98826215 0.74617711 ## 36 37 38 39 40 41 42 ## 0.85527797 0.93130409 0.91134267 0.71884097 0.87722046 0.94059041 0.32816243 ## 43 44 45 46 47 48 49 ## 0.81728097 0.96088927 0.95751567 0.76554646 0.95043481 0.80466977 0.94554618 ## 50 51 52 53 54 55 56 ## 0.85736072 0.73176309 0.80169711 0.67139461 0.22449792 0.54276509 0.38957228 ## 57 58 59 60 61 62 63 ## 0.84119709 0.26480262 0.48001142 0.55343880 0.08486774 0.75194940 0.09707761 ## 64 65 66 67 68 69 70 ## 0.48595238 0.72874996 0.73970106 0.65505546 0.26653438 0.19020107 0.25348721 ## 71 72 73 74 75 76 77 ## 0.84731131 0.57587291 0.24774872 0.29079401 0.56587107 0.67704460 0.42666361 ## 78 79 80 81 82 83 84 ## 0.69212565 0.61297198 0.37091722 0.22219378 0.19791172 0.45168126 0.35209499 ## 85 86 87 88 89 90 91 ## 0.64388465 0.89434765 0.71670119 0.17238522 0.64819734 0.33618752 0.23077053 ## 92 93 94 95 96 97 98 ## 0.58778493 0.35322319 0.21818490 0.41115082 0.55637471 0.55592485 0.55377949 ## 99 100 101 102 103 104 105 ## 0.49487549 0.51907745 0.92128497 0.54353665 0.69789142 0.48049108 0.75012175 ## 106 107 108 109 110 111 112 ## 0.51009989 0.40780938 0.32032711 0.20403096 0.96733885 0.88313482 0.51476423 ## 113 114 115 116 117 118 119 ## 0.78399944 0.49998454 0.86335239 0.92881951 0.59435952 0.92046379 0.29067444 ## 120 121 122 123 124 125 126 ## 0.10390241 0.89714680 0.72025710 0.28831708 0.57007603 0.86304725 0.61212629 ## 127 128 129 130 131 132 133 ## 0.66120010 0.74498856 0.62253140 0.40227498 0.38536752 0.91182791 0.68527934 ## 134 135 136 137 138 139 140 ## 0.36948806 0.11130750 0.78201219 0.95199319 0.65410628 0.76474800 0.84873120 ## 141 142 143 144 145 146 147 ## 0.90433846 0.93550181 0.54353665 0.86745626 0.95036600 0.90153107 0.46737587 ## 148 149 150 ## 0.79113474 0.95014869 0.68141028 two_class(data_is_virginica) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## 1 2 3 4 5 6 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 7 8 9 10 11 12 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 13 14 15 16 17 18 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 19 20 21 22 23 24 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 25 26 27 28 29 30 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 31 32 33 34 35 36 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 37 38 39 40 41 42 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 43 44 45 46 47 48 ## 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 1.000000e+00 ## 49 50 51 52 53 54 ## 1.000000e+00 1.000000e+00 9.999883e-01 9.999514e-01 9.988014e-01 9.999578e-01 ## 55 56 57 58 59 60 ## 9.985915e-01 9.998981e-01 9.986943e-01 1.000000e+00 9.999854e-01 9.999852e-01 ## 61 62 63 64 65 66 ## 1.000000e+00 9.999626e-01 9.999999e-01 9.992011e-01 1.000000e+00 9.999972e-01 ## 67 68 69 70 71 72 ## 9.986740e-01 1.000000e+00 9.404018e-01 9.999999e-01 5.951619e-01 9.999997e-01 ## 73 74 75 76 77 78 ## 7.751662e-01 9.999598e-01 9.999986e-01 9.999929e-01 9.992876e-01 7.239383e-01 ## 79 80 81 82 83 84 ## 9.990348e-01 1.000000e+00 9.999999e-01 1.000000e+00 9.999999e-01 1.323701e-01 ## 85 86 87 88 89 90 ## 9.978308e-01 9.997870e-01 9.997020e-01 9.997449e-01 9.999992e-01 9.999889e-01 ## 91 92 93 94 95 96 ## 9.999603e-01 9.998404e-01 9.999996e-01 1.000000e+00 9.999850e-01 9.999997e-01 ## 97 98 99 100 101 102 ## 9.999969e-01 9.999977e-01 1.000000e+00 9.999977e-01 2.585234e-10 3.860921e-04 ## 103 104 105 106 107 108 ## 9.653910e-07 2.811503e-04 9.071537e-08 4.502238e-09 1.091877e-01 4.498827e-06 ## 109 110 111 112 113 114 ## 7.901235e-06 6.870636e-09 9.741562e-03 2.570987e-04 2.002431e-05 3.272223e-05 ## 115 116 117 118 119 120 ## 8.057896e-08 4.834130e-06 2.300631e-03 7.550917e-08 6.068062e-13 7.950771e-02 ## 121 122 123 124 125 126 ## 3.815786e-07 4.870092e-04 3.671479e-09 5.156610e-02 1.761615e-05 4.413947e-04 ## 127 128 129 130 131 132 ## 1.754560e-01 1.977010e-01 7.647259e-07 2.879872e-02 3.125252e-06 8.108555e-05 ## 133 134 135 136 137 138 ## 1.228427e-07 7.951259e-01 3.359534e-02 1.658605e-08 1.364130e-07 3.502736e-03 ## 139 140 141 142 143 144 ## 3.308575e-01 1.283021e-04 4.927955e-08 5.603902e-05 3.860921e-04 4.523713e-08 ## 145 146 147 148 149 150 ## 1.172982e-08 6.834793e-06 8.933153e-04 1.006087e-03 4.374956e-06 2.232115e-02 此时的概率就可以作为分类的依据，这种找最大值或者最小值的方法被称为hardmax，同理有softmax。softmax的形式是将概率指数化，是一种将概率归一化的方法，求导时softmax的梯度就是对应的softmax值，所以深度学习分类任务中常用softmax而非hardmax。无他，反向传播算的快。 6.2 hardmax result_hardmax = tibble( is_setosa = two_class(data_is_setosa), is_versicolor = two_class(data_is_versicolor), is_virginica = two_class(data_is_virginica) ) %&gt;% mutate( # pmap是一种按行操作的方法 which = pmap_dbl(., ~ which.min(c(...))), prob = pmap_dbl(., ~ c(...)[which.min(c(...))]) ) ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred 6.3 softmax softmax = function(value, values) { exp(value) / sum(exp(values)) } result_softmax = tibble( is_setosa = two_class(data_is_setosa), is_versicolor = two_class(data_is_versicolor), is_virginica = two_class(data_is_virginica) ) %&gt;% mutate( is_setosa_so = pmap_dbl(., ~ softmax(..1, c(..1, ..2, ..3))), is_versicolor_so = pmap_dbl(., ~ softmax(..2, c(..1, ..2, ..3))), is_virginica_so = pmap_dbl(., ~ softmax(..3, c(..1, ..2, ..3))) ) %&gt;% select(contains(&quot;so&quot;)) %&gt;% mutate( which = pmap_dbl(., ~ which.min(c(...))), prob = pmap_dbl(., ~ c(...)[which.min(c(...))]) ) ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred 多分类的评价指标是对数似然损失，交叉熵是对数似然损失的特殊形式。 log_likelihood = function (truth, response) { -log(sum(truth * response)) } 交叉熵的完整写法 truth = list(c(1, 0), c(0, 1)) response = list(c(0.9, 0.1), c(0.2, 0.8)) mean(map2_dbl(truth, response, log_likelihood)) ## [1] 0.164252 二者等价 truth = c(1, 0) response = c(0.9, 0.2) cross_entropy(truth, response) ## [1] 0.164252 6.4 多分类的损失函数 truth = list(c(1, 0, 0), c(0, 1, 0)) response = list(c(0.85, 0.1, 0.05), c(0.15, 0.8, 0.05)) mean(map2_dbl(truth, response, log_likelihood)) ## [1] 0.1928312 "],["特征工程交叉验证和超参数调优.html", "7 特征工程、交叉验证和超参数调优 7.1 特征工程 7.2 超参数调优 7.3 交叉验证", " 7 特征工程、交叉验证和超参数调优 7.1 特征工程 被纳入模型中的变量称为数据的特征。特征工程就是对特征进行操作。在建模技巧相近的情况下，特征工程能给模型带来较大的提升。特征工程的方法有，清洗数据、尝试从复杂的变量（如姓名）中提取模型能够利用的信息、探索两特征的交互项等。 7.1.1 读入数据 首先请读入数据，包括playground的回归数据和泰坦尼克号生存预测的二分类数据，这里分别缩写为pg和tt。由于泰坦尼克早已有完整的数据，为了方便测试，直接读取。 data_pg_train_raw = read_csv(&quot;../playground-regression/data/train.csv&quot;) data_pg_test_raw = read_csv(&quot;../playground-regression/data/test.csv&quot;) data_tt_train_raw = read_csv(&quot;../twoclass/data/train.csv&quot;) data_tt_test_raw = read_csv(&quot;../twoclass/data/test.csv&quot;) data_tt_test_leak = read_csv(&quot;../twoclass/data/titanic.csv&quot;) 操作一下数据。 data_tt_test_raw = data_tt_test_raw %&gt;% mutate(Name = str_replace(Name, &quot;\\&quot;&quot;, &quot;&quot;)) %&gt;% mutate(Name = str_replace(Name, &quot;\\\\\\&quot;&quot;, &quot;&quot;)) data_tt_test_leak = data_tt_test_leak %&gt;% mutate(name = str_replace(name, &quot;\\&quot;&quot;, &quot;&quot;)) %&gt;% mutate(name = str_replace(name, &quot;\\\\\\&quot;&quot;, &quot;&quot;)) data_tt_test_raw = inner_join(data_tt_test_raw, select(data_tt_test_leak, survived, name), by = c(&quot;Name&quot; = &quot;name&quot;)) %&gt;% rename(Survived = survived) %&gt;% distinct(Name, .keep_all = T) # load(&quot;./data/example6.Rdata&quot;) 7.1.2 创建任务 首先查看数据形式，将无需特征工程的pg数据转为mlr3中的任务。 不知道你们从哪学的Task$new()的方法，尽量少用。 task_pg_train = as_task_regr(data_pg_train_raw[, -1], target = &quot;target&quot;) task_pg_test = as_task_regr(mutate(data_pg_test_raw[, -1], target = 1), target = &quot;target&quot;) 接下来是大头。然后观察tt数据的形式。 计算缺失值。r中有用于专业分析缺失值分布规律的包，如mice。但是缺失值的处理一般从简即可。 ## PassengerId Survived Pclass Name Sex Age ## 0 0 0 0 0 177 ## SibSp Parch Ticket Fare Cabin Embarked ## 0 0 0 0 687 2 有关泰坦尼克生存预测的具体特征处理方法，请参考超复杂的特征处理。我这里简化了部分，包括将name列中的的称呼提取并概括为若干类，然后将含有大量的缺失的列中的缺失值填充为未知类，然后将Cabin列中的首字母提取，其意义是客舱，具有一定价值，删除ID、Embarked等列。plural_names这部分没用，但我懒得删。 data_train_mediated1 = data_tt_train_raw %&gt;% mutate(Family = str_extract(Name, &quot;^[^,]+&quot;), Appellation = str_sub(str_extract(Name, &quot;,\\\\s(\\\\w+)(?=\\\\.)&quot;), start = 3), Seat = str_sub(Cabin, end = 1)) a = table(data_train_mediated1$Family) plural_names = names(a)[a != 1] b = table(data_train_mediated1$Appellation) TitleDict = c(Mr = &quot;Mr&quot;, Mlle = &quot;Miss&quot;, Miss = &quot;Miss&quot;, Master = &quot;Master&quot;, Jonkheer = &quot;Master&quot;, Mme = &quot;Mrs&quot;, Ms = &quot;Mrs&quot;, Mrs = &quot;Mrs&quot;, Don = &quot;Master&quot;, Sir = &quot;Master&quot;, Lady = &quot;Miss&quot;, Capt = &quot;Officer&quot;, Col = &quot;Officer&quot;, Major = &quot;Officer&quot;, Dr = &quot;Officer&quot;, Rev = &quot;Officer&quot;) Seats = LETTERS[1:7] lst_par = list(plural_names, TitleDict, Seats) preprocess = function (pars, data_raw) { judge = &quot;Survived&quot; %in% colnames(data_raw) if (judge) { data_raw %&gt;% mutate(Name = str_replace(Name, &quot;\\&quot;&quot;, &quot;&quot;)) %&gt;% mutate(Name = str_replace(Name, &quot;\\\\\\&quot;&quot;, &quot;&quot;)) %&gt;% mutate(Family = str_extract(Name, &quot;^[^,]+&quot;), Appellation = str_sub(str_extract(Name, &quot;,\\\\s(\\\\w+)(?=\\\\.)&quot;), start = 3), Seat = str_sub(Cabin, end = 1)) %&gt;% select(-Name, -Cabin, -PassengerId, -Ticket) %&gt;% mutate(Seat = ifelse(Seat %in% lst_par[[3]], Seat, &quot;Unknown&quot;), # Family = ifelse(Family %in% lst_par[[1]], &quot;pairs&quot;, &quot;Single&quot;), Appellation = lst_par[[2]][Appellation], across(where(is.character), factor), Sex = as.integer(Sex) - 1) %&gt;% select(-Family, -Embarked) } else { data_raw %&gt;% mutate(Name = str_replace(Name, &quot;\\&quot;&quot;, &quot;&quot;)) %&gt;% mutate(Name = str_replace(Name, &quot;\\\\\\&quot;&quot;, &quot;&quot;)) %&gt;% mutate(Family = str_extract(Name, &quot;^[^,]+&quot;), Appellation = str_sub(str_extract(Name, &quot;,\\\\s(\\\\w+)(?=\\\\.)&quot;), start = 3), Seat = str_sub(Cabin, end = 1)) %&gt;% select(-Name, -Cabin, -PassengerId, -Ticket) %&gt;% mutate(Seat = ifelse(Seat %in% lst_par[[3]], Seat, &quot;Unknown&quot;), # Family = ifelse(Family %in% lst_par[[1]], &quot;pairs&quot;, &quot;Single&quot;), Appellation = lst_par[[2]][Appellation], across(where(is.character), factor), Sex = as.integer(Sex) - 1) %&gt;% select(-Family, -Embarked) %&gt;% mutate(Survived = c(rep(0, nrow(.)-1), 1)) } } data_tt_train = preprocess(lst_par, data_tt_train_raw) data_tt_test = preprocess(lst_par, data_tt_test_raw) task_tt_train_raw = as_task_classif(data_tt_train, target = &quot;Survived&quot;) task_tt_test_raw = as_task_classif(data_tt_test, target = &quot;Survived&quot;) 然后对泰坦尼克生存数据中的数据进行进一步处理，包括对含少量缺失值的特征进行插补、将因子型特征进行独热编码。 泰坦尼克生存数据属于分类任务，分类任务需要注意的一点是各个类别的数量要均等，否则会导致模型比较偏好一部分类别，这时候如果还以0.5作为阈值，预测结果就会偏向占比较多的类别。这一现象称作类别不均衡。对此要进行抽样或生成假样本以平衡数据。 但是这个任务十分特殊，即训练集、测试集数据分布不一致。这时候进行抽样或者生成假样本会导致模型错误的重视某一部分样本，导致模型效果反而变差。考虑到样本不均衡其实并不严重，所以我们也不进行平衡样本，只在后面改阈值。 这里为了分辨效果，我们做一组平衡的，做一组不平衡的。 preprocess_pipe = po(&quot;imputelearner&quot;, id = &quot;num&quot;, lrn(&quot;regr.lightgbm&quot;), affect_columns = selector_type(&quot;numeric&quot;)) %&gt;&gt;% po(&quot;imputelearner&quot;, id = &quot;fct&quot;, lrn(&quot;classif.rpart&quot;), affect_columns = selector_type(&quot;factor&quot;)) %&gt;&gt;% po(&quot;encode&quot;, method = &quot;one-hot&quot;) up_sp = po(&quot;classbalancing&quot;, reference = &quot;minor&quot;, adjust = &quot;major&quot;) preprocess_pipe$train(task_tt_train_raw) ## $encode.output ## &lt;TaskClassif:data_tt_train&gt; (891 x 21) ## * Target: Survived ## * Properties: twoclass ## * Features (20): ## - dbl (20): Age, Appellation.Master, Appellation.Miss, ## Appellation.Mr, Appellation.Mrs, Appellation.Officer, ## Appellation.factor, Fare, Parch, Pclass, Seat.A, Seat.B, Seat.C, ## Seat.D, Seat.E, Seat.F, Seat.G, Seat.Unknown, Sex, SibSp task_tt_train_nob = preprocess_pipe$predict(task_tt_train_raw)[[1]] task_tt_train = up_sp$train(list(preprocess_pipe$predict(task_tt_train_raw)[[1]]))[[1]] task_tt_test = preprocess_pipe$predict(task_tt_test_raw)[[1]] 处理好了以后我们使用简单的学习器进行拟合。 toylrn_r = lrn(&quot;regr.lightgbm&quot;) # 用lightgbm的原因是因为它收敛的快 toylrn_r$train(task_pg_train) toypre_r = toylrn_r$predict(task_pg_test) toylrn_c = lrn(&quot;classif.lightgbm&quot;) toylrn_c$train(task_tt_train) toypre_c = toylrn_c$predict(task_tt_test) 设计一个小函数用于汇总结果提交 summary_of = function(pre, is_titanic = F) { if (is_titanic) { tibble( PassengerId = data_tt_test_raw$PassengerId, Survived = pre$response ) } else { tibble( id = data_pg_test_raw$id, target = pre$response ) } } write_csv(summary_of(toypre_r), &quot;temp/submission.csv&quot;) playground的结果为0.7031，模型偏差已经十分逼近随机误差了，再看一下泰坦尼克的结果。 toypre_c$score(msr(&quot;classif.acc&quot;)) ## classif.acc ## 0.7296651 不好也不坏，接下来我们想一下如何优化这两个结果。 7.2 超参数调优 模型参数是模型内部的配置变量，需要用数据估计模型参数的值，随数据变化而变化；模型超参数是模型外部的配置，需要人为设定，不随数据变化而变化。 举深度学习中最简单的多层感知机的例子可能更好懂，参数是每个具体节点的斜率和偏置，超参数是模型的层数和节点数。 下图中的线就是节点中斜率，表示节点之间的联系，节点数和层数就是模型的超参数。 除了这些有形的超参数以外，还有许多更多无形的超参数。模型在不同的超参数下对数据的学习程度也不同。合适的超参数能加速模型收敛、增强模型泛化能力或者对数据的学习程度，不合适的超参数会导致模型压根没法训练。比如knn中如果把k设的过大，会发现所有数据都被分成了一个类。梯度提升时如果学习率设的过高，那模型效果反而会越训练越差。 图7.1: 较小的学习率能够正确拟合模型（左图）；过大的学习率会发生剧烈的震荡，越来越偏离目标（右图） 所以，就此意义上说，超参数最重要的意义是让模型能够训练起来，大多数情况下，一组经过测验的甚至默认固定超参数即可满足需求。超参数调优本质上就是对每一种超参数组合拟合一个模型，以期模型在交叉验证中性能提升。根据调优过程中使用的数值优化的方式不同可分为若干类，常用的策略如下： 网格搜索：对每种超参数组合进行评估，在超参数个数较少的模型上适用，精确度最高。 随机搜索：随机组合超参数，有一定的局部最优的风险，耗时短。 贝叶斯优化：记录每次组合的结果，在以往结果的基础上进行优化。耗时长。由于假设较强，实际套用效果并不佳。 hyperband：综合随机搜索和贝叶斯优化，逐渐缩小搜索空间。实际使用较多。 可以设置若干个超参数组合的效果不如此前最优的超参数组合后终止搜索。 如果一个模型需要调参才能得到某个结果，大概率已经过拟合了。如果调参都得不到某个结果，那只能说明结果本就不存在。另外，模型本身不会保证结果对错，只能给出在模型假设下的推断，但很多模型不但是错的，还没有用。 —于淼 须知，超参数调优只能略微提升训练集上交叉验证的结果，如果超参数搜索区间比较离谱，甚至会比默认参数更差。如果训练集、测试集分布不一致，调优后的结果在测试集上的表现甚至会更差。 7.2.1 mlr3实现 mlr3中使用to_tune()为学习器的超参数指定搜索空间，使用ti()将任务、学习器、指标、终止条件等内容绑定为一个instance（实例），调用tnr()进行调参。 搜索空间与设置普通参数的形式无异： learner_lgb_r = lrn(&quot;regr.lightgbm&quot;) learner_lgb_r$param_set$set_values( learning_rate = to_tune(0.001, 0.1), num_iterations = to_tune(p_int(256, 1024, tags = &quot;budget&quot;)), max_depth = to_tune(1, 10), num_leaves = to_tune(5, 53), bagging_fraction = to_tune(0.75, 1) ) 然后与任务绑定到同一个实例里后使用tnr搜索。也可以使用auto_tuner等函数绑定搜索。 对于决策树、随机森林、xgboost这种著名模型，mlr3在mlr3tuningspace中收集了常用的搜索空间，只需要用lts()括起来就不需要手动设置值了。如下。 learner_xgb_r = lts(lrn(&quot;regr.xgboost&quot;)) 超参数搜索空间一般只用一套固定的经过验证的超参数搜索空间和调参器。只有在复杂的任务上才需要灵活做出调整。 下面的超参数我之前搜索过了，十分耗时，就不再搜索了。 # learner_lgb_r = lrn(&quot;regr.lightgbm&quot;) # learner_lgb_r$param_set$set_values( # learning_rate = to_tune(0.001, 0.1), # num_iterations = to_tune(p_int(256, 1024, tags = &quot;budget&quot;)), # max_depth = to_tune(1, 10), # num_leaves = to_tune(5, 53), # bagging_fraction = to_tune(0.75, 1) # ) # # instance = ti( # task = task_pg_train, # learner = learner_lgb_r, # resampling = rsmp(&quot;holdout&quot;), # measures = msr(&quot;regr.rmse&quot;), # terminator = trm(&quot;evals&quot;, n_evals = 50) # ) # # tuner = tnr(&quot;hyperband&quot;, eta = 2, repetitions = 2) # tuner$optimize(instance) learner_lgb_r = lrn(&quot;regr.lightgbm&quot;) # 这是我之前搜索过的结果 learner_lgb_r$param_set$set_values( learning_rate = 0.0539276, num_iterations = 256, max_depth = 3, num_leaves = 5, bagging_fraction = 0.82 ) learner_lgb_r$train(task_pg_train) prediction_lgb_r = learner_lgb_r$predict(task_pg_test) write_csv(summary_of(prediction_lgb_r), &quot;temp/submission.csv&quot;) 超参数调优后的结果是0.6994。 一鼓作气，我们再试试在泰坦尼克生存预测数据中测试集上的表现。 learner_lgb_c = lrn(&quot;classif.lightgbm&quot;) learner_lgb_c$param_set$set_values( learning_rate = to_tune(0.001, 0.1), num_iterations = to_tune(p_int(256, 1024, tags = &quot;budget&quot;)), max_depth = to_tune(1, 10), num_leaves = to_tune(5, 53), bagging_fraction = to_tune(0.75, 1) # early_stopping = TRUE, # early_stopping_rounds = to_tune(128, 256) ) instance = ti( task = task_tt_train, learner = learner_lgb_c, resampling = rsmp(&quot;holdout&quot;), measures = msr(&quot;classif.acc&quot;), terminator = trm(&quot;evals&quot;, n_evals = 20) ) tuner = tnr(&quot;mbo&quot;) # 贝叶斯优化 tuner$optimize(instance) learner_lgb_c$param_set$values = instance$result_learner_param_vals learner_lgb_c$train(task_tt_train) prediction_lgb_c = learner_lgb_c$predict(task_tt_test) prediction_lgb_c$score(msr(&quot;classif.acc&quot;)) ## classif.acc ## 0.7416268 效果反而变差了（如果文档上显示没有变差，那就是碰巧了，下同理）。我们来进一步分析原因。 cv5_lgb_c = resample(task_tt_train, toylrn_c, rsmp(&quot;cv&quot;, folds = 5)) cv5_lgb_c_tuned = resample(task_tt_train, learner_lgb_c, rsmp(&quot;cv&quot;, folds = 5)) cv5_lgb_c$aggregate(msr(&quot;classif.acc&quot;)); cv5_lgb_c_tuned$aggregate(msr(&quot;classif.acc&quot;)) ## classif.acc ## 0.7996458 ## classif.acc ## 0.8026191 调参的结果在训练集上的交叉验证更好，这是再自然不过的。但是如果训练集和测试集的分布不一致，那就会导致调参的结果在测试集上变差。 7.2.2 使用注意 7.2.2.1 hyperband 使用hyperband调参时，需要为某个参数指定budget标签，意指控制计算资源的超参数，一般设置在GBDT的迭代轮数或树的深度上。 7.2.2.2 early stop 我们再来看几个对比。 简单模型在未经平衡的数据上表现较好。 lrn(&quot;classif.rpart&quot;)$train(task_tt_train)$predict(task_tt_test)$score(msr(&quot;classif.acc&quot;)) ## classif.acc ## 0.7368421 lrn(&quot;classif.rpart&quot;)$train(task_tt_train_nob)$predict(task_tt_test)$score(msr(&quot;classif.acc&quot;)) ## classif.acc ## 0.777512 learner_rpart_c = lts(lrn(&quot;classif.rpart&quot;)) instance = ti( task = task_tt_train_nob, learner = learner_rpart_c, resampling = rsmp(&quot;holdout&quot;), measures = msr(&quot;classif.acc&quot;), terminator = trm(&quot;evals&quot;, n_evals = 20) ) tuner = tnr(&quot;grid_search&quot;) tuner$optimize(instance) learner_rpart_c$param_set$values = instance$result_learner_param_vals learner_rpart_c$train(task_tt_train_nob) 调参后的决策树表现变差。 learner_rpart_c$predict(task_tt_test)$score(msr(&quot;classif.acc&quot;)) ## classif.acc ## 0.7631579 复杂模型在相同的数据上表现劣于简单模型。 lrn(&quot;classif.lightgbm&quot;)$train(task_tt_train_nob)$predict(task_tt_test)$score(msr(&quot;classif.acc&quot;)) ## classif.acc ## 0.7440191 这些都离不开我们所说的训练集测试集分布不一致的问题。模型学习了过多训练集中的特征，部分特征在测试集中并不存在，也可以称之为过拟合的一种（但也未必是过拟合，有可能是由于测试集样本数量较少，不具有代表性）。这种现象在过采样中十分常见。 所以面对这种做法，我们需要尽量减小模型的学习能力。 对于简单模型，提供未经平衡的数据或欠采样的数据；对于GBDT等需要迭代的复杂模型，采用及早终止的方法。设置及早终止，需要将一部分数据划分为验证集，即标签已知但不用于训练只用于评价模型能力的样本。 下面采用及早终止，略微减轻了过拟合，但还是劣于简单的决策树。 split = partition(task_tt_train_nob) task_tt_train_nob_e = task_tt_train_nob$clone(deep = T)$set_row_roles(split$test, &quot;test&quot;) learner_lgb_c = lrn(&quot;classif.lightgbm&quot;) learner_lgb_c$param_set$set_values( early_stopping = TRUE, early_stopping_rounds = 128 ) pre_test = learner_lgb_c$train(task_tt_train_nob_e)$predict(task_tt_test) pre_test$score(msr(&quot;classif.acc&quot;)) ## classif.acc ## 0.784689 须知，不是任何场合都适用早停。如果用于验证的数据过少，很可能导致模型受到这部分数据的约束，从而欠拟合。如果划分的数据过多，那么又会损失宝贵的训练数据。早停只适用于复杂的数据分布上。 由于使用了不平衡数据，我们采用另一种策略平衡模型，调整阈值，也是后处理的一种。 # 注意一定要在训练集上找阈值 pre_train = learner_lgb_c$train(task_tt_train_nob_e)$predict(task_tt_train_nob_e) # optimize是一种一维优化函数，用于使目标函数的值最小 # 我们的目标使得acc最大，也就是使得acc的负值最小 to_optim = function(pre_train, num) { pre_train$set_threshold(num) -pre_train$score(msr(&quot;classif.acc&quot;)) } for_thres = function(pre_train) { thres = optimize(\\(x) to_optim(pre_train, x), lower = 0, upper = 1)$minimum cat(&quot;before: &quot;, pre_train$set_threshold(0.5)$score(msr(&quot;classif.acc&quot;)), &quot;\\n&quot;, &quot;after: &quot;, pre_train$set_threshold(thres)$score(msr(&quot;classif.acc&quot;))) thres } thres = for_thres(pre_train) ## before: 0.8710218 ## after: 0.879397 # 这个方法仅供尝试，未必总是能取得好效果。 pre_test$set_threshold(thres)$score(msr(&quot;classif.acc&quot;)) ## classif.acc ## 0.7799043 7.3 交叉验证 交叉验证是一种抽样策略，k折交叉验证意为将数据随机分为均等的k份，每次用k-1折的数据训练一个模型，再剩下一折上验证，反复k次。k个模型在各自验证集上评价指标取平均，可以相对正确的反映模型的泛化能力。假如训练集和测试集分布一致，交叉验证的结果与在测试集上的表现趋势相同。 上述方法中我们已经看到了用resample和rsmp进行不同策略的抽样以评价模型的实现。下面我们以提升交叉验证表现的方式尝试对playground的结果作进一步优化。 7.3.1 模型堆叠 stack（堆叠）是表格型数据的一种策略，在baseline几乎能完全解释模型方差的当下，其能给模型带来0.1%到1%的提升，这一点点差距就可以在排名上把其它人甩开。但是这种提升是一种双刃剑，以降低模型解释性为代价。 有关stack的介绍可以参考视频和文解。 简而言之，堆叠就是在预测数据的基础上用一个弱学习器综合多个强预测的结果。 load(&quot;./data/hps.Rdata&quot;) learner_xgb_r = lrn(&quot;regr.xgboost&quot;); learner_xgb_r$param_set$values = hp_xgb learner_lgb_r = lrn(&quot;regr.lightgbm&quot;); learner_lgb_r$param_set$values = hp_lgb learner_ctb_r = lrn(&quot;regr.catboost&quot;); learner_ctb_r$param_set$values = hp_ctb # 我们可以使用以下两种方式创建交叉验证的预测结果 # learner_cv默认是3折且无法显式修改，只能通过改源码的方法修改 gstack_test = gunion(list( po(&quot;learner_cv&quot;, learner_xgb_r, id = &quot;xgb&quot;), po(&quot;learner_cv&quot;, learner_lgb_r, id = &quot;lgb&quot;), po(&quot;learner_cv&quot;, learner_ctb_r, id = &quot;ctb&quot;))) %&gt;&gt;% po(&quot;featureunion&quot;) resampled_1 = gstack_test$train(task_pg_train)[[1]] # 此为task对象 cv5_lgb = resample(task_pg_train, learner_lgb_r, rsmp(&quot;cv&quot;, folds = 5)) # 如此重复三次即可 resampled_lgb_2 = cv5_lgb$prediction() # 此为prediction对象 接下来我们训练一个基学习器。岭回归的较为常用的基学习器。 learner_base = lrn(&quot;regr.glmnet&quot;, alpha = 0) search_space = ps( s = p_dbl(lower = 0.001, upper = 2) ) at = auto_tuner( tuner = tnr(&quot;grid_search&quot;), learner = learner_base, resampling = rsmp(&quot;cv&quot;, folds = 5), measure = msr(&quot;regr.rmse&quot;), search_space = search_space, terminator = trm(&quot;evals&quot;, n_evals = 50) ) at$train(resampled_1) hp_base = at$tuning_result$learner_param_vals[[1]] learner_base$param_set$values = hp_base # 用po链接还是一个po，所以最后要转成一个learner glearner = gunion(list( po(&quot;learner_cv&quot;, learner_xgb_r, id = &quot;xgb&quot;), po(&quot;learner_cv&quot;, learner_lgb_r, id = &quot;lgb&quot;), po(&quot;learner_cv&quot;, learner_ctb_r, id = &quot;ctb&quot;))) %&gt;&gt;% po(&quot;featureunion&quot;) %&gt;&gt;% learner_base %&gt;% as_learner() # 再用相同的方法重抽样看效果 cv_stack = resample(task_pg_train, glearner, rsmp(&quot;cv&quot;, folds = 5)) cv_stack$aggregate() 计算速度太慢了，不在文档里计算了。总之cv是要低一点，然后再榜上成绩也达到了0.696。还可以用cv优化超参数进一步提升排名。但是没啥收益，就不卷了。 7.3.2 交叉验证失效怎么办？ 在真实数据中，能够用于训练的数据少，数据的复杂度高。如果交叉验证和在测试集上的表现趋势不明显，主要可能有以下情况： 训练集不具有代表性，测试集的分布与训练集不一致 测试集过小 训练集测试集的特征质量低下，模型在训练集上的cv就不高 此情况下大多数技巧都会失效，对此常用的做法是使用简单的模型。如上面不调参决策树强于调参后的决策树强于lightgbm。在一定范围内降低训练集cv，提升在测试集上的效果。 "],["理解机器学习里数据不均衡.html", "8 理解机器学习里数据不均衡 8.1 data_prepare 8.2 model 8.3 rough edged", " 8 理解机器学习里数据不均衡 8.1 data_prepare mu = rep(0, 2) Sigma1 = matrix(0.7, nrow=2, ncol=2) + diag(2) * 0.3 Sigma2 = matrix(0.5, nrow=2, ncol=2) + diag(2) * 0.5 mat1 = mvrnorm(n = 1000, mu = mu, Sigma = Sigma1) + matrix(rep(c(0, 1), each = 1000), ncol=2) mat2 = mvrnorm(n = 1000, mu = mu, Sigma = Sigma2) + matrix(rep(c(1.5, 0), each = 1000), ncol=2) data_train = rbind(mat1, mat2) %&gt;% as_tibble() %&gt;% mutate(group = factor(rep(c(&quot;G1&quot;, &quot;G2&quot;), each = 1000))) ## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if ## `.name_repair` is omitted as of tibble 2.0.0. ## ℹ Using compatibility `.name_repair`. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. num = 200 data_train_un = rbind(mat1, mat2[sample(1:1000, num), ]) %&gt;% as_tibble() %&gt;% mutate(group = factor(rep(c(&quot;G1&quot;, &quot;G2&quot;), c(1000, num)))) data_test = rbind( mvrnorm(n = 500, mu = mu, Sigma = Sigma1) + matrix(rep(c(0, 1), each = 500), ncol=2), mvrnorm(n = 500, mu = mu, Sigma = Sigma2) + matrix(rep(c(1.5, 0), each = 500), ncol=2) ) %&gt;% as_tibble() %&gt;% mutate(group = factor(rep(c(&quot;G1&quot;, &quot;G2&quot;), each = 500))) data_train %&gt;% ggplot() + geom_point(aes(V1, V2, color = group, alpha = 0.5)) + scale_color_aaas() + xlim(-4, 5) + ylim(-4, 5) + ggtitle(&quot;数据分布&quot;) + theme_bw() + theme( legend.position = &quot;top&quot;, plot.title = element_text(hjust = 0.5) ) data_train_un %&gt;% ggplot() + geom_point(aes(V1, V2, color = group, alpha = 0.5)) + scale_color_aaas() + xlim(-4, 5) + ylim(-4, 5) + ggtitle(&quot;数据分布&quot;) + theme_bw() + theme( legend.position = &quot;top&quot;, plot.title = element_text(hjust = 0.5) ) 8.2 model 8.2.1 数据均衡的模型 model1 = glm(group ~ ., data = data_train, family = binomial()) # as.matrix(data.frame(V0 = 1, V1 = data_train$V1, V2 = data_train$V2)) %*% coef(model1) coef1 = coef(model1) coef1 ## (Intercept) V1 V2 ## -0.6530901 3.4637531 -3.1321564 beta0 = coef1[1] beta1 = coef1[2] beta2 = coef1[3] data_test %&gt;% ggplot() + geom_point(aes(V1, V2, color = group, alpha = 0.5)) + geom_abline(intercept = -beta0/beta2, slope = -beta1/beta2, linetype = &quot;dashed&quot;, linewidth = 1) + scale_color_aaas() + xlim(-4, 5) + ylim(-4, 5) + ggtitle(&quot;数据均衡的模型&quot;) + theme_bw() + theme( legend.position = &quot;top&quot;, plot.title = element_text(hjust = 0.5) ) 8.2.2 数据不均衡的模型 model2 = glm(group ~ ., data = data_train_un, family = binomial()) coef2 = coef(model2) coef2 ## (Intercept) V1 V2 ## -2.377790 3.458357 -2.936726 beta0 = coef2[1] beta1 = coef2[2] beta2 = coef2[3] data_test %&gt;% ggplot() + geom_point(aes(V1, V2, color = group, alpha = 0.5)) + geom_abline(intercept = -beta0/beta2, slope = -beta1/beta2, linetype = &quot;dashed&quot;, linewidth = 1) + scale_color_aaas() + xlim(-4, 5) + ylim(-4, 5) + ggtitle(&quot;数据不均衡的模型&quot;) + theme_bw() + theme( legend.position = &quot;top&quot;, plot.title = element_text(hjust = 0.5) ) coef1; coef2 ## (Intercept) V1 V2 ## -0.6530901 3.4637531 -3.1321564 ## (Intercept) V1 V2 ## -2.377790 3.458357 -2.936726 8.2.3 训练集上后处理准确率的模型 model2 = glm(group ~ ., data = data_train_un, family = binomial()) coef2 = coef(model2) coef2 ## (Intercept) V1 V2 ## -2.377790 3.458357 -2.936726 beta0 = coef2[1] beta1 = coef2[2] beta2 = coef2[3] logit = function(x) exp(x) / (exp(x) + 1) logit_inverse = function(y) log(y) - log(1-y) acc = function(truth, response) sum(truth - response == 0)/length(truth) decide = function(response, thres=0.5) ifelse(response &gt; thres, 1, 0) # acc(as.numeric(data_train_un$group) - 1, decide(logit(predict(model, data_train_un)))) post = optimize(\\(x) -acc(as.numeric(data_train_un$group) - 1, decide(logit(predict(model2, data_train_un)), thres=x)), lower = 0, upper = 1) Z = post$minimum post ## $minimum ## [1] 0.5305961 ## ## $objective ## [1] -0.9608333 data_test %&gt;% ggplot() + geom_point(aes(V1, V2, color = group, alpha = 0.5)) + geom_abline(intercept = Z/beta2 - beta0/beta2, slope = -beta1/beta2, linetype = &quot;dashed&quot;, linewidth = 1) + scale_color_aaas() + xlim(-4, 5) + ylim(-4, 5) + ggtitle(&quot;训练集上后处理准确率的模型&quot;) + theme_bw() + theme( legend.position = &quot;top&quot;, plot.title = element_text(hjust = 0.5) ) 8.2.4 测试集上后处理准确率 model2 = glm(group ~ ., data = data_train_un, family = binomial()) coef2 = coef(model2) coef2 ## (Intercept) V1 V2 ## -2.377790 3.458357 -2.936726 beta0 = coef2[1] beta1 = coef2[2] beta2 = coef2[3] post = optimize(\\(x) -acc(as.numeric(data_test$group) - 1, decide(logit(predict(model2, data_test)), thres=x)), lower = 0, upper = 1) Z = post$minimum post ## $minimum ## [1] 0.2293371 ## ## $objective ## [1] -0.909 data_test %&gt;% ggplot() + geom_point(aes(V1, V2, color = group, alpha = 0.5)) + geom_abline(intercept = Z/beta2 - beta0/beta2, slope = -beta1/beta2, linetype = &quot;dashed&quot;, linewidth = 1) + scale_color_aaas() + xlim(-4, 5) + ylim(-4, 5) + ggtitle(&quot;测试集上后处理准确率&quot;) + theme_bw() + theme( legend.position = &quot;top&quot;, plot.title = element_text(hjust = 0.5) ) 8.2.5 过采样的模型 up_sample = function(tb, index) tb[c(1:1000, sample(index, 1000, replace = T)), ] data_train_un_up = up_sample(data_train_un, 1001:1200) model_up = glm(group ~ ., data = data_train_un_up, family = binomial()) coef2 = coef(model_up) beta0 = coef2[1] beta1 = coef2[2] beta2 = coef2[3] data_test %&gt;% ggplot() + geom_point(aes(V1, V2, color = group, alpha = 0.5)) + geom_abline(intercept = -beta0/beta2, slope = -beta1/beta2, linetype = &quot;dashed&quot;, linewidth = 1) + scale_color_aaas() + xlim(-4, 5) + ylim(-4, 5) + ggtitle(&quot;过采样的模型&quot;) + theme_bw() + theme( legend.position = &quot;top&quot;, plot.title = element_text(hjust = 0.5) ) 8.2.6 欠采样的模型 down_sample = function(tb, index) tb[c(1001:1200, sample(index, 200)), ] data_train_un_down = down_sample(data_train_un, 1:1000) model_down = glm(group ~ ., data = data_train_un_down, family = binomial()) coef2 = coef(model_down) beta0 = coef2[1] beta1 = coef2[2] beta2 = coef2[3] data_test %&gt;% ggplot() + geom_point(aes(V1, V2, color = group, alpha = 0.5)) + geom_abline(intercept = -beta0/beta2, slope = -beta1/beta2, linetype = &quot;dashed&quot;, linewidth = 1) + scale_color_aaas() + xlim(-4, 5) + ylim(-4, 5) + ggtitle(&quot;欠采样的模型&quot;) + theme_bw() + theme( legend.position = &quot;top&quot;, plot.title = element_text(hjust = 0.5) ) 8.2.7 添加权重的模型 model3 = glm(group ~ ., data = data_train_un, family = binomial(), weights = rep(c(1, 5), c(1000, 200))) coef3 = coef(model3) coef3 ## (Intercept) V1 V2 ## -0.7018305 3.1359096 -2.6647095 beta0 = coef3[1] beta1 = coef3[2] beta2 = coef3[3] data_test %&gt;% ggplot() + geom_point(aes(V1, V2, color = group, alpha = 0.5)) + geom_abline(intercept = -beta0/beta2, slope = -beta1/beta2, linetype = &quot;dashed&quot;, linewidth = 1) + scale_color_aaas() + xlim(-4, 5) + ylim(-4, 5) + ggtitle(&quot;添加权重的模型&quot;) + theme_bw() + theme( legend.position = &quot;top&quot;, plot.title = element_text(hjust = 0.5) ) library(mlr3verse) ## Loading required package: mlr3 ## Warning: package &#39;mlr3&#39; was built under R version 4.3.2 pre = data.table( row_ids = 1:2000, truth = factor(as.numeric(data_train$group) - 1), response = factor(round(logit(predict(model1, data_train)))), prob.0 = 1-logit(predict(model1, data_train)), prob.1 = logit(predict(model1, data_train)) ) %&gt;% as_prediction_classif() pre$score(msr(&quot;classif.prauc&quot;)) ## classif.prauc ## 0.9766404 8.3 rough edged angle = -30 mat3 = mat1 %*% matrix(c(sin(angle), cos(angle), cos(angle), -sin(angle)), ncol = 2) + matrix(rep(c(0.5, 3.5), each = 1000), ncol=2) mat4 = rbind(mat1, mat3) ind = sample(2000, 1000) data_train_r = rbind(mat4[ind, ], mat2) %&gt;% as_tibble() %&gt;% mutate(group = factor(rep(c(&quot;G1&quot;, &quot;G2&quot;), each = 1000))) num = 200 data_train_r_un = rbind(mat4[ind, ], mat2[sample(1:1000, num), ]) %&gt;% as_tibble() %&gt;% mutate(group = factor(rep(c(&quot;G1&quot;, &quot;G2&quot;), c(1000, num)))) data_test_r = rbind( mat4[-ind, ][sample(1000, 500), ], mvrnorm(n = 500, mu = mu, Sigma = Sigma2) + matrix(rep(c(1.5, 0), each = 500), ncol=2) ) %&gt;% as_tibble() %&gt;% mutate(group = factor(rep(c(&quot;G1&quot;, &quot;G2&quot;), each = 500))) # model_r1 = glm(group ~ ., data = data_train_r, family = binomial()) coef1 = coef(model_r1) coef1 ## (Intercept) V1 V2 ## -0.06661368 2.13031750 -2.58225266 beta0 = coef1[1] beta1 = coef1[2] beta2 = coef1[3] data_test_r %&gt;% ggplot() + geom_point(aes(V1, V2, color = group, alpha = 0.5)) + geom_abline(intercept = -beta0/beta2, slope = -beta1/beta2, linetype = &quot;dashed&quot;, linewidth = 1) + scale_color_aaas() + xlim(-4, 5) + ylim(-4, 5) + ggtitle(&quot;线性不可分的均衡数据上的决策边界&quot;) + theme_bw() + theme( legend.position = &quot;top&quot;, plot.title = element_text(hjust = 0.5) ) ## Warning: Removed 1 rows containing missing values (`geom_point()`). model_r2 = glm(group ~ ., data = data_train_r_un, family = binomial()) coef2 = coef(model_r2) coef2 ## (Intercept) V1 V2 ## -1.411091 1.773981 -2.381293 beta0 = coef2[1] beta1 = coef2[2] beta2 = coef2[3] data_test_r %&gt;% ggplot() + geom_point(aes(V1, V2, color = group, alpha = 0.5)) + geom_abline(intercept = -beta0/beta2, slope = -beta1/beta2, linetype = &quot;dashed&quot;, linewidth = 1) + scale_color_aaas() + xlim(-4, 5) + ylim(-4, 5) + ggtitle(&quot;线性不可分的不均衡数据上的决策边界&quot;) + theme_bw() + theme( legend.position = &quot;top&quot;, plot.title = element_text(hjust = 0.5) ) ## Warning: Removed 1 rows containing missing values (`geom_point()`). "],["rmarkdown中插入文献和修改引用样式.html", "9 （R）markdown中插入文献和修改引用样式 9.1 Rmarkdown的组成 9.2 插入参考文献 9.3 修改引用样式 9.4 其它技巧和问题 9.5 参考文献", " 9 （R）markdown中插入文献和修改引用样式 markdown是一类轻量级的标记语言，是被二次封装html文本，具有简洁、规范、高效的优点，为各大平台所支持。市面上常用的markdown编辑器有typora、Typedown、Rmarkdown等，除了支持原生的markdown语法外，还不同程度的支持markdown的拓展语法或Tex语法，其中Rmarkdown不仅部分支持markdown拓展语法（不支持的语法也可以使用html和tex自定义），其最主要的特点还是原生的支持R、Python等语言的运行结果在文本中的复杂展示，所谓文学化编程。如今Rmarkdown已发展成一个完整的生态，包含Bookdown、Blogdown、Pkgdown等html衍生，能够高效的制作出美观的展示页面。 9.1 Rmarkdown的组成 图9.1: Rmarkdown由三部分组成：Yaml头、可随处插入的代码块、文本 Rmarkdown由三部分组成：Yaml头、可随处插入的代码块、文本其中Yaml头存储了这篇文档将被导出为什么格式、使用的模板等元信息、代码块可以用于插入图片、运行结果等用途，文本则是我们我们正在写作的部分。 9.2 插入参考文献 在Rmarkdown中插入参考文献至少需要在Yaml头中设置bibliography，其后为一个bib或bibtex文件。bib文件形如： 图9.2: bib文件就是被整理规范的引用信息 于{前的形如“anderson_macrophage-based_2021”字样的即为这篇文献的唯一ID。我们可以@它的ID以引用它1，只要[@anderson_macrophage-based_2021]1就可以引用它，好处就是添加参考文献时不再需要手动更改footnote。 我们可以从endnote或者zotero中导出参考文献列表形成bib文件。 图9.3: 导出即可 9.3 修改引用样式 我们可以通过两种方式修改引用样式，一是修改yaml头中的biblio-style，其内包含一系列预设的样式。但是多数情况下我们需要使用期刊推荐的样式，这时候需要在yaml头中添加csl（引用样式列表）文件。 我们可以从zotero官网上查找各期刊的引用格式，如下： 图9.4: 点击source字样 光标移至附近点击source字样，然后将其内的代码复制到本地并命名。即可在文本中使用。 9.4 其它技巧和问题 9.4.1 快捷键 我们可以通过devtools::install_github(\"crsh/citr\")安装citr包，就可以在插件里找到Insert Citation功能，点击他会开启一个shiny程序，自动搜索当下的bib文件，提供便捷的引用功能。 图9.5: 点击source字样 我们还可以给它设置一个全局快捷键以方便使用，参考设置Rstudio快捷键，我设置的是Ctrl Shift V。 9.4.2 引用多篇 用分号隔开即可，如[@anderson_macrophage-based_2021; @liu_s100a4_2021]1,2。 9.4.3 导出注意事项 markdown文件导出docx依赖于pandoc，导出html文件依赖于htmlx，导出pdf依赖于Tex。所以除了原生的markdown及其拓展语法外，互不支持。 htmlx支持html、pandoc和tex语法，Tex支持tex语法，pandoc支持pandoc语法。所以如果要导出多种格式，还需要再次修改。 9.5 参考文献 References 1. Anderson, N. R., Minutolo, N. G., Gill, S. &amp; Klichinsky, M. Macrophage-Based Approaches for Cancer Immunotherapy. Cancer Research 81, 1201–1208 (2021). 2. Liu, S. et al. S100A4 enhances protumor macrophage polarization by control of PPAR-γ-dependent induction of fatty acid oxidation. Journal For Immunotherapy of Cancer 9, (2021). "],["puzzle1展开折叠数据.html", "10 puzzle1：展开折叠数据 10.1 数据展开 10.2 模型建立", " 10 puzzle1：展开折叠数据 10.1 数据展开 Cattle = fread( &quot; id group dfactor cattle infect 1 1 1 11 8 2 1 2 10 7 3 1 3 12 5 4 1 4 11 3 5 1 5 12 2 6 2 1 10 10 7 2 2 10 9 8 2 3 12 8 9 2 4 11 6 10 2 5 10 4 &quot; ) infect_ = Cattle$infect cattle_ = Cattle$cattle Cattle_expanded = copy(Cattle)[rep(id, cattle), ] Cattle_expanded[, infect := unlist(map2(infect_, cattle_ - infect_, ~ rep(c(1, 0), c(.x, .y))))] Cattle_expanded ## id group dfactor cattle infect ## 1: 1 1 1 11 1 ## 2: 1 1 1 11 1 ## 3: 1 1 1 11 1 ## 4: 1 1 1 11 1 ## 5: 1 1 1 11 1 ## --- ## 105: 10 2 5 10 0 ## 106: 10 2 5 10 0 ## 107: 10 2 5 10 0 ## 108: 10 2 5 10 0 ## 109: 10 2 5 10 0 Cattle %&gt;% group_by(id) %&gt;% mutate(infected = list(c(rep(1, infect), rep(0, cattle-infect)))) %&gt;% unnest() %&gt;% View() ## Warning: `cols` is now required when using `unnest()`. ## ℹ Please use `cols = c(infected)`. Cattle %&gt;% group_nest(id) %&gt;% mutate(infected = map(data, ~ c(rep(1, .x$infect), rep(0, .x$cattle-.x$infect)))) %&gt;% unnest(data) %&gt;% unnest(infected) %&gt;% select(-infect) ## # A tibble: 109 × 5 ## id group dfactor cattle infected ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 1 11 1 ## 2 1 1 1 11 1 ## 3 1 1 1 11 1 ## 4 1 1 1 11 1 ## 5 1 1 1 11 1 ## 6 1 1 1 11 1 ## 7 1 1 1 11 1 ## 8 1 1 1 11 1 ## 9 1 1 1 11 0 ## 10 1 1 1 11 0 ## # ℹ 99 more rows 10.2 模型建立 Model1 = glm(cbind(infect, cattle - infect) ~ factor(group) + dfactor, family = binomial(link = logit), data = Cattle) summary(Model1) ## ## Call: ## glm(formula = cbind(infect, cattle - infect) ~ factor(group) + ## dfactor, family = binomial(link = logit), data = Cattle) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.1310 0.6113 3.486 0.00049 *** ## factor(group)2 1.3059 0.4654 2.806 0.00502 ** ## dfactor -0.7874 0.1813 -4.342 1.41e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 33.5256 on 9 degrees of freedom ## Residual deviance: 2.4508 on 7 degrees of freedom ## AIC: 32.254 ## ## Number of Fisher Scoring iterations: 4 Model2 = glm(infect ~ factor(group) + dfactor, family = binomial(link = logit), data = Cattle_expanded) summary(Model2) ## ## Call: ## glm(formula = infect ~ factor(group) + dfactor, family = binomial(link = logit), ## data = Cattle_expanded) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.1310 0.6113 3.486 0.00049 *** ## factor(group)2 1.3059 0.4654 2.806 0.00502 ** ## dfactor -0.7874 0.1813 -4.342 1.41e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 149.04 on 108 degrees of freedom ## Residual deviance: 117.96 on 106 degrees of freedom ## AIC: 123.96 ## ## Number of Fisher Scoring iterations: 4 "],["puzzle2初试假设检验.html", "11 puzzle2：初试假设检验 11.1 示例 11.2 题目", " 11 puzzle2：初试假设检验 11.1 示例 r中的rnorm函数可以生成若干符合正态分布的数。我们现生成100个总体均值未知（假装未知），总体方差为1的符合正态分布的数。但是先让我假设不知道这列数的总体参数是什么。 x = 1 samples = rnorm(n = 10000, mean = x, sd = 1) 假设这些数的总体均数均数不可能小于0，我们要检验这些数的总体均值是等于0还是大于0。 \\[ 原假设H_0: \\mu = 0 \\\\ 备择假设H_1: \\mu &gt; 0 \\] 在\\(\\alpha = 0.05\\)的水准上： 能否拒绝\\(H_0\\) 有多少的把握(power)能够拒绝\\(H_0\\) 11.1.1 计算 在原假设H0为真的情况下： 样本服从总体均值\\(\\bar{\\mu}\\)为0，总体方差\\(\\bar{\\sigma}^2\\)为1的正态分布。依据中心极限定理，从总体中抽样的样本均值\\(\\hat{\\mu}\\)服从 \\[ \\hat{\\mu} \\sim N(\\bar{\\mu}, \\bar{\\sigma}^2/n) \\] ggplot(tibble(x = c(-1.2, 1.2)), aes(x = x)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 1/10), color = &quot;firebrick&quot;) + labs(title = &quot;均值分布曲线&quot;) + theme_bw() + theme(plot.title = element_text(hjust = 0.5)) 在\\(\\alpha = 0.05\\)的水准上，拒绝域为0.95，可以用qnorm计算得给定概率0.95后的下分位点，对于标准正态分布来说是1.6448536，当样本均值\\(\\hat{\\mu}\\)&gt;\\(1.644 \\times \\bar{\\sigma}/\\sqrt{n}\\)时，认为可以拒绝\\(H_0\\)。 样本均值为0.9979786，大于0.01644，落在拒绝域 ggplot(tibble(x = c(-1.2, 1.2)), aes(x = x)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 1/10), color = &quot;firebrick&quot;) + geom_vline(xintercept = mean(samples), linetype = 5, color = &quot;navy&quot;) + geom_vline(xintercept = qnorm(0.95) * 1/10, linetype = 5) + labs(title = &quot;均值分布曲线&quot;) + theme_bw() + theme(plot.title = element_text(hjust = 0.5)) 图9.3: α值就是黑色虚线后的曲线下面积 故在\\(\\alpha = 0.05\\)的水准上，能够拒绝\\(H_0\\)。 接下来我们计算β值。 根据最大似然，样本均数最有可能服从总体均数为样本均数，标准差为1/10（由于总体方差已知）的正态分布。当\\(\\alpha = 0.05\\)时，拒绝域分位值为0.1644854。β值根据定义为犯二类错误，也就是假阴性的概率，所以可以为黑色虚线前的曲线下面积。 ggplot(tibble(x = c(0, 2)), aes(x = x)) + stat_function(fun = dnorm, args = list(mean = mean(samples), sd = 1/10), color = &quot;firebrick&quot;) + geom_vline(xintercept = mean(samples), linetype = 5, color = &quot;navy&quot;) + geom_vline(xintercept = qnorm(0.95) * 1/10, linetype = 5) + labs(title = &quot;均值分布曲线&quot;) + theme_bw() + theme(plot.title = element_text(hjust = 0.5)) 图9.4: β值就是黑色虚线前的曲线下面积 可计算得\\(\\beta = 5.6e-18\\)，把握值接近于1。 最后我们可以把两组图像画在一起，更直观的观察一下。 ggplot(tibble(x = c(-1.2, 2)), aes(x = x)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 1/10), color = &quot;#CD853F&quot;) + stat_function(fun = dnorm, args = list(mean = mean(samples), sd = 1/10), color = &quot;firebrick&quot;) + geom_vline(xintercept = mean(samples), linetype = 5, color = &quot;navy&quot;) + geom_vline(xintercept = qnorm(0.95) * 1/10, linetype = 5) + labs(title = &quot;均值分布曲线&quot;) + theme_bw() + theme(plot.title = element_text(hjust = 0.5)) 11.2 题目 让我们去除这些数的总体均数均数不可能小于0的假设，现在的备择假设是双侧的。 \\[ 原假设H_0: \\mu = 0 \\\\ 备择假设H_1: \\mu \\neq 0 \\] 在\\(\\alpha = 0.05\\)的水准上： 能否拒绝\\(H_0\\) 有多少的把握(power)能够拒绝\\(H_0\\) 一个人声称自己能分辨出星巴克和瑞幸咖啡口味的分别，现有十杯咖啡请他品尝。这个人至少要分辨对多少杯，才在\\(\\alpha = 0.1\\)的水准上，拒绝这个人在胡说八道（即完全瞎猜，50%的概率预测成功）的假设？ 1. Anderson, N. R., Minutolo, N. G., Gill, S. &amp; Klichinsky, M. Macrophage-Based Approaches for Cancer Immunotherapy. Cancer Research 81, 1201–1208 (2021). 2. Liu, S. et al. S100A4 enhances protumor macrophage polarization by control of PPAR-γ-dependent induction of fatty acid oxidation. Journal For Immunotherapy of Cancer 9, (2021). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
