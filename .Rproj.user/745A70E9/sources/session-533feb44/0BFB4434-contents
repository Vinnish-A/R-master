---
title: "20231023报告"
author: "MITUS团队"
date: "2023-09-22"
output: 
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

```{r setup, include=FALSE}
set.seed(10)

knitr::opts_chunk$set(echo = TRUE)
```

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy(position = c("top", "right"))
```

# 概要

本报告将为您提供以下信息：

1. 复杂模型xgboost的全新调参策略
2. 效果较优的简单模型
3. 复杂模型失效的原因

# 复杂的模型不能取得好效果

建模中，一般意义上效果最好的通用模型是以xgboost为代表的GBDT模型，在表格数据上的表现远胜其它模型（包括深度学习）。但是在数据噪声极大的转录组数据上表现极差，所以不应采用复杂模型，而应采用简单的模型。

```{r message=FALSE, warning=FALSE}
set.seed(1001)

library(tidyverse)
library(mlr3verse)
library(mlr3proba)

raw = read_csv("TCGA-log2TPM+1.csv")
clinical = read_csv("TCGA-clinical.csv")
# gene = openxlsx::read.xlsx("筛选基因.xlsx")[[1]]
gene_shared = readRDS("gene_shared.Rds")
gene = gene_shared

data_tidy = raw %>% 
  filter(Tag %in% gene) %>% 
  pivot_longer(-Tag, names_to = "Sample", values_to = "Expression") %>% 
  mutate(Sample = gsub("\\.", "-", Sample)) %>% 
  pivot_wider(names_from = "Tag", values_from = "Expression") %>%
  filter(!str_detect(Sample, "-11")) %>% 
  mutate(Sample = str_sub(Sample, end = -4)) %>% 
  distinct(Sample, .keep_all = T) %>% 
  inner_join(clinical, ., by = "Sample") %>% 
  select(-Sample) %>% 
  filter(OS.time > 30) %>% 
  rename_all(~ gsub("[.-]", "_", .x)) %>% 
  mutate(across(where(is.character), as.factor))

task = as_task_surv(data_tidy, time = "OS_time", event = "OS", type = "right")

preprocess_pipe = 
  po("imputelearner", id = "num", lrn("regr.lightgbm"), affect_columns = selector_type("numeric")) %>>% 
  po("imputelearner", id = "fct", lrn("classif.rpart"), affect_columns = selector_type("factor")) %>>% 
  po("encode", method = "one-hot")

preprocess_pipe$train(task)
task_train = preprocess_pipe$predict(task)[[1]]

split = partition(task_train, 0.8)
task_train$set_row_roles(split$test, "test")
```

这里与上次不同的是为了让xgboost避免过拟合，采用了更广泛的搜索空间和“早停”策略。就是在训练数据中划分一部分验证数据，在验证数据性能下降时终止训练。

```{r message=FALSE, warning=FALSE}
learner = lrn("surv.xgboost")

learner$param_set$set_values(
  tree_method = "gpu_hist", # 这里用的是gpu版本的xgboost
  booster = "gbtree", # 也可以尝试泛化能力更强的dart，但是速度会慢的难以接受
  nrounds = to_tune(p_int(128, 512, tags = "budget")),
  eta = to_tune(1e-4, 1, logscale = TRUE),
  gamma = to_tune(1e-5, 7, logscale = TRUE), 
  max_depth = to_tune(1, 20),
  colsample_bytree = to_tune(1e-2, 1),
  colsample_bylevel = to_tune(1e-2, 1),
  lambda = to_tune(1e-3, 1e3, logscale = TRUE),
  alpha = to_tune(1e-3, 1e3, logscale = TRUE),
  subsample = to_tune(1e-1, 1), 
  early_stopping_rounds = 120,
  early_stopping_set = "test"
)

instance = ti(
  task = task_train,
  learner = learner,
  resampling = rsmp("holdout"),
  measures = msr("surv.cindex"),
  terminator = trm("evals", n_evals = 5)
)

tuner = tnr("hyperband", eta = 2, repetitions = 1)

tuner$optimize(instance)

learner$param_set$values = instance$result_learner_param_vals
learner$train(task_train, row_ids = split$train)

prediction_train = learner$predict(task_train, row_ids = split$train)
prediction_test = learner$predict(task_train, row_ids = split$test)
```

xgboost在验证集上的效果超过了coxph。但是这并不意味着这里xgboost的泛化能力优于线性模型。合理的解释是xgboost在验证集上过拟合了。

```{r message=FALSE, warning=FALSE}
prediction_test$score()
```

```{r message=FALSE, warning=FALSE}
learner_cph = lrn("surv.coxph")$train(task_train, row_ids = split$train)

prediction_cph = learner_cph$predict(task_train, row_ids = split$test)
prediction_cph$score()
```

接下来处理GEO数据

```{r message=FALSE, warning=FALSE}

GSE19234 = GEOquery::getGEO(filename = "GSE19234_series_matrix.txt", getGPL = F)
data_GSE19234 = GSE19234 %>% 
  as_tibble(rownames = "Sample")

load("GPL570_bioc.rda")
probeid = GPL570_bioc$probe_id
names(probeid) = GPL570_bioc$symbol

filteron = function(element) {
  if (element %in% names(probeid)) {
    probeid[[element]]
  } else {
    NA
  }
}

index = sapply(gene, filteron)
index = index[!is.na(index)]
gene_shared = names(index)
# saveRDS(gene_shared, file = "gene_shared.Rds")

data_GSE19234_tidy = data_GSE19234 %>% 
  rename_with(~ str_sub(.x, start = 2), contains("X")) %>% 
  select(Sample, all_of(index)) %>% 
  set_names(c("Sample", names(index))) %>% 
  rename_all(~ gsub("[.-]", "_", .x)) %>% 
  mutate(across(where(is.numeric), ~ log2(.x + 1)), 
         OS_time = as.numeric(GSE19234$`days since initial diagnosis:ch1`), 
         OS = as.numeric(GSE19234$`staus dead or alive:ch1`), 
         age = as.numeric(GSE19234$`age:ch1`), 
         stage = as.factor(paste0("Stage ", GSE19234$`stage at reccurence:ch1`))) %>% 
  select(-Sample)

task_GSE19234_raw = as_task_surv(data_GSE19234_tidy, time = "OS_time", event = "OS", type = "right")
task_GSE19234 = preprocess_pipe$predict(task_GSE19234_raw)[[1]]
```

可以看到对数值敏感的xgboost在处理芯片数据时完全没有效力。

```{r message=FALSE, warning=FALSE}
predicition_xgb = learner$predict(task_GSE19234)
predicition_xgb$score()
```

```{r message=FALSE, warning=FALSE}
prediction_cph = learner_cph$predict(task_GSE19234)
prediction_cph$score()
```
同样的，较为简单的SVM在此类任务上表现更好

```{r message=FALSE, warning=FALSE}
svm_test = read_csv("yy/SVM_test.csv")
svm_geo = read_csv("yy/SVM_geo.csv")

library(survcomp)

concordance.index(svm_test$score, surv.time = svm_test$OS.time, surv.event = svm_test$OS)$c.index
concordance.index(svm_geo$score, surv.time = svm_geo$OS.time, surv.event = svm_geo$OS)$c.index
```
# 为什么复杂的模型会失效？

每个基因的c指数如下所示。GBDT模型属于树模型，树模型的特点是总结各变量间的关系，所以如果变量中有预测效果特别好的基因，那么就能够取长补短。但如果如下所示变量都很差。那么树模型就会错误的总结特征。而线性模型假设数据独立，所以模型的效果就接近表现最好的基因，相对稳定。

```{r}
cal_cindex = function(vec, time = data_tidy$OS_time, event = data_tidy$OS) {
  concordance.index(vec, time, event)$c.index
}

data_tidy %>% 
  select(!1:4) %>% 
  map_dbl(~ abs(cal_cindex(vec = .x) - 0.5) + 0.5) 
```




