plural_names = names(a)[a != 1]
b = table(data_train_mediated1$Appellation)
TitleDict = c(Mr = "Mr",
Mlle = "Miss", Miss = "Miss",
Master = "Master", Jonkheer = "Master",
Mme = "Mrs", Ms = "Mrs", Mrs = "Mrs",
Don = "Master", Sir = "Master",
Lady = "Miss",
Capt = "Officer", Col = "Officer", Major = "Officer", Dr = "Officer", Rev = "Officer")
Seats = LETTERS[1:7]
lst_par = list(plural_names, TitleDict, Seats)
preprocess = function (pars, data_raw) {
judge = "Survived" %in% colnames(data_raw)
if (judge) {
data_raw %>%
mutate(Name = str_replace(Name, "\"", "")) %>%
mutate(Name = str_replace(Name, "\\\"", "")) %>%
mutate(Family = str_extract(Name, "^[^,]+"),
Appellation = str_sub(str_extract(Name, ",\\s(\\w+)(?=\\.)"), start = 3),
Seat = str_sub(Cabin, end = 1)) %>%
select(-Name, -Cabin, -PassengerId, -Ticket) %>%
mutate(Seat = ifelse(Seat %in% lst_par[[3]], Seat, "Unknown"),
# Family = ifelse(Family %in% lst_par[[1]], "pairs", "Single"),
Appellation = lst_par[[2]][Appellation],
across(where(is.character), factor),
Sex = as.integer(Sex) - 1) %>%
select(-Family, -Embarked)
} else {
data_raw %>%
mutate(Name = str_replace(Name, "\"", "")) %>%
mutate(Name = str_replace(Name, "\\\"", "")) %>%
mutate(Family = str_extract(Name, "^[^,]+"),
Appellation = str_sub(str_extract(Name, ",\\s(\\w+)(?=\\.)"), start = 3),
Seat = str_sub(Cabin, end = 1)) %>%
select(-Name, -Cabin, -PassengerId, -Ticket) %>%
mutate(Seat = ifelse(Seat %in% lst_par[[3]], Seat, "Unknown"),
# Family = ifelse(Family %in% lst_par[[1]], "pairs", "Single"),
Appellation = lst_par[[2]][Appellation],
across(where(is.character), factor),
Sex = as.integer(Sex) - 1) %>%
select(-Family, -Embarked) %>%
mutate(Survived = c(rep(0, nrow(.)-1), 1))
}
}
data_tt_train = preprocess(lst_par, data_tt_train_raw)
data_tt_test  = preprocess(lst_par, data_tt_test_raw)
task_tt_train_raw = as_task_classif(data_tt_train, target = "Survived")
task_tt_test_raw  = as_task_classif(data_tt_test, target = "Survived")
preprocess_pipe =
po("imputelearner", id = "num", lrn("regr.lightgbm"), affect_columns = selector_type("numeric")) %>>%
po("imputelearner", id = "fct", lrn("classif.rpart"), affect_columns = selector_type("factor")) %>>%
po("encode", method = "one-hot")
up_sp = po("classbalancing", reference = "minor",
adjust = "major")
preprocess_pipe$train(task_tt_train_raw)
task_tt_train_nob = preprocess_pipe$predict(task_tt_train_raw)[[1]]
task_tt_train = up_sp$train(list(preprocess_pipe$predict(task_tt_train_raw)[[1]]))[[1]]
task_tt_test  = preprocess_pipe$predict(task_tt_test_raw)[[1]]
toylrn_r = lrn("regr.lightgbm") # 用lightgbm的原因是因为它收敛的快
toylrn_r$train(task_pg_train)
toypre_r = toylrn_r$predict(task_pg_test)
toylrn_c = lrn("classif.lightgbm")
toylrn_c$train(task_tt_train)
toypre_c = toylrn_c$predict(task_tt_test)
summary_of = function(pre, is_titanic = F) {
if (is_titanic) {
tibble(
PassengerId = data_tt_test_raw$PassengerId,
Survived = pre$response
)
} else {
tibble(
id = data_pg_test_raw$id,
target = pre$response
)
}
}
write_csv(summary_of(toypre_r), "temp/submission.csv")
toypre_c$score(msr("classif.acc"))
knitr::include_graphics("./pic/6-dp.jpg")
knitr::include_graphics(c("./pic/6-lr1.jpg", "./pic/6-lr2.jpg"))
learner_lgb_r = lrn("regr.lightgbm")
learner_lgb_r$param_set$set_values(
learning_rate = to_tune(0.001, 0.1),
num_iterations = to_tune(p_int(256, 1024, tags = "budget")),
max_depth = to_tune(1, 10),
num_leaves = to_tune(5, 53),
bagging_fraction = to_tune(0.75, 1)
)
learner_xgb_r = lts(lrn("regr.xgboost"))
# learner_lgb_r = lrn("regr.lightgbm")
# learner_lgb_r$param_set$set_values(
#   learning_rate = to_tune(0.001, 0.1),
#   num_iterations = to_tune(p_int(256, 1024, tags = "budget")),
#   max_depth = to_tune(1, 10),
#   num_leaves = to_tune(5, 53),
#   bagging_fraction = to_tune(0.75, 1)
# )
#
# instance = ti(
#   task = task_pg_train,
#   learner = learner_lgb_r,
#   resampling = rsmp("holdout"),
#   measures = msr("regr.rmse"),
#   terminator = trm("evals", n_evals = 50)
# )
#
# tuner = tnr("hyperband", eta = 2, repetitions = 2)
# tuner$optimize(instance)
learner_lgb_r = lrn("regr.lightgbm")
# 这是我之前搜索过的结果
learner_lgb_r$param_set$set_values(
learning_rate = 0.0539276,
num_iterations = 256,
max_depth = 3,
num_leaves = 5,
bagging_fraction = 0.82
)
learner_lgb_r$train(task_pg_train)
prediction_lgb_r = learner_lgb_r$predict(task_pg_test)
write_csv(summary_of(prediction_lgb_r), "temp/submission.csv")
learner_lgb_c = lrn("classif.lightgbm")
learner_lgb_c$param_set$set_values(
learning_rate = to_tune(0.001, 0.1),
num_iterations = to_tune(p_int(256, 1024, tags = "budget")),
max_depth = to_tune(1, 10),
num_leaves = to_tune(5, 53),
bagging_fraction = to_tune(0.75, 1)
# early_stopping = TRUE,
# early_stopping_rounds = to_tune(128, 256)
)
instance = ti(
task = task_tt_train,
learner = learner_lgb_c,
resampling = rsmp("holdout"),
measures = msr("classif.acc"),
terminator = trm("evals", n_evals = 20)
)
tuner = tnr("mbo") # 贝叶斯优化
tuner$optimize(instance)
learner_lgb_c$param_set$values = instance$result_learner_param_vals
learner_lgb_c$train(task_tt_train)
prediction_lgb_c = learner_lgb_c$predict(task_tt_test)
prediction_lgb_c$score(msr("classif.acc"))
cv5_lgb_c = resample(task_tt_train, toylrn_c, rsmp("cv", folds = 5))
cv5_lgb_c_tuned = resample(task_tt_train, learner_lgb_c, rsmp("cv", folds = 5))
cv5_lgb_c$aggregate(msr("classif.acc")); cv5_lgb_c_tuned$aggregate(msr("classif.acc"))
lrn("classif.rpart")$train(task_tt_train)$predict(task_tt_test)$score(msr("classif.acc"))
lrn("classif.rpart")$train(task_tt_train_nob)$predict(task_tt_test)$score(msr("classif.acc"))
learner_rpart_c = lts(lrn("classif.rpart"))
instance = ti(
task = task_tt_train_nob,
learner = learner_rpart_c,
resampling = rsmp("holdout"),
measures = msr("classif.acc"),
terminator = trm("evals", n_evals = 20)
)
tuner = tnr("grid_search") # 贝叶斯优化
tuner$optimize(instance)
learner_rpart_c$param_set$values = instance$result_learner_param_vals
learner_rpart_c$train(task_tt_train_nob)
learner_rpart_c$predict(task_tt_test)$score(msr("classif.acc"))
lrn("classif.lightgbm")$train(task_tt_train_nob)$predict(task_tt_test)$score(msr("classif.acc"))
split = partition(task_tt_train_nob)
task_tt_train_nob_e = task_tt_train_nob$clone(deep = T)$set_row_roles(split$test, "test")
learner_lgb_c = lrn("classif.lightgbm")
learner_lgb_c$param_set$set_values(
early_stopping = TRUE,
early_stopping_rounds = 128
)
pre_test = learner_lgb_c$train(task_tt_train_nob_e)$predict(task_tt_test)
pre_test$score(msr("classif.acc"))
# 注意一定要在训练集上找阈值
pre_train = learner_lgb_c$train(task_tt_train_nob_e)$predict(task_tt_train_nob_e)
# optimize是一种一维优化函数，用于使目标函数的值最小
# 我们的目标使得acc最大，也就是使得acc的负值最小
to_optim = function(pre_train, num) {
pre_train$set_threshold(num)
-pre_train$score(msr("classif.acc"))
}
for_thres = function(pre_train) {
thres = optimize(\(x) to_optim(pre_train, x), lower = 0, upper = 1)$minimum
cat("before: ", pre_train$set_threshold(0.5)$score(msr("classif.acc")),
"\n",
"after: ", pre_train$set_threshold(thres)$score(msr("classif.acc")))
thres
}
thres = for_thres(pre_train)
# 这个方法仅供尝试，未必总是能取得好效果。
pre_test$set_threshold(thres)$score(msr("classif.acc"))
learner_ctb$param_set$values
hp_lgb = readRDS("./data/hp_lgb.Rds")
test = lrn("regr.xgboost")
hp_rpart = instance$result_learner_param_vals
learner_rpart_c$param_set$values = hp_rpart
learner_rpart_c$train(task_tt_train_nob)
instance$result_learner_param_vals
hp_lgb
hp_xgb = learner_xgb$param_set$values
hp_ctb = learner_ctb$param_set$values
test$param_set$values = hp_xgb
test$train(task_pg_train)
test = lrn("regr.lightgbm")
test$param_set$values = hp_lgb
test$train(task_pg_train)
list(
learning_rate = 0.0539276,
num_iterations = 256,
max_depth = 3,
num_leaves = 5,
bagging_fraction = 0.82
)
test = lrn("regr.lightgbm")
test$param_set$values
hp_lgb
hp_lgb = list(
num_threads = 1,
verbose = 1,
objective = "regression",
convert_categorical = T,
learning_rate = 0.0539276,
num_iterations = 256,
max_depth = 3,
num_leaves = 5,
bagging_fraction = 0.82
)
test$param_set$values = hp_lgb
test$train(task_pg_train)
hp_lgb = list(
num_threads = 1,
verbose = -1,
objective = "regression",
convert_categorical = T,
learning_rate = 0.0539276,
num_iterations = 256,
max_depth = 3,
num_leaves = 5,
bagging_fraction = 0.82
)
test$param_set$values = hp_lgb
test$train(task_pg_train)
saveRDS(hp_lgb, "./data/hp_lgb.Rds")
save(hp_lgb, hp_ctb, hp_xgb, file = "./data/hps.Rdata")
learner_xgb_r = lrn("regr.xgboost"); learner_xgb_r$param_set$values = hp_xgb
learner_lgb_r = lrn("regr.lightgbm"); learner_lgb_r$param_set$values = hp_lgb
learner_ctb_r = lrn("regr.ctboost"); learner_ctb_r$param_set$values = hp_ctb
learner_ctb_r = lrn("regr.catboost"); learner_ctb_r$param_set$values = hp_ctb
gstack_test = gunion(list(
po("learner_cv", learner_xgb_r, id = "xgb"),
po("learner_cv", learner_lgb_r, id = "lgb"),
po("learner_cv", learner_ctb_r, id = "ctb"))) %>>%
po("featureunion")
a = gstack_test$train(task_pg_train)
lgr::get_logger("mlr3")$set_threshold("info")
lgr::get_logger("mlr3")$set_threshold("off")
a
resampled_1 = 1
resampled_1 = a
cv5_lgb = resample(task_pg_train, learner_lgb_r, rsmp("cv", folds = 5))
cv5_lgb$prediction()
resampled_2 = cv5_lgb$predictions()
resampled_2
cv5_lgb$prediction()
cv5_lgb$prediction() %>% View()
resampled_1
resampled_1 = resampled_1[[1]]
learner_base = lrn("regr.glmnet", alpha = 0)
search_space = ps(
s = p_dbl(lower = 0.001, upper = 2)
)
at = auto_tuner(
tuner = tnr("grid_search"),
learner = learner,
resampling = rsmp("cv", folds = 5),
measure = msr("regr.rmse"),
search_space = search_space,
terminator = trm("evals", n_evals = 50)
)
at = auto_tuner(
tuner = tnr("grid_search"),
learner = learner_base,
resampling = rsmp("cv", folds = 5),
measure = msr("regr.rmse"),
search_space = search_space,
terminator = trm("evals", n_evals = 50)
)
at$train(resampled_1)
future::plan("multisession", workers = 4)
at$train(resampled_1)
at$param_set
at$param_set$values
at$tuning_result
at$tuning_result$learner_param_vals
lts()
lts(lrn("regr.glmnet.default"))
lts(lrn("regr.glmnet"))
lts(lrn("regr.glmnet"))$param_Set
lts(lrn("regr.glmnet"))$param_set
glearner = gunion(list(
po("learner_cv", learner_xgb_r, id = "xgb"),
po("learner_cv", learner_lgb_r, id = "lgb"),
po("learner_cv", learner_ctb_r, id = "ctb"))) %>>%
po("featureunion") %>>%
po(lrn("regr.glmnet", param_vals = hp_base)) %>%
as_learner()
hp_base = at$tuning_result$learner_param_vals[[1]]
glearner = gunion(list(
po("learner_cv", learner_xgb_r, id = "xgb"),
po("learner_cv", learner_lgb_r, id = "lgb"),
po("learner_cv", learner_ctb_r, id = "ctb"))) %>>%
po("featureunion") %>>%
po(lrn("regr.glmnet", param_vals = hp_base)) %>%
as_learner()
hp_base
learner_base$param_set$values = hp_base
glearner = gunion(list(
po("learner_cv", learner_xgb_r, id = "xgb"),
po("learner_cv", learner_lgb_r, id = "lgb"),
po("learner_cv", learner_ctb_r, id = "ctb"))) %>>%
po("featureunion") %>>%
learner_base %>%
as_learner()
glearner
glearner$train(task_pg_train)
glearner
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(DT)
rnorm(10, mean = 1, sd = 1)
rnorm(10, mean = 1, sd = 1)
x = 1
samples = rnorm(10, mean = x, sd = 1)
samples
mean(sample3)
mean(samples)
x = 1
samples = rnorm(100, mean = x, sd = 1)
samples
samples = rnorm(100, mean = x, sd = 1)
mean(samples)
hist(samples)
x = 1
samples = rnorm(10000, mean = x, sd = 1)
hist(samples)
x = 1
samples = rnorm(n = 10000, mean = x, sd = 1)
hist(samples)
mean(samples)
mean(rnorm(n = 10, mean = 0, sd = 1))
mean(rnorm(n = 10, mean = 1, sd = 1))
mean(rnorm(n = 10, mean = 0, sd = 1))
mean(rnorm(n = 10, mean = 0, sd = 1))
mean(rnorm(n = 10, mean = 0, sd = 1))
mean(rnorm(n = 10, mean = 0, sd = 1))
mean(rnorm(n = 10, mean = 0, sd = 1))
mean(rnorm(n = 10, mean = 0, sd = 1))
mean(rnorm(n = 10, mean = 0, sd = 1))
mean(rnorm(n = 10, mean = 0, sd = 1))
mean(rnorm(n = 10, mean = 0, sd = 1))
mean(rnorm(n = 10, mean = 0, sd = 1))
mean(rnorm(n = 10, mean = 0, sd = 1))
mean(rnorm(n = 10, mean = 0, sd = 1))
dnorm(0, mean = 0, sd = 1)
ggplot(data = data.frame(x = c(-3, 3)), aes(x = x)) +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "blue") +
labs(title = "标准正态分布曲线")
ggplot(data = data.frame(x = c(-3, 3)), aes(x = x)) +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "blue") +
labs(title = "标准正态分布曲线") +
z
ggplot(data = data.frame(x = c(-3, 3)), aes(x = x)) +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "blue") +
labs(title = "标准正态分布曲线") +
theme(plot.title = element_text(hjust = 0.5))
ggplot(data = tibble(x = c(-3, 3)), aes(x = x)) +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "blue") +
labs(title = "标准正态分布曲线") +
theme(plot.title = element_text(hjust = 0.5)) +
theme_bw()
ggplot(data = tibble(x = c(-3, 3)), aes(x = x)) +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "blue") +
labs(title = "标准正态分布曲线") +
theme(plot.title = element_text(hjust = 0.5)) +
theme_bw()
ggplot(data = tibble(x = c(-3, 3)), aes(x = x)) +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "blue") +
labs(title = "标准正态分布曲线") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5))
ggplot(data = tibble(x = c(-3, 3)), aes(x = x)) +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "navy") +
labs(title = "标准正态分布曲线") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5))
ggplot(data = tibble(x = c(-3, 3)), aes(x = x)) +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "red") +
labs(title = "标准正态分布曲线") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5))
qnorm(0.95)
ggplot(data = tibble(x = c(-10, 10)), aes(x = x)) +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "red") +
labs(title = "标准正态分布曲线") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5))
ggplot(data = tibble(x = c(-5, 5)), aes(x = x)) +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "red") +
labs(title = "标准正态分布曲线") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5))
rnorm（100,0，c（1，2））
rnorm(100,0,c(1,2))
hist(rnorm(100,0,c(1,2)))
hist(rnorm(100,0,c(1,100)))
hist(rnorm(100,c(1,100),0))
hist(rnorm(100,c(1,10),0))
hist(rnorm(100,c(1,10),1))
hist(rnorm(1000,c(1,10),1))
mean(samples)
ggplot(data = tibble(x = c(-5, 5)), aes(x = x)) +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "red") +
geom_vline(xintercept = qnorm(0.95) * 1/100) +
labs(title = "标准正态分布曲线") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5))
ggplot(data = tibble(x = c(-5, 5)), aes(x = x)) +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "red") +
geom_vline(xintercept = qnorm(0.95) * 1/100) +
labs(title = "标准正态分布曲线") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(DT)
x = 1
samples = rnorm(n = 10000, mean = x, sd = 1)
ggplot(data = tibble(x = c(-5, 5)), aes(x = x)) +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "red") +
labs(title = "标准正态分布曲线") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5))
ggplot(data = tibble(x = c(-5, 5)), aes(x = x)) +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "red") +
geom_vline(xintercept = qnorm(0.95) * 1/10) +
labs(title = "标准正态分布曲线") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5))
ggplot(data = tibble(x = c(-5, 5)), aes(x = x)) +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1/10), color = "red") +
labs(title = "标准正态分布曲线") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5))
ggplot(tibble(x = c(-1, 1)), aes(x = x)) +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1/10), color = "red") +
labs(title = "标准正态分布曲线") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5))
ggplot(tibble(x = c(-1, 1)), aes(x = x)) +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1/10), color = "red") +
geom_vline(xintercept = qnorm(0.95) * 1/10) +
labs(title = "标准正态分布曲线") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5))
ggplot(tibble(x = c(-1, 1)), aes(x = x)) +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1/10), color = "red") +
geom_vline(xintercept = qnorm(0.95) * 1/10, linetype = 5) +
labs(title = "标准正态分布曲线") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5))
mean(samples)
ggplot(tibble(x = c(-1.2, 1.2)), aes(x = x)) +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1/10), color = "firebrick") +
geom_vline(xintercept = mean(samples), linetype = 5, color = "navy") +
geom_vline(xintercept = qnorm(0.95) * 1/10, linetype = 5) +
labs(title = "标准正态分布曲线") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5))
ggplot(tibble(x = c(-1.2, 1.2)), aes(x = x)) +
stat_function(fun = dnorm, args = list(mean = mean(samples), sd = 1/10), color = "firebrick") +
geom_vline(xintercept = mean(samples), linetype = 5, color = "navy") +
geom_vline(xintercept = qnorm(0.95) * 1/10, linetype = 5) +
labs(title = "均值分布曲线") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5))
ggplot(tibble(x = c(0, 2)), aes(x = x)) +
stat_function(fun = dnorm, args = list(mean = mean(samples), sd = 1/10), color = "firebrick") +
geom_vline(xintercept = mean(samples), linetype = 5, color = "navy") +
geom_vline(xintercept = qnorm(0.95) * 1/10, linetype = 5) +
labs(title = "均值分布曲线") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5))
pnorm(0.164, 1.02, 1)
qnorm(0.95)
qnorm(0.95) * 1/10
pnorm(0.164, 1.02, 1/10)
pnorm(0.164, 1.02, 1/10) %>% signif(digits = 2)
pnorm(0.164, 1.02, 1/10) %>% signif(digits = 6)
ggplot(tibble(x = c(0, 2)), aes(x = x)) +
stat_function(fun = dnorm, args = list(mean = mean(samples), sd = 1/10), color = "firebrick") +
geom_vline(xintercept = mean(samples), linetype = 5, color = "navy") +
geom_vline(xintercept = qnorm(0.95) * 1/10, linetype = 5) +
labs(title = "均值分布曲线") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5))
ggplot(tibble(x = c(-1.2, 2)), aes(x = x)) +
stat_function(fun = dnorm, args = list(mean = 0, sd = 1/10), color = "#CD853F") +
stat_function(fun = dnorm, args = list(mean = mean(samples), sd = 1/10), color = "firebrick") +
geom_vline(xintercept = mean(samples), linetype = 5, color = "navy") +
geom_vline(xintercept = qnorm(0.95) * 1/10, linetype = 5) +
labs(title = "均值分布曲线") +
theme_bw() +
theme(plot.title = element_text(hjust = 0.5))
